{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20bbd8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import Loader\n",
    "import tensorflow as tf\n",
    "import Transformer\n",
    "from sklearn import preprocessing\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.losses import MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "947b37be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2717 340 340\n"
     ]
    }
   ],
   "source": [
    "compoundsTrain, smilesTrain, labelsTrain, compoundDataTrain, activitiesTrain = Loader.getTrain(defaultValue=0)\n",
    "compoundsTest, smilesTest, labelsTest, compoundDataTest, activitiesTest = Loader.getTest(defaultValue=0)\n",
    "compoundsValidate, smilesValidate, labelsValidate, compoundDataValidate, activitiesValidate = Loader.getValidate(defaultValue=0)\n",
    "\n",
    "#print(labelsTrain)\n",
    "#print(compoundsTrain)\n",
    "#print(smilesTrain)\n",
    "#print(activitiesTrain)\n",
    "\n",
    "#for i in range(len(labelsTrain)):\n",
    "#    print(labelsTrain[i] + \": \", compoundDataTrain[0,i])\n",
    "\n",
    "def toClassification(y): # The resulting array will contain values of -1 if it is below 4.5 and 1 if it is above\n",
    "    y = np.array(y)\n",
    "    classification = (y.astype(float)>4).astype(int)\n",
    "    return classification * 2 - 1\n",
    "\n",
    "def normalizeData(train,test,validate):\n",
    "    for i in range(np.shape(train)[1]):\n",
    "        std = np.std(train[:,i])\n",
    "        mean = np.mean(train[:,i])\n",
    "        if(std == 0):\n",
    "            std = 1\n",
    "        train[:,i] = (train[:,i] - mean) / std\n",
    "        test[:,i] = (test[:,i] - mean) / std\n",
    "        validate[:,i] = (validate[:,i] - mean) / std\n",
    "    return train, test, validate\n",
    "\n",
    "def makeAverage(arr): # Averages the ten values for each score and creates a new array\n",
    "    arr = np.array(arr)\n",
    "    newArr = np.empty((np.shape(arr)[0], np.shape(arr)[1] - 18))\n",
    "    newArr[:,0] = np.mean(arr[:,:10], axis = 1)\n",
    "    newArr[:,1] = np.mean(arr[:,10:20], axis = 1)\n",
    "    newArr[:,2:] = arr[:,20:]\n",
    "    return newArr\n",
    "\n",
    "def averageScores(train, test, validate): #wrapper function to handle all conversions at once\n",
    "    newTrain = makeAverage(train)\n",
    "    newTest = makeAverage(test)\n",
    "    newValid = makeAverage(validate)\n",
    "    return newTrain, newTest, newValid\n",
    "    \n",
    "compoundDataTrain, compoundDataTest, compoundDataValidate = averageScores(compoundDataTrain, compoundDataTest, compoundDataValidate)\n",
    "compoundDataTrain, compoundDataTest, compoundDataValidate = normalizeData(compoundDataTrain, compoundDataTest, compoundDataValidate)\n",
    "print(len(compoundsTrain), len(compoundsTest), len(compoundsValidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ed3b5",
   "metadata": {},
   "source": [
    "# Testing by taking votes from full networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf8c8378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeVote(models, xValidate, yValidate, sameData = True, dataInds = []):\n",
    "    yAggregate = np.zeros(len(yValidate))\n",
    "    if(sameData):\n",
    "        for model in models:\n",
    "            yAggregate = yAggregate + np.sign(model.predict(xValidate))\n",
    "    else:\n",
    "        xValidate = np.array(xValidate)\n",
    "        for i in range(len(models)):\n",
    "            yAggregate = yAggregate + models[i].predict(xValidate[:,dataInds[i][0]:dataInds[i][1]])\n",
    "    return np.sign(yAggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c021d9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcut2d retention: [0.99364773]\n",
      "\ttotal: 99.36477273412316%\n",
      "chi retention: [0.9541968]\n",
      "\ttotal: 95.41967968597893%\n",
      "paoe retention: [0.31495127 0.19509321 0.1390042  0.08838179 0.07155396 0.04590061\n",
      " 0.03243    0.02673268 0.02402293]\n",
      "\ttotal: 93.80706737004672%\n",
      "smr retention: [0.50634726 0.24568468 0.08792319 0.07435827]\n",
      "\ttotal: 91.43133907840067%\n",
      "slogp retention: [0.44620276 0.22134465 0.15753302 0.04213752 0.03159298 0.02850562]\n",
      "\ttotal: 92.73165471968957%\n",
      "estate_vsa retention: [0.29224011 0.18964809 0.14361318 0.10642656 0.07394495 0.06391846\n",
      " 0.05513575]\n",
      "\ttotal: 92.49271047439832%\n",
      "vsa_estate retention: [0.49255304 0.32866098 0.09719245]\n",
      "\ttotal: 91.84064669216791%\n",
      "fr retention: [0.32153498 0.12578177 0.09944384 0.0570485  0.05494644 0.04503917\n",
      " 0.03646285 0.02896244 0.02597271 0.02289905 0.01957313 0.01726898\n",
      " 0.015143   0.01174721 0.01137858 0.00991649]\n",
      "\ttotal: 90.31191467920144%\n",
      "(2717, 95)\n",
      "0 \b:\t docking_score_max \t [0.32490387 0.10843408 1.51548771]\n",
      "1 \b:\t fusion_score_max \t [1.67291634 0.45186328 0.36357662]\n",
      "2 \b:\t maxestateindex \t [0.81474398 0.90374202 0.64206858]\n",
      "3 \b:\t minestateindex \t [0.26651101 0.37143365 0.31577349]\n",
      "4 \b:\t maxabsestateindex \t [0.81474398 0.90374202 0.64206858]\n",
      "5 \b:\t minabsestateindex \t [-0.66554839 -0.17464371 -0.36640431]\n",
      "6 \b:\t qed \t [ 0.27192664  0.41023473 -1.07122195]\n",
      "7 \b:\t molwt \t [ 0.52318683 -0.14128162  0.61300158]\n",
      "8 \b:\t heavyatommolwt \t [ 0.54984112 -0.12618969  0.60043001]\n",
      "9 \b:\t exactmolwt \t [ 0.52151743 -0.13876569  0.61529711]\n",
      "10 \b:\t numvalenceelectrons \t [ 0.45695003 -0.0587975   0.7148238 ]\n",
      "11 \b:\t numradicalelectrons \t [0. 0. 0.]\n",
      "12 \b:\t maxpartialcharge \t [-0.01918824 -0.01918824 -0.01918824]\n",
      "13 \b:\t minpartialcharge \t [-0.88605919  1.07966271  0.33858193]\n",
      "14 \b:\t maxabspartialcharge \t [-0.01918824 -0.01918824 -0.01918824]\n",
      "15 \b:\t minabspartialcharge \t [-0.7116252  -0.15498216 -0.07601257]\n",
      "16 \b:\t fpdensitymorgan1 \t [-0.04102765 -0.49817928 -0.22651605]\n",
      "17 \b:\t fpdensitymorgan2 \t [ 0.17652537 -0.29361553 -0.0178253 ]\n",
      "18 \b:\t fpdensitymorgan3 \t [ 0.46806209 -0.09369789 -0.21668676]\n",
      "19 \b:\t bcut2d_0 \t [ 0.56064602 -0.86712282 -0.86723949]\n",
      "20 \b:\t balabanj \t [-0.74209059 -0.10777129  0.0770716 ]\n",
      "21 \b:\t bertzct \t [0.92402273 0.04951178 0.89916713]\n",
      "22 \b:\t chi_0 \t [ 0.58290221 -0.09911968  0.71022275]\n",
      "23 \b:\t hallkieralpha \t [-0.6313074  -0.22443765 -1.38245156]\n",
      "24 \b:\t ipc \t [-0.01922524 -0.01922525 -0.01922524]\n",
      "25 \b:\t kappa1 \t [ 0.16025158 -0.10320667  0.74223064]\n",
      "26 \b:\t kappa2 \t [-0.15856955 -0.14236392  0.74953481]\n",
      "27 \b:\t kappa3 \t [-0.53101939 -0.31735682  0.63302177]\n",
      "28 \b:\t labuteasa \t [ 0.61094081 -0.12548248  0.7633054 ]\n",
      "29 \b:\t paoe_0 \t [-0.25074117 -0.37028283  1.0396446 ]\n",
      "30 \b:\t paoe_1 \t [-0.66488561  1.77489235  0.35948985]\n",
      "31 \b:\t paoe_2 \t [-2.59271489  1.39408251 -0.13973721]\n",
      "32 \b:\t paoe_3 \t [ 0.89411182 -0.55643661 -0.62969049]\n",
      "33 \b:\t paoe_4 \t [ 0.91494099  0.3928239  -0.35176385]\n",
      "34 \b:\t paoe_5 \t [-0.08026039 -0.37176127 -0.30052082]\n",
      "35 \b:\t paoe_6 \t [-0.23065638 -0.30230903 -0.87511141]\n",
      "36 \b:\t paoe_7 \t [-0.52832067  1.60389924 -0.67118738]\n",
      "37 \b:\t paoe_8 \t [-1.2507303   1.83215979  1.69521642]\n",
      "38 \b:\t smr_0 \t [ 0.01127435 -0.23594093 -0.62309861]\n",
      "39 \b:\t smr_1 \t [0.38138734 0.63552143 2.10992109]\n",
      "40 \b:\t smr_2 \t [ 0.97978612 -1.64311582 -0.97925273]\n",
      "41 \b:\t smr_3 \t [-0.39279063  0.75417526  0.84871852]\n",
      "42 \b:\t slogp_0 \t [ 0.0910766  -0.14782029  0.02312011]\n",
      "43 \b:\t slogp_1 \t [0.45089969 0.00810868 1.82239003]\n",
      "44 \b:\t slogp_2 \t [ 1.11494417 -1.65078578 -0.8745718 ]\n",
      "45 \b:\t slogp_3 \t [ 0.45314613 -2.0171456  -0.03837734]\n",
      "46 \b:\t slogp_4 \t [ 0.81754054  0.00584266 -1.0890697 ]\n",
      "47 \b:\t slogp_5 \t [-0.31899365  1.70298274 -0.2692608 ]\n",
      "48 \b:\t tpsa \t [-0.15259304 -0.41208112  0.30684791]\n",
      "49 \b:\t estate_vsa_0 \t [-0.40445902  0.52045338  0.20443278]\n",
      "50 \b:\t estate_vsa_1 \t [-0.39866308  0.32021    -0.46698605]\n",
      "51 \b:\t estate_vsa_2 \t [-1.82234697  1.45726027 -0.07636032]\n",
      "52 \b:\t estate_vsa_3 \t [ 0.59060734  0.90013931 -0.03346837]\n",
      "53 \b:\t estate_vsa_4 \t [ 0.13559068 -0.87399849  2.14514972]\n",
      "54 \b:\t estate_vsa_5 \t [-0.83040561  0.54198028  1.32432957]\n",
      "55 \b:\t estate_vsa_6 \t [ 0.54224444 -0.2828298   0.05769927]\n",
      "56 \b:\t vsa_estate_0 \t [-0.67861118  1.24858427 -0.20730574]\n",
      "57 \b:\t vsa_estate_1 \t [ 0.70838449 -0.28820433  0.08809216]\n",
      "58 \b:\t vsa_estate_2 \t [-0.74244715 -0.62543266  0.78403703]\n",
      "59 \b:\t fractioncsp3 \t [ 0.00533651 -0.33143908 -0.46002613]\n",
      "60 \b:\t heavyatomcount \t [ 0.58932562 -0.00450234  0.82685681]\n",
      "61 \b:\t nhohcount \t [-0.68073424  0.58956448  0.58956448]\n",
      "62 \b:\t nocount \t [ 0.23582134 -0.85138441  0.23582134]\n",
      "63 \b:\t numaliphaticcarbocycles \t [-0.52172007  0.98627293 -0.52172007]\n",
      "64 \b:\t numaliphaticheterocycles \t [ 2.76391298 -1.22063861 -1.22063861]\n",
      "65 \b:\t numaliphaticrings \t [ 1.62188604 -0.24772125 -1.18252489]\n",
      "66 \b:\t numaromaticcarbocycles \t [ 0.54061178 -0.6783444   0.54061178]\n",
      "67 \b:\t numaromaticheterocycles \t [0.08601029 1.39887527 1.39887527]\n",
      "68 \b:\t numaromaticrings \t [0.4552267  0.4552267  1.34955132]\n",
      "69 \b:\t numhacceptors \t [-0.12624794 -0.58482501 -0.12624794]\n",
      "70 \b:\t numhdonors \t [-0.66911996  0.00847934  0.68607865]\n",
      "71 \b:\t numheteroatoms \t [ 0.08420687 -0.23940001  0.08420687]\n",
      "72 \b:\t numrotatablebonds \t [-0.69039938 -0.69039938  0.65369285]\n",
      "73 \b:\t numsaturatedcarbocycles \t [-0.50306549  1.12024679 -0.50306549]\n",
      "74 \b:\t numsaturatedheterocycles \t [ 1.93455293 -0.89820391 -0.89820391]\n",
      "75 \b:\t numsaturatedrings \t [ 0.99671653  0.05280617 -0.89110418]\n",
      "76 \b:\t ringcount \t [1.84916394 0.20108179 0.20108179]\n",
      "77 \b:\t mollogp \t [0.21802916 0.96069093 0.63251503]\n",
      "78 \b:\t molmr \t [ 0.61642672 -0.17950015  0.84899018]\n",
      "79 \b:\t fr_0 \t [ 0.27978358 -0.70049038 -0.16882389]\n",
      "80 \b:\t fr_1 \t [0.70136431 0.54115737 1.01508294]\n",
      "81 \b:\t fr_2 \t [-0.63591165  1.07594406 -0.61679917]\n",
      "82 \b:\t fr_3 \t [-0.18307998 -0.23373869 -0.32155852]\n",
      "83 \b:\t fr_4 \t [ 2.73398743 -0.68157465 -0.66038425]\n",
      "84 \b:\t fr_5 \t [-1.60824989  0.1507125   0.71470688]\n",
      "85 \b:\t fr_6 \t [ 0.43909192 -0.40906254  0.27420296]\n",
      "86 \b:\t fr_7 \t [ 0.94925832 -0.35526253 -1.31576257]\n",
      "87 \b:\t fr_8 \t [-0.42762937  0.13845538  0.92807489]\n",
      "88 \b:\t fr_9 \t [0.21483778 0.22937691 0.95452272]\n",
      "89 \b:\t fr_10 \t [ 0.95524824 -1.31425701 -1.26728633]\n",
      "90 \b:\t fr_11 \t [-0.22164991 -2.56793984  0.42747004]\n",
      "91 \b:\t fr_12 \t [ 0.11473447 -1.25044937 -1.37293136]\n",
      "92 \b:\t fr_13 \t [0.0588535  0.00311926 0.46419232]\n",
      "93 \b:\t fr_14 \t [-0.09618347 -0.13923815 -0.87875007]\n",
      "94 \b:\t fr_15 \t [-0.66903993 -0.75604764 -0.34809065]\n"
     ]
    }
   ],
   "source": [
    "compoundsTrain, smilesTrain, labelsTrain, compoundDataTrain, activitiesTrain = Loader.getTrain(defaultValue=0)\n",
    "compoundsTest, smilesTest, labelsTest, compoundDataTest, activitiesTest = Loader.getTest(defaultValue=0)\n",
    "compoundsValidate, smilesValidate, labelsValidate, compoundDataValidate, activitiesValidate = Loader.getValidate(defaultValue=0)\n",
    "\n",
    "\n",
    "labelsPCA, trainPCA, testPCA, valPCA = Transformer.applyPCA(labelsTrain,  compoundDataTrain, \n",
    "                                                            compoundDataTest, compoundDataValidate,\n",
    "                                                            endDims=[1,1,9,4,6,7,3,16])\n",
    "\n",
    "labelsMeanPCA, trainMeanPCA = Transformer.useAverageFD(labelsPCA, trainPCA)\n",
    "_, testMeanPCA = Transformer.useAverageFD(labelsPCA, testPCA)\n",
    "_, valMeanPCA = Transformer.useAverageFD(labelsPCA, valPCA)\n",
    "\n",
    "labelsMaxPCA, trainMaxPCA = Transformer.useMaxFD(labelsPCA, trainPCA)\n",
    "_, testMaxPCA = Transformer.useMaxFD(labelsPCA, testPCA)\n",
    "_, valMaxPCA = Transformer.useMaxFD(labelsPCA, valPCA)\n",
    "\n",
    "#after transformations are done assign data\n",
    "dataLabels = labelsMaxPCA\n",
    "trainData = trainMaxPCA\n",
    "testData = testMaxPCA\n",
    "valData = valMaxPCA\n",
    "\n",
    "trainData, testData, valData = Transformer.normalizeData(trainData, testData, valData, newMean=0, newStd=1)\n",
    "\n",
    "print(np.shape(trainData))\n",
    "for i in range(len(dataLabels)):\n",
    "    print(i, \"\\b:\\t\", dataLabels[i], \"\\t\", trainData[0:3,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d645750c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 1.9582 - accuracy: 0.8318 - val_loss: 0.6999 - val_accuracy: 0.8529\n",
      "Epoch 2/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.5797 - accuracy: 0.8955 - val_loss: 0.5632 - val_accuracy: 0.8824\n",
      "Epoch 3/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.4892 - accuracy: 0.9003 - val_loss: 0.4913 - val_accuracy: 0.8912\n",
      "Epoch 4/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.4361 - accuracy: 0.9150 - val_loss: 0.4227 - val_accuracy: 0.9059\n",
      "Epoch 5/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.4059 - accuracy: 0.9183 - val_loss: 0.4417 - val_accuracy: 0.8912\n",
      "Epoch 6/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.3745 - accuracy: 0.9275 - val_loss: 0.4396 - val_accuracy: 0.8971\n",
      "Epoch 7/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.3552 - accuracy: 0.9282 - val_loss: 0.4119 - val_accuracy: 0.9118\n",
      "Epoch 8/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.3396 - accuracy: 0.9360 - val_loss: 0.4197 - val_accuracy: 0.8971\n",
      "Epoch 9/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.3274 - accuracy: 0.9382 - val_loss: 0.4231 - val_accuracy: 0.8912\n",
      "Epoch 10/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.3255 - accuracy: 0.9378 - val_loss: 0.3897 - val_accuracy: 0.9088\n",
      "Epoch 11/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.3065 - accuracy: 0.9437 - val_loss: 0.3928 - val_accuracy: 0.9029\n",
      "Epoch 12/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.3108 - accuracy: 0.9430 - val_loss: 0.3780 - val_accuracy: 0.9147\n",
      "Epoch 13/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2895 - accuracy: 0.9485 - val_loss: 0.4063 - val_accuracy: 0.9088\n",
      "Epoch 14/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2898 - accuracy: 0.9459 - val_loss: 0.3952 - val_accuracy: 0.9147\n",
      "Epoch 15/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2833 - accuracy: 0.9477 - val_loss: 0.4085 - val_accuracy: 0.9088\n",
      "Epoch 16/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2836 - accuracy: 0.9488 - val_loss: 0.4081 - val_accuracy: 0.8971\n",
      "Epoch 17/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2787 - accuracy: 0.9510 - val_loss: 0.4204 - val_accuracy: 0.9176\n",
      "Epoch 18/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2670 - accuracy: 0.9533 - val_loss: 0.3902 - val_accuracy: 0.9059\n",
      "Epoch 19/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2733 - accuracy: 0.9488 - val_loss: 0.4917 - val_accuracy: 0.8971\n",
      "Epoch 20/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2673 - accuracy: 0.9544 - val_loss: 0.3988 - val_accuracy: 0.9088\n",
      "Epoch 21/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2633 - accuracy: 0.9547 - val_loss: 0.4146 - val_accuracy: 0.9059\n",
      "Epoch 22/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2589 - accuracy: 0.9514 - val_loss: 0.4045 - val_accuracy: 0.9059\n",
      "Epoch 23/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2534 - accuracy: 0.9606 - val_loss: 0.4182 - val_accuracy: 0.9059\n",
      "Epoch 24/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2608 - accuracy: 0.9555 - val_loss: 0.3838 - val_accuracy: 0.9147\n",
      "Epoch 25/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2427 - accuracy: 0.9621 - val_loss: 0.4483 - val_accuracy: 0.8971\n",
      "Epoch 26/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2572 - accuracy: 0.9533 - val_loss: 0.3954 - val_accuracy: 0.9088\n",
      "Epoch 27/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2380 - accuracy: 0.9654 - val_loss: 0.4754 - val_accuracy: 0.8912\n",
      "Epoch 28/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2459 - accuracy: 0.9628 - val_loss: 0.4561 - val_accuracy: 0.9059\n",
      "Epoch 29/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2478 - accuracy: 0.9577 - val_loss: 0.4343 - val_accuracy: 0.9000\n",
      "Epoch 30/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2377 - accuracy: 0.9650 - val_loss: 0.4113 - val_accuracy: 0.8941\n",
      "Epoch 31/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2502 - accuracy: 0.9636 - val_loss: 0.4001 - val_accuracy: 0.9000\n",
      "Epoch 32/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2377 - accuracy: 0.9625 - val_loss: 0.4454 - val_accuracy: 0.9029\n",
      "Epoch 33/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2512 - accuracy: 0.9606 - val_loss: 0.4111 - val_accuracy: 0.9000\n",
      "Epoch 34/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2302 - accuracy: 0.9669 - val_loss: 0.4475 - val_accuracy: 0.9029\n",
      "Epoch 35/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2347 - accuracy: 0.9639 - val_loss: 0.4322 - val_accuracy: 0.9000\n",
      "Epoch 36/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2279 - accuracy: 0.9632 - val_loss: 0.4104 - val_accuracy: 0.9088\n",
      "Epoch 37/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2439 - accuracy: 0.9580 - val_loss: 0.4213 - val_accuracy: 0.9000\n",
      "Epoch 38/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2279 - accuracy: 0.9654 - val_loss: 0.4266 - val_accuracy: 0.9059\n",
      "Epoch 39/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2298 - accuracy: 0.9654 - val_loss: 0.4493 - val_accuracy: 0.8971\n",
      "Epoch 40/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2351 - accuracy: 0.9625 - val_loss: 0.4303 - val_accuracy: 0.9000\n",
      "Epoch 41/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2281 - accuracy: 0.9691 - val_loss: 0.4395 - val_accuracy: 0.9000\n",
      "Epoch 42/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2237 - accuracy: 0.9669 - val_loss: 0.4535 - val_accuracy: 0.8971\n",
      "Epoch 43/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2282 - accuracy: 0.9687 - val_loss: 0.4550 - val_accuracy: 0.8971\n",
      "Epoch 44/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2311 - accuracy: 0.9658 - val_loss: 0.4210 - val_accuracy: 0.9118\n",
      "Epoch 45/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2230 - accuracy: 0.9698 - val_loss: 0.4554 - val_accuracy: 0.8912\n",
      "Epoch 46/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2257 - accuracy: 0.9676 - val_loss: 0.4315 - val_accuracy: 0.9118\n",
      "Epoch 47/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2244 - accuracy: 0.9665 - val_loss: 0.4102 - val_accuracy: 0.9029\n",
      "Epoch 48/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2129 - accuracy: 0.9735 - val_loss: 0.4190 - val_accuracy: 0.9176\n",
      "Epoch 49/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2186 - accuracy: 0.9706 - val_loss: 0.4244 - val_accuracy: 0.9206\n",
      "Epoch 50/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2210 - accuracy: 0.9691 - val_loss: 0.4276 - val_accuracy: 0.9118\n",
      "Epoch 51/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2208 - accuracy: 0.9691 - val_loss: 0.4723 - val_accuracy: 0.9000\n",
      "Epoch 52/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2198 - accuracy: 0.9702 - val_loss: 0.5096 - val_accuracy: 0.9000\n",
      "Epoch 53/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2217 - accuracy: 0.9702 - val_loss: 0.4226 - val_accuracy: 0.9000\n",
      "Epoch 54/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2286 - accuracy: 0.9680 - val_loss: 0.4176 - val_accuracy: 0.9147\n",
      "Epoch 55/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2166 - accuracy: 0.9720 - val_loss: 0.4903 - val_accuracy: 0.8824\n",
      "Epoch 56/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2143 - accuracy: 0.9735 - val_loss: 0.4236 - val_accuracy: 0.9088\n",
      "Epoch 57/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2193 - accuracy: 0.9746 - val_loss: 0.4549 - val_accuracy: 0.9029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2267 - accuracy: 0.9687 - val_loss: 0.4107 - val_accuracy: 0.9118\n",
      "Epoch 59/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2123 - accuracy: 0.9742 - val_loss: 0.4288 - val_accuracy: 0.8912\n",
      "Epoch 60/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2192 - accuracy: 0.9676 - val_loss: 0.4297 - val_accuracy: 0.9118\n",
      "Epoch 61/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2175 - accuracy: 0.9739 - val_loss: 0.4652 - val_accuracy: 0.9000\n",
      "Epoch 62/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2152 - accuracy: 0.9724 - val_loss: 0.4850 - val_accuracy: 0.8853\n",
      "Epoch 63/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2209 - accuracy: 0.9739 - val_loss: 0.4386 - val_accuracy: 0.8971\n",
      "Epoch 64/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2200 - accuracy: 0.9724 - val_loss: 0.4743 - val_accuracy: 0.8912\n",
      "Epoch 65/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2012 - accuracy: 0.9787 - val_loss: 0.4406 - val_accuracy: 0.9000\n",
      "Epoch 66/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2166 - accuracy: 0.9742 - val_loss: 0.4177 - val_accuracy: 0.9147\n",
      "Epoch 67/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2177 - accuracy: 0.9717 - val_loss: 0.4033 - val_accuracy: 0.9147\n",
      "Epoch 68/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2074 - accuracy: 0.9787 - val_loss: 0.4702 - val_accuracy: 0.9029\n",
      "Epoch 69/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2045 - accuracy: 0.9779 - val_loss: 0.4297 - val_accuracy: 0.9147\n",
      "Epoch 70/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2190 - accuracy: 0.9724 - val_loss: 0.4396 - val_accuracy: 0.9118\n",
      "Epoch 71/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2208 - accuracy: 0.9724 - val_loss: 0.4475 - val_accuracy: 0.9000\n",
      "Epoch 72/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2015 - accuracy: 0.9798 - val_loss: 0.4609 - val_accuracy: 0.8971\n",
      "Epoch 73/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2076 - accuracy: 0.9746 - val_loss: 0.4690 - val_accuracy: 0.8941\n",
      "Epoch 74/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2078 - accuracy: 0.9746 - val_loss: 0.5007 - val_accuracy: 0.8912\n",
      "Epoch 75/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2077 - accuracy: 0.9764 - val_loss: 0.4782 - val_accuracy: 0.8941\n",
      "Epoch 76/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2171 - accuracy: 0.9698 - val_loss: 0.4448 - val_accuracy: 0.9059\n",
      "Epoch 77/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2034 - accuracy: 0.9757 - val_loss: 0.4544 - val_accuracy: 0.9088\n",
      "Epoch 78/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.1991 - accuracy: 0.9805 - val_loss: 0.4523 - val_accuracy: 0.9118\n",
      "Epoch 79/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2086 - accuracy: 0.9768 - val_loss: 0.4374 - val_accuracy: 0.9029\n",
      "Epoch 80/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2001 - accuracy: 0.9794 - val_loss: 0.4654 - val_accuracy: 0.9059\n",
      "Epoch 81/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2065 - accuracy: 0.9805 - val_loss: 0.4313 - val_accuracy: 0.9088\n",
      "Epoch 82/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2044 - accuracy: 0.9794 - val_loss: 0.4406 - val_accuracy: 0.9088\n",
      "Epoch 83/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2090 - accuracy: 0.9790 - val_loss: 0.4411 - val_accuracy: 0.9147\n",
      "Epoch 84/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.1996 - accuracy: 0.9794 - val_loss: 0.4260 - val_accuracy: 0.9118\n",
      "Epoch 85/100\n",
      "680/680 [==============================] - 2s 4ms/step - loss: 0.1996 - accuracy: 0.9805 - val_loss: 0.4567 - val_accuracy: 0.8912\n",
      "Epoch 86/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2102 - accuracy: 0.9739 - val_loss: 0.4706 - val_accuracy: 0.9029\n",
      "Epoch 87/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2111 - accuracy: 0.9731 - val_loss: 0.3893 - val_accuracy: 0.9147\n",
      "Epoch 88/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2015 - accuracy: 0.9783 - val_loss: 0.4485 - val_accuracy: 0.9059\n",
      "Epoch 89/100\n",
      "680/680 [==============================] - 2s 4ms/step - loss: 0.1973 - accuracy: 0.9790 - val_loss: 0.4410 - val_accuracy: 0.9059\n",
      "Epoch 90/100\n",
      "680/680 [==============================] - 2s 4ms/step - loss: 0.2020 - accuracy: 0.9790 - val_loss: 0.4388 - val_accuracy: 0.9147\n",
      "Epoch 91/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2238 - accuracy: 0.9735 - val_loss: 0.4551 - val_accuracy: 0.8971\n",
      "Epoch 92/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.1989 - accuracy: 0.9816 - val_loss: 0.4141 - val_accuracy: 0.9118\n",
      "Epoch 93/100\n",
      "680/680 [==============================] - 2s 4ms/step - loss: 0.1974 - accuracy: 0.9798 - val_loss: 0.4510 - val_accuracy: 0.9118\n",
      "Epoch 94/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2051 - accuracy: 0.9768 - val_loss: 0.4420 - val_accuracy: 0.9147\n",
      "Epoch 95/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2135 - accuracy: 0.9709 - val_loss: 0.4485 - val_accuracy: 0.9000\n",
      "Epoch 96/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.1943 - accuracy: 0.9783 - val_loss: 0.4757 - val_accuracy: 0.9118\n",
      "Epoch 97/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2059 - accuracy: 0.9787 - val_loss: 0.4953 - val_accuracy: 0.9000\n",
      "Epoch 98/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.1987 - accuracy: 0.9812 - val_loss: 0.5205 - val_accuracy: 0.8941\n",
      "Epoch 99/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2014 - accuracy: 0.9779 - val_loss: 0.4353 - val_accuracy: 0.9029\n",
      "Epoch 100/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.1916 - accuracy: 0.9816 - val_loss: 0.4723 - val_accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "classTrain = Transformer.toBinaryClassification(activitiesTrain)\n",
    "classVal = Transformer.toBinaryClassification(activitiesValidate)\n",
    "classTest = Transformer.toBinaryClassification(activitiesTest)\n",
    "inputDim = len(dataLabels)\n",
    "\n",
    "\n",
    "l1Reg = keras.regularizers.L1(.001)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False\n",
    ")\n",
    "\n",
    "model1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(inputDim, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(200, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(300, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(200, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(100, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(50, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(10, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(2)\n",
    "])\n",
    "\n",
    "model1.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model1.fit(trainData, classTrain, \n",
    "                    validation_data = (valData, classVal), epochs=100, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6605b5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "680/680 [==============================] - 3s 4ms/step - loss: 0.8703 - accuracy: 0.7913 - val_loss: 0.4752 - val_accuracy: 0.8676\n",
      "Epoch 2/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.4475 - accuracy: 0.8730 - val_loss: 0.4196 - val_accuracy: 0.8676\n",
      "Epoch 3/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.4056 - accuracy: 0.8918 - val_loss: 0.4282 - val_accuracy: 0.8647\n",
      "Epoch 4/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.3782 - accuracy: 0.9021 - val_loss: 0.3879 - val_accuracy: 0.8765\n",
      "Epoch 5/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.3612 - accuracy: 0.9047 - val_loss: 0.3867 - val_accuracy: 0.8853\n",
      "Epoch 6/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.3506 - accuracy: 0.9113 - val_loss: 0.3713 - val_accuracy: 0.8853\n",
      "Epoch 7/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.3464 - accuracy: 0.9106 - val_loss: 0.3881 - val_accuracy: 0.8824\n",
      "Epoch 8/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.3346 - accuracy: 0.9161 - val_loss: 0.3759 - val_accuracy: 0.8794\n",
      "Epoch 9/100\n",
      "680/680 [==============================] - 2s 4ms/step - loss: 0.3208 - accuracy: 0.9216 - val_loss: 0.3478 - val_accuracy: 0.8971\n",
      "Epoch 10/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.3175 - accuracy: 0.9257 - val_loss: 0.3478 - val_accuracy: 0.9088\n",
      "Epoch 11/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.3058 - accuracy: 0.9260 - val_loss: 0.3577 - val_accuracy: 0.9000\n",
      "Epoch 12/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2962 - accuracy: 0.9315 - val_loss: 0.3378 - val_accuracy: 0.9088\n",
      "Epoch 13/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.3011 - accuracy: 0.9286 - val_loss: 0.3363 - val_accuracy: 0.9088\n",
      "Epoch 14/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2832 - accuracy: 0.9352 - val_loss: 0.3382 - val_accuracy: 0.9118\n",
      "Epoch 15/100\n",
      "680/680 [==============================] - 2s 4ms/step - loss: 0.2801 - accuracy: 0.9371 - val_loss: 0.3462 - val_accuracy: 0.9000\n",
      "Epoch 16/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2775 - accuracy: 0.9349 - val_loss: 0.3316 - val_accuracy: 0.9176\n",
      "Epoch 17/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2738 - accuracy: 0.9382 - val_loss: 0.3204 - val_accuracy: 0.9235\n",
      "Epoch 18/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2636 - accuracy: 0.9404 - val_loss: 0.3306 - val_accuracy: 0.9000\n",
      "Epoch 19/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2722 - accuracy: 0.9378 - val_loss: 0.3265 - val_accuracy: 0.9176\n",
      "Epoch 20/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2631 - accuracy: 0.9374 - val_loss: 0.3437 - val_accuracy: 0.9147\n",
      "Epoch 21/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2628 - accuracy: 0.9393 - val_loss: 0.3261 - val_accuracy: 0.9088\n",
      "Epoch 22/100\n",
      "680/680 [==============================] - 2s 4ms/step - loss: 0.2582 - accuracy: 0.9389 - val_loss: 0.3150 - val_accuracy: 0.9265\n",
      "Epoch 23/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2520 - accuracy: 0.9433 - val_loss: 0.3332 - val_accuracy: 0.9059\n",
      "Epoch 24/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2562 - accuracy: 0.9452 - val_loss: 0.3797 - val_accuracy: 0.9088\n",
      "Epoch 25/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2526 - accuracy: 0.9418 - val_loss: 0.3354 - val_accuracy: 0.9118\n",
      "Epoch 26/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2481 - accuracy: 0.9426 - val_loss: 0.3266 - val_accuracy: 0.9176\n",
      "Epoch 27/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2495 - accuracy: 0.9430 - val_loss: 0.3188 - val_accuracy: 0.9294\n",
      "Epoch 28/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2495 - accuracy: 0.9455 - val_loss: 0.3292 - val_accuracy: 0.9294\n",
      "Epoch 29/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2495 - accuracy: 0.9396 - val_loss: 0.3234 - val_accuracy: 0.9147\n",
      "Epoch 30/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2457 - accuracy: 0.9430 - val_loss: 0.3226 - val_accuracy: 0.9206\n",
      "Epoch 31/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2379 - accuracy: 0.9474 - val_loss: 0.3214 - val_accuracy: 0.9176\n",
      "Epoch 32/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2388 - accuracy: 0.9470 - val_loss: 0.3225 - val_accuracy: 0.9206\n",
      "Epoch 33/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2363 - accuracy: 0.9444 - val_loss: 0.3163 - val_accuracy: 0.9235\n",
      "Epoch 34/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2357 - accuracy: 0.9448 - val_loss: 0.3806 - val_accuracy: 0.9029\n",
      "Epoch 35/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2398 - accuracy: 0.9430 - val_loss: 0.3295 - val_accuracy: 0.9206\n",
      "Epoch 36/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2404 - accuracy: 0.9430 - val_loss: 0.3217 - val_accuracy: 0.9206\n",
      "Epoch 37/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2328 - accuracy: 0.9492 - val_loss: 0.3405 - val_accuracy: 0.9147\n",
      "Epoch 38/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2326 - accuracy: 0.9492 - val_loss: 0.3304 - val_accuracy: 0.9118\n",
      "Epoch 39/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2355 - accuracy: 0.9448 - val_loss: 0.3426 - val_accuracy: 0.9118\n",
      "Epoch 40/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2302 - accuracy: 0.9499 - val_loss: 0.3316 - val_accuracy: 0.9176\n",
      "Epoch 41/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2317 - accuracy: 0.9474 - val_loss: 0.3495 - val_accuracy: 0.9176\n",
      "Epoch 42/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2310 - accuracy: 0.9470 - val_loss: 0.3293 - val_accuracy: 0.9118\n",
      "Epoch 43/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2317 - accuracy: 0.9448 - val_loss: 0.3161 - val_accuracy: 0.9235\n",
      "Epoch 44/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2340 - accuracy: 0.9452 - val_loss: 0.3496 - val_accuracy: 0.9147\n",
      "Epoch 45/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2285 - accuracy: 0.9514 - val_loss: 0.3245 - val_accuracy: 0.9235\n",
      "Epoch 46/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2327 - accuracy: 0.9444 - val_loss: 0.3210 - val_accuracy: 0.9088\n",
      "Epoch 47/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2249 - accuracy: 0.9510 - val_loss: 0.3188 - val_accuracy: 0.9235\n",
      "Epoch 48/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2294 - accuracy: 0.9507 - val_loss: 0.3714 - val_accuracy: 0.9147\n",
      "Epoch 49/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2342 - accuracy: 0.9544 - val_loss: 0.3323 - val_accuracy: 0.9235\n",
      "Epoch 50/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2265 - accuracy: 0.9496 - val_loss: 0.3256 - val_accuracy: 0.9235\n",
      "Epoch 51/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2287 - accuracy: 0.9529 - val_loss: 0.3309 - val_accuracy: 0.9206\n",
      "Epoch 52/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2262 - accuracy: 0.9522 - val_loss: 0.3212 - val_accuracy: 0.9147\n",
      "Epoch 53/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2280 - accuracy: 0.9477 - val_loss: 0.3259 - val_accuracy: 0.9118\n",
      "Epoch 54/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2288 - accuracy: 0.9518 - val_loss: 0.3476 - val_accuracy: 0.9059\n",
      "Epoch 55/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2327 - accuracy: 0.9503 - val_loss: 0.3331 - val_accuracy: 0.9265\n",
      "Epoch 56/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2272 - accuracy: 0.9522 - val_loss: 0.3254 - val_accuracy: 0.9206\n",
      "Epoch 57/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2218 - accuracy: 0.9536 - val_loss: 0.3434 - val_accuracy: 0.8941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2205 - accuracy: 0.9551 - val_loss: 0.3784 - val_accuracy: 0.9088\n",
      "Epoch 59/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2270 - accuracy: 0.9522 - val_loss: 0.3383 - val_accuracy: 0.9118\n",
      "Epoch 60/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2189 - accuracy: 0.9544 - val_loss: 0.3948 - val_accuracy: 0.9029\n",
      "Epoch 61/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2232 - accuracy: 0.9503 - val_loss: 0.3561 - val_accuracy: 0.9206\n",
      "Epoch 62/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2365 - accuracy: 0.9437 - val_loss: 0.3137 - val_accuracy: 0.9118\n",
      "Epoch 63/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2217 - accuracy: 0.9544 - val_loss: 0.3561 - val_accuracy: 0.9147\n",
      "Epoch 64/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2253 - accuracy: 0.9503 - val_loss: 0.3965 - val_accuracy: 0.9088\n",
      "Epoch 65/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2216 - accuracy: 0.9551 - val_loss: 0.3733 - val_accuracy: 0.9118\n",
      "Epoch 66/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2257 - accuracy: 0.9488 - val_loss: 0.3125 - val_accuracy: 0.9176\n",
      "Epoch 67/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2226 - accuracy: 0.9551 - val_loss: 0.3546 - val_accuracy: 0.9147\n",
      "Epoch 68/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2294 - accuracy: 0.9485 - val_loss: 0.3391 - val_accuracy: 0.9088\n",
      "Epoch 69/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2231 - accuracy: 0.9533 - val_loss: 0.3450 - val_accuracy: 0.9118\n",
      "Epoch 70/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2109 - accuracy: 0.9573 - val_loss: 0.3387 - val_accuracy: 0.9088\n",
      "Epoch 71/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2237 - accuracy: 0.9488 - val_loss: 0.3405 - val_accuracy: 0.9088\n",
      "Epoch 72/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2180 - accuracy: 0.9536 - val_loss: 0.3743 - val_accuracy: 0.9059\n",
      "Epoch 73/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2212 - accuracy: 0.9525 - val_loss: 0.3536 - val_accuracy: 0.9088\n",
      "Epoch 74/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2221 - accuracy: 0.9533 - val_loss: 0.3320 - val_accuracy: 0.9118\n",
      "Epoch 75/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2147 - accuracy: 0.9540 - val_loss: 0.3656 - val_accuracy: 0.9088\n",
      "Epoch 76/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2211 - accuracy: 0.9533 - val_loss: 0.3287 - val_accuracy: 0.9176\n",
      "Epoch 77/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2194 - accuracy: 0.9533 - val_loss: 0.3648 - val_accuracy: 0.9147\n",
      "Epoch 78/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2260 - accuracy: 0.9514 - val_loss: 0.3171 - val_accuracy: 0.9265\n",
      "Epoch 79/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2165 - accuracy: 0.9507 - val_loss: 0.3500 - val_accuracy: 0.9176\n",
      "Epoch 80/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2173 - accuracy: 0.9547 - val_loss: 0.3510 - val_accuracy: 0.9206\n",
      "Epoch 81/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2154 - accuracy: 0.9492 - val_loss: 0.3877 - val_accuracy: 0.9118\n",
      "Epoch 82/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2138 - accuracy: 0.9547 - val_loss: 0.3613 - val_accuracy: 0.9088\n",
      "Epoch 83/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2262 - accuracy: 0.9522 - val_loss: 0.3638 - val_accuracy: 0.9118\n",
      "Epoch 84/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.2180 - accuracy: 0.9566 - val_loss: 0.3208 - val_accuracy: 0.9235\n",
      "Epoch 85/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2176 - accuracy: 0.9551 - val_loss: 0.3635 - val_accuracy: 0.9176\n",
      "Epoch 86/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2191 - accuracy: 0.9551 - val_loss: 0.3287 - val_accuracy: 0.9206\n",
      "Epoch 87/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2215 - accuracy: 0.9547 - val_loss: 0.3438 - val_accuracy: 0.9176\n",
      "Epoch 88/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2139 - accuracy: 0.9580 - val_loss: 0.3562 - val_accuracy: 0.9059\n",
      "Epoch 89/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2196 - accuracy: 0.9562 - val_loss: 0.3385 - val_accuracy: 0.9265\n",
      "Epoch 90/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2220 - accuracy: 0.9544 - val_loss: 0.3342 - val_accuracy: 0.9206\n",
      "Epoch 91/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2145 - accuracy: 0.9558 - val_loss: 0.3349 - val_accuracy: 0.9265\n",
      "Epoch 92/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2159 - accuracy: 0.9544 - val_loss: 0.3564 - val_accuracy: 0.9059\n",
      "Epoch 93/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2255 - accuracy: 0.9533 - val_loss: 0.3578 - val_accuracy: 0.9088\n",
      "Epoch 94/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2124 - accuracy: 0.9562 - val_loss: 0.3439 - val_accuracy: 0.9147\n",
      "Epoch 95/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2126 - accuracy: 0.9573 - val_loss: 0.3701 - val_accuracy: 0.9118\n",
      "Epoch 96/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2188 - accuracy: 0.9536 - val_loss: 0.3514 - val_accuracy: 0.9206\n",
      "Epoch 97/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2152 - accuracy: 0.9573 - val_loss: 0.3372 - val_accuracy: 0.9176\n",
      "Epoch 98/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2170 - accuracy: 0.9566 - val_loss: 0.3438 - val_accuracy: 0.9176\n",
      "Epoch 99/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2250 - accuracy: 0.9536 - val_loss: 0.3617 - val_accuracy: 0.9176\n",
      "Epoch 100/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.2182 - accuracy: 0.9536 - val_loss: 0.3388 - val_accuracy: 0.9147\n"
     ]
    }
   ],
   "source": [
    "model2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(inputDim, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(200, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(300, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(200, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(100, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(50, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(10, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(2)\n",
    "])\n",
    "\n",
    "model2.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model2.fit(trainData, classTrain, \n",
    "                    validation_data = (valData, classVal), epochs=100, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9168365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.9407 - accuracy: 0.6161 - val_loss: 0.6961 - val_accuracy: 0.6118\n",
      "Epoch 2/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6893 - accuracy: 0.6198 - val_loss: 0.6903 - val_accuracy: 0.6118\n",
      "Epoch 3/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6859 - accuracy: 0.6198 - val_loss: 0.6883 - val_accuracy: 0.6118\n",
      "Epoch 4/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6846 - accuracy: 0.6198 - val_loss: 0.6875 - val_accuracy: 0.6118\n",
      "Epoch 5/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6838 - accuracy: 0.6198 - val_loss: 0.6873 - val_accuracy: 0.6118\n",
      "Epoch 6/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6833 - accuracy: 0.6198 - val_loss: 0.6867 - val_accuracy: 0.6118\n",
      "Epoch 7/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6870 - val_accuracy: 0.6118\n",
      "Epoch 8/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6831 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 9/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 10/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 11/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6868 - val_accuracy: 0.6118\n",
      "Epoch 12/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6831 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 13/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 14/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 15/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6831 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 16/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6867 - val_accuracy: 0.6118\n",
      "Epoch 17/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 18/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 19/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6864 - val_accuracy: 0.6118\n",
      "Epoch 20/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 21/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6864 - val_accuracy: 0.6118\n",
      "Epoch 22/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 23/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 24/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 25/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 26/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6828 - accuracy: 0.6198 - val_loss: 0.6864 - val_accuracy: 0.6118\n",
      "Epoch 27/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 28/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6864 - val_accuracy: 0.6118\n",
      "Epoch 29/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6864 - val_accuracy: 0.6118\n",
      "Epoch 30/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 31/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 32/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 33/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6831 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 34/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 35/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 36/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6864 - val_accuracy: 0.6118\n",
      "Epoch 37/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6868 - val_accuracy: 0.6118\n",
      "Epoch 38/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 39/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6828 - accuracy: 0.6198 - val_loss: 0.6867 - val_accuracy: 0.6118\n",
      "Epoch 40/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6828 - accuracy: 0.6198 - val_loss: 0.6863 - val_accuracy: 0.6118\n",
      "Epoch 41/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6864 - val_accuracy: 0.6118\n",
      "Epoch 42/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 43/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6864 - val_accuracy: 0.6118\n",
      "Epoch 44/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 45/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 46/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 47/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 48/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 49/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6864 - val_accuracy: 0.6118\n",
      "Epoch 50/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 51/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6867 - val_accuracy: 0.6118\n",
      "Epoch 52/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6864 - val_accuracy: 0.6118\n",
      "Epoch 53/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 54/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6831 - accuracy: 0.6198 - val_loss: 0.6864 - val_accuracy: 0.6118\n",
      "Epoch 55/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6864 - val_accuracy: 0.6118\n",
      "Epoch 56/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 57/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6867 - val_accuracy: 0.6118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 59/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6867 - val_accuracy: 0.6118\n",
      "Epoch 60/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 61/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 62/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 63/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 64/100\n",
      "680/680 [==============================] - 2s 4ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 65/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 66/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 67/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 68/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 69/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 70/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 71/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 72/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6868 - val_accuracy: 0.6118\n",
      "Epoch 73/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6864 - val_accuracy: 0.6118\n",
      "Epoch 74/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6867 - val_accuracy: 0.6118\n",
      "Epoch 75/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6868 - val_accuracy: 0.6118\n",
      "Epoch 76/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 77/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 78/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 79/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6867 - val_accuracy: 0.6118\n",
      "Epoch 80/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6868 - val_accuracy: 0.6118\n",
      "Epoch 81/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6868 - val_accuracy: 0.6118\n",
      "Epoch 82/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6864 - val_accuracy: 0.6118\n",
      "Epoch 83/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 84/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 85/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 86/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6867 - val_accuracy: 0.6118\n",
      "Epoch 87/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6867 - val_accuracy: 0.6118\n",
      "Epoch 88/100\n",
      "680/680 [==============================] - 2s 4ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6867 - val_accuracy: 0.6118\n",
      "Epoch 89/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 90/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 91/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6864 - val_accuracy: 0.6118\n",
      "Epoch 92/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 93/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 94/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 95/100\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 96/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n",
      "Epoch 97/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 98/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6867 - val_accuracy: 0.6118\n",
      "Epoch 99/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6829 - accuracy: 0.6198 - val_loss: 0.6865 - val_accuracy: 0.6118\n",
      "Epoch 100/100\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.6198 - val_loss: 0.6866 - val_accuracy: 0.6118\n"
     ]
    }
   ],
   "source": [
    "model3 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(inputDim, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(200, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(300, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(200, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(100, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(50, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(10, activation='relu', kernel_regularizer = l1Reg),\n",
    "    tf.keras.layers.Dense(2)\n",
    "])\n",
    "\n",
    "model3.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model3.fit(trainData, classTrain, \n",
    "                    validation_data = (valData, classVal), epochs=100, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f5fd9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (340,) (340,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-dd6af4336225>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Aggregation: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassVal\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtakeVote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassVal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassVal\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassVal\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassVal\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-45728c2fcc40>\u001b[0m in \u001b[0;36mtakeVote\u001b[1;34m(models, xValidate, yValidate, sameData, dataInds)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msameData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m             \u001b[0myAggregate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myAggregate\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxValidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mxValidate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxValidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (340,) (340,2) "
     ]
    }
   ],
   "source": [
    "print(\"Aggregation: \", np.mean(classVal == takeVote([model1,model2,model3], valData, classVal)))\n",
    "print(np.mean(classVal == np.sign(model1.predict(valData))))\n",
    "print(np.mean(classVal == np.sign(model2.predict(valData))))\n",
    "print(np.mean(classVal == np.sign(model3.predict(valData))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae3f9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x27fcba05910>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "classify = Transformer.toClassification(activitiesTrain)\n",
    "constantGuess = (len(classify[classify == 1]))/len(classify)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,8))\n",
    "\n",
    "ax.plot(history.history[\"accuracy\"], color=\"green\")\n",
    "ax.plot(history.history[\"val_accuracy\"], color=\"purple\")\n",
    "ax.axhline(constantGuess, color=\"blue\", linestyle=\"dashed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4dc3b946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "85/85 [==============================] - 1s 2ms/step - loss: 5.5849 - accuracy: 0.0000e+00\n",
      "Epoch 2/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 5.5255 - accuracy: 0.0000e+00\n",
      "Epoch 3/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 5.4748 - accuracy: 0.0000e+00\n",
      "Epoch 4/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 5.4228 - accuracy: 0.0000e+00\n",
      "Epoch 5/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 5.3735 - accuracy: 0.0000e+00\n",
      "Epoch 6/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 5.3232 - accuracy: 0.0000e+00\n",
      "Epoch 7/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 5.2726 - accuracy: 0.0000e+00\n",
      "Epoch 8/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 5.2224 - accuracy: 0.0000e+00\n",
      "Epoch 9/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 5.1732 - accuracy: 0.0000e+00\n",
      "Epoch 10/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 5.1235 - accuracy: 0.0000e+00\n",
      "Epoch 11/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 5.0735 - accuracy: 0.0000e+00\n",
      "Epoch 12/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 5.0230 - accuracy: 0.0000e+00\n",
      "Epoch 13/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 4.9713 - accuracy: 0.0000e+00\n",
      "Epoch 14/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 4.9153 - accuracy: 3.6805e-04\n",
      "Epoch 15/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 4.8592 - accuracy: 0.0040\n",
      "Epoch 16/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 4.7957 - accuracy: 0.0372\n",
      "Epoch 17/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 4.7240 - accuracy: 0.1020\n",
      "Epoch 18/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 4.6496 - accuracy: 0.1542\n",
      "Epoch 19/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 4.5786 - accuracy: 0.1932\n",
      "Epoch 20/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 4.5127 - accuracy: 0.2127\n",
      "Epoch 21/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 4.4536 - accuracy: 0.2219\n",
      "Epoch 22/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 4.3840 - accuracy: 0.2282\n",
      "Epoch 23/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 4.3062 - accuracy: 0.2363\n",
      "Epoch 24/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 4.2182 - accuracy: 0.2477\n",
      "Epoch 25/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 4.1442 - accuracy: 0.2724\n",
      "Epoch 26/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 4.0620 - accuracy: 0.3114\n",
      "Epoch 27/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 3.9743 - accuracy: 0.3522\n",
      "Epoch 28/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 3.9091 - accuracy: 0.3861\n",
      "Epoch 29/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 3.8357 - accuracy: 0.4085\n",
      "Epoch 30/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 3.7726 - accuracy: 0.4384\n",
      "Epoch 31/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 3.7092 - accuracy: 0.4582\n",
      "Epoch 32/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 3.6521 - accuracy: 0.4674\n",
      "Epoch 33/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 3.5863 - accuracy: 0.4844\n",
      "Epoch 34/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 3.5358 - accuracy: 0.4958\n",
      "Epoch 35/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 3.4821 - accuracy: 0.4994\n",
      "Epoch 36/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 3.4400 - accuracy: 0.5024\n",
      "Epoch 37/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 3.3856 - accuracy: 0.5086\n",
      "Epoch 38/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 3.3446 - accuracy: 0.5134\n",
      "Epoch 39/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 3.2937 - accuracy: 0.5193\n",
      "Epoch 40/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 3.2586 - accuracy: 0.5175\n",
      "Epoch 41/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 3.2105 - accuracy: 0.5307\n",
      "Epoch 42/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 3.1615 - accuracy: 0.5282\n",
      "Epoch 43/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 3.1209 - accuracy: 0.5326\n",
      "Epoch 44/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 3.0699 - accuracy: 0.5355\n",
      "Epoch 45/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 3.0319 - accuracy: 0.5388\n",
      "Epoch 46/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.9848 - accuracy: 0.5410\n",
      "Epoch 47/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.9432 - accuracy: 0.5418\n",
      "Epoch 48/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.9078 - accuracy: 0.5410\n",
      "Epoch 49/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.8619 - accuracy: 0.5491\n",
      "Epoch 50/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.8241 - accuracy: 0.5510\n",
      "Epoch 51/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.7866 - accuracy: 0.5477\n",
      "Epoch 52/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.7491 - accuracy: 0.5513\n",
      "Epoch 53/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.7062 - accuracy: 0.5539\n",
      "Epoch 54/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.6708 - accuracy: 0.5554\n",
      "Epoch 55/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.6319 - accuracy: 0.5550\n",
      "Epoch 56/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.5984 - accuracy: 0.5572\n",
      "Epoch 57/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.5596 - accuracy: 0.5594\n",
      "Epoch 58/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.5248 - accuracy: 0.5628\n",
      "Epoch 59/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.4867 - accuracy: 0.5642\n",
      "Epoch 60/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.4496 - accuracy: 0.5650\n",
      "Epoch 61/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.4167 - accuracy: 0.5624\n",
      "Epoch 62/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.3839 - accuracy: 0.5679\n",
      "Epoch 63/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.3486 - accuracy: 0.5657\n",
      "Epoch 64/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.3141 - accuracy: 0.5683\n",
      "Epoch 65/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.2827 - accuracy: 0.5686\n",
      "Epoch 66/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.2512 - accuracy: 0.5731\n",
      "Epoch 67/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.2192 - accuracy: 0.5720\n",
      "Epoch 68/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.1843 - accuracy: 0.5745\n",
      "Epoch 69/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.1555 - accuracy: 0.5727\n",
      "Epoch 70/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.1209 - accuracy: 0.5738\n",
      "Epoch 71/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.0890 - accuracy: 0.5771\n",
      "Epoch 72/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.0623 - accuracy: 0.5767\n",
      "Epoch 73/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.0292 - accuracy: 0.5808\n",
      "Epoch 74/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 2.0012 - accuracy: 0.5782\n",
      "Epoch 75/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.9709 - accuracy: 0.5804\n",
      "Epoch 76/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.9415 - accuracy: 0.5786\n",
      "Epoch 77/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.9160 - accuracy: 0.5812\n",
      "Epoch 78/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.8857 - accuracy: 0.5815\n",
      "Epoch 79/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.8563 - accuracy: 0.5841\n",
      "Epoch 80/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.8283 - accuracy: 0.5834\n",
      "Epoch 81/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.8032 - accuracy: 0.5804\n",
      "Epoch 82/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.7735 - accuracy: 0.5848\n",
      "Epoch 83/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.7503 - accuracy: 0.5841\n",
      "Epoch 84/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.7216 - accuracy: 0.5830\n",
      "Epoch 85/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.6957 - accuracy: 0.5848\n",
      "Epoch 86/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.6714 - accuracy: 0.5867\n",
      "Epoch 87/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.6450 - accuracy: 0.5874\n",
      "Epoch 88/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.6194 - accuracy: 0.5867\n",
      "Epoch 89/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5950 - accuracy: 0.5867\n",
      "Epoch 90/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5684 - accuracy: 0.5870\n",
      "Epoch 91/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5444 - accuracy: 0.5896\n",
      "Epoch 92/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5206 - accuracy: 0.5878\n",
      "Epoch 93/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.4968 - accuracy: 0.5885\n",
      "Epoch 94/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.4742 - accuracy: 0.5885\n",
      "Epoch 95/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.4498 - accuracy: 0.5893\n",
      "Epoch 96/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.4273 - accuracy: 0.5915\n",
      "Epoch 97/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.4064 - accuracy: 0.5900\n",
      "Epoch 98/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.3841 - accuracy: 0.5896\n",
      "Epoch 99/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.3626 - accuracy: 0.5911\n",
      "Epoch 100/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.3412 - accuracy: 0.5915\n",
      "Epoch 101/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.3190 - accuracy: 0.5907\n",
      "Epoch 102/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.2986 - accuracy: 0.5922\n",
      "Epoch 103/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.2779 - accuracy: 0.5900\n",
      "Epoch 104/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.2591 - accuracy: 0.5904\n",
      "Epoch 105/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.2376 - accuracy: 0.5918\n",
      "Epoch 106/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.2183 - accuracy: 0.5922\n",
      "Epoch 107/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.1980 - accuracy: 0.5929\n",
      "Epoch 108/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.1787 - accuracy: 0.5922\n",
      "Epoch 109/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.1603 - accuracy: 0.5911\n",
      "Epoch 110/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.1430 - accuracy: 0.5929\n",
      "Epoch 111/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.1220 - accuracy: 0.5929\n",
      "Epoch 112/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.1038 - accuracy: 0.5929\n",
      "Epoch 113/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.0853 - accuracy: 0.5929\n",
      "Epoch 114/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.0673 - accuracy: 0.5918\n",
      "Epoch 115/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.0521 - accuracy: 0.5926\n",
      "Epoch 116/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.0335 - accuracy: 0.5926\n",
      "Epoch 117/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.0163 - accuracy: 0.5933\n",
      "Epoch 118/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.9992 - accuracy: 0.5937\n",
      "Epoch 119/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.9835 - accuracy: 0.5933\n",
      "Epoch 120/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.9662 - accuracy: 0.5922\n",
      "Epoch 121/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.9525 - accuracy: 0.5937\n",
      "Epoch 122/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.9361 - accuracy: 0.5937\n",
      "Epoch 123/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.9194 - accuracy: 0.5929\n",
      "Epoch 124/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.9045 - accuracy: 0.5951\n",
      "Epoch 125/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.8901 - accuracy: 0.5944\n",
      "Epoch 126/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.8751 - accuracy: 0.5948\n",
      "Epoch 127/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.8611 - accuracy: 0.5933\n",
      "Epoch 128/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.8465 - accuracy: 0.5951\n",
      "Epoch 129/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.8329 - accuracy: 0.5948\n",
      "Epoch 130/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.8194 - accuracy: 0.5951\n",
      "Epoch 131/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.8059 - accuracy: 0.5948\n",
      "Epoch 132/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.7937 - accuracy: 0.5937\n",
      "Epoch 133/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.7808 - accuracy: 0.5955\n",
      "Epoch 134/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.7682 - accuracy: 0.5940\n",
      "Epoch 135/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.7560 - accuracy: 0.5966\n",
      "Epoch 136/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.7435 - accuracy: 0.5959\n",
      "Epoch 137/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.7328 - accuracy: 0.5962\n",
      "Epoch 138/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.7205 - accuracy: 0.5970\n",
      "Epoch 139/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.7079 - accuracy: 0.5974\n",
      "Epoch 140/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.6983 - accuracy: 0.5959\n",
      "Epoch 141/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.6866 - accuracy: 0.5985\n",
      "Epoch 142/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.6767 - accuracy: 0.5944\n",
      "Epoch 143/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.6647 - accuracy: 0.5988\n",
      "Epoch 144/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.6559 - accuracy: 0.5959\n",
      "Epoch 145/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.6448 - accuracy: 0.5988\n",
      "Epoch 146/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.6364 - accuracy: 0.5970\n",
      "Epoch 147/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.6260 - accuracy: 0.5996\n",
      "Epoch 148/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.6172 - accuracy: 0.5981\n",
      "Epoch 149/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.6072 - accuracy: 0.5996\n",
      "Epoch 150/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5983 - accuracy: 0.5988\n",
      "Epoch 151/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5899 - accuracy: 0.5992\n",
      "Epoch 152/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5811 - accuracy: 0.5992\n",
      "Epoch 153/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5731 - accuracy: 0.6003\n",
      "Epoch 154/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5656 - accuracy: 0.5985\n",
      "Epoch 155/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5585 - accuracy: 0.5996\n",
      "Epoch 156/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5506 - accuracy: 0.6007\n",
      "Epoch 157/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5433 - accuracy: 0.5988\n",
      "Epoch 158/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5361 - accuracy: 0.6014\n",
      "Epoch 159/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5295 - accuracy: 0.5996\n",
      "Epoch 160/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5223 - accuracy: 0.6003\n",
      "Epoch 161/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5164 - accuracy: 0.6025\n",
      "Epoch 162/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.6025\n",
      "Epoch 163/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5029 - accuracy: 0.6021\n",
      "Epoch 164/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4964 - accuracy: 0.6018\n",
      "Epoch 165/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4912 - accuracy: 0.6032\n",
      "Epoch 166/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4859 - accuracy: 0.6025\n",
      "Epoch 167/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4792 - accuracy: 0.6025\n",
      "Epoch 168/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4732 - accuracy: 0.6036\n",
      "Epoch 169/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4678 - accuracy: 0.6021\n",
      "Epoch 170/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4623 - accuracy: 0.6032\n",
      "Epoch 171/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4572 - accuracy: 0.6032\n",
      "Epoch 172/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4519 - accuracy: 0.6029\n",
      "Epoch 173/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4466 - accuracy: 0.6040\n",
      "Epoch 174/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4413 - accuracy: 0.6036\n",
      "Epoch 175/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4370 - accuracy: 0.6040\n",
      "Epoch 176/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4319 - accuracy: 0.6043\n",
      "Epoch 177/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4276 - accuracy: 0.6036\n",
      "Epoch 178/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4231 - accuracy: 0.6054\n",
      "Epoch 179/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4179 - accuracy: 0.6036\n",
      "Epoch 180/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4136 - accuracy: 0.6043\n",
      "Epoch 181/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4099 - accuracy: 0.6032\n",
      "Epoch 182/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4052 - accuracy: 0.6036\n",
      "Epoch 183/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4014 - accuracy: 0.6062\n",
      "Epoch 184/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3972 - accuracy: 0.6051\n",
      "Epoch 185/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3932 - accuracy: 0.6047\n",
      "Epoch 186/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.6051\n",
      "Epoch 187/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3856 - accuracy: 0.6058\n",
      "Epoch 188/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3819 - accuracy: 0.6043\n",
      "Epoch 189/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3784 - accuracy: 0.6054\n",
      "Epoch 190/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3745 - accuracy: 0.6054\n",
      "Epoch 191/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3712 - accuracy: 0.6066\n",
      "Epoch 192/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3679 - accuracy: 0.6066\n",
      "Epoch 193/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3646 - accuracy: 0.6054\n",
      "Epoch 194/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3610 - accuracy: 0.6069\n",
      "Epoch 195/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3580 - accuracy: 0.6058\n",
      "Epoch 196/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3547 - accuracy: 0.6062\n",
      "Epoch 197/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3515 - accuracy: 0.6062\n",
      "Epoch 198/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3484 - accuracy: 0.6069\n",
      "Epoch 199/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3458 - accuracy: 0.6066\n",
      "Epoch 200/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3429 - accuracy: 0.6073\n",
      "Epoch 201/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3404 - accuracy: 0.6066\n",
      "Epoch 202/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3379 - accuracy: 0.6062\n",
      "Epoch 203/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3354 - accuracy: 0.6062\n",
      "Epoch 204/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3331 - accuracy: 0.6077\n",
      "Epoch 205/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3307 - accuracy: 0.6077\n",
      "Epoch 206/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.3288 - accuracy: 0.6077\n",
      "Epoch 207/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.3261 - accuracy: 0.6077\n",
      "Epoch 208/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.3245 - accuracy: 0.6073\n",
      "Epoch 209/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3224 - accuracy: 0.6069\n",
      "Epoch 210/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.3206 - accuracy: 0.6080\n",
      "Epoch 211/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.3185 - accuracy: 0.6080\n",
      "Epoch 212/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3169 - accuracy: 0.6077\n",
      "Epoch 213/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.3149 - accuracy: 0.6080\n",
      "Epoch 214/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.3135 - accuracy: 0.6077\n",
      "Epoch 215/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.3117 - accuracy: 0.6088\n",
      "Epoch 216/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.3100 - accuracy: 0.6077\n",
      "Epoch 217/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.3087 - accuracy: 0.6080\n",
      "Epoch 218/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.3071 - accuracy: 0.6077\n",
      "Epoch 219/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3054 - accuracy: 0.6084\n",
      "Epoch 220/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3041 - accuracy: 0.6088\n",
      "Epoch 221/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3025 - accuracy: 0.6084\n",
      "Epoch 222/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.3012 - accuracy: 0.6088\n",
      "Epoch 223/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.2999 - accuracy: 0.6084\n",
      "Epoch 224/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2986 - accuracy: 0.6084\n",
      "Epoch 225/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2972 - accuracy: 0.6080\n",
      "Epoch 226/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2959 - accuracy: 0.6084\n",
      "Epoch 227/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.2948 - accuracy: 0.6084\n",
      "Epoch 228/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2935 - accuracy: 0.6088\n",
      "Epoch 229/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2923 - accuracy: 0.6088\n",
      "Epoch 230/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2912 - accuracy: 0.6088\n",
      "Epoch 231/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2899 - accuracy: 0.6084\n",
      "Epoch 232/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2888 - accuracy: 0.6088\n",
      "Epoch 233/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2877 - accuracy: 0.6095\n",
      "Epoch 234/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2865 - accuracy: 0.6091\n",
      "Epoch 235/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2858 - accuracy: 0.6084\n",
      "Epoch 236/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2845 - accuracy: 0.6091\n",
      "Epoch 237/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2835 - accuracy: 0.6091\n",
      "Epoch 238/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2825 - accuracy: 0.6088\n",
      "Epoch 239/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2814 - accuracy: 0.6091\n",
      "Epoch 240/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2805 - accuracy: 0.6095\n",
      "Epoch 241/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2794 - accuracy: 0.6095\n",
      "Epoch 242/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2784 - accuracy: 0.6080\n",
      "Epoch 243/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2775 - accuracy: 0.6084\n",
      "Epoch 244/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2764 - accuracy: 0.6088\n",
      "Epoch 245/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2755 - accuracy: 0.6088\n",
      "Epoch 246/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2744 - accuracy: 0.6095\n",
      "Epoch 247/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2735 - accuracy: 0.6088\n",
      "Epoch 248/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2725 - accuracy: 0.6091\n",
      "Epoch 249/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2717 - accuracy: 0.6095\n",
      "Epoch 250/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2707 - accuracy: 0.6088\n",
      "Epoch 251/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2700 - accuracy: 0.6091\n",
      "Epoch 252/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2690 - accuracy: 0.6095\n",
      "Epoch 253/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2681 - accuracy: 0.6091\n",
      "Epoch 254/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2673 - accuracy: 0.6091\n",
      "Epoch 255/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2663 - accuracy: 0.6091\n",
      "Epoch 256/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2656 - accuracy: 0.6095\n",
      "Epoch 257/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2647 - accuracy: 0.6091\n",
      "Epoch 258/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2639 - accuracy: 0.6091\n",
      "Epoch 259/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2630 - accuracy: 0.6095\n",
      "Epoch 260/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2622 - accuracy: 0.6091\n",
      "Epoch 261/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2614 - accuracy: 0.6091\n",
      "Epoch 262/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.6091\n",
      "Epoch 263/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2598 - accuracy: 0.6091\n",
      "Epoch 264/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2590 - accuracy: 0.6095\n",
      "Epoch 265/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2582 - accuracy: 0.6095\n",
      "Epoch 266/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2575 - accuracy: 0.6095\n",
      "Epoch 267/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2567 - accuracy: 0.6091\n",
      "Epoch 268/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2560 - accuracy: 0.6091\n",
      "Epoch 269/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2551 - accuracy: 0.6091\n",
      "Epoch 270/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2544 - accuracy: 0.6091\n",
      "Epoch 271/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2538 - accuracy: 0.6095\n",
      "Epoch 272/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2530 - accuracy: 0.6091\n",
      "Epoch 273/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2523 - accuracy: 0.6095\n",
      "Epoch 274/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2513 - accuracy: 0.6091\n",
      "Epoch 275/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2508 - accuracy: 0.6091\n",
      "Epoch 276/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2501 - accuracy: 0.6095\n",
      "Epoch 277/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2494 - accuracy: 0.6091\n",
      "Epoch 278/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2487 - accuracy: 0.6091\n",
      "Epoch 279/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2480 - accuracy: 0.6091\n",
      "Epoch 280/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2473 - accuracy: 0.6091\n",
      "Epoch 281/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2466 - accuracy: 0.6091\n",
      "Epoch 282/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2459 - accuracy: 0.6091\n",
      "Epoch 283/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.6091\n",
      "Epoch 284/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2445 - accuracy: 0.6095\n",
      "Epoch 285/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2438 - accuracy: 0.6088\n",
      "Epoch 286/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2429 - accuracy: 0.6091\n",
      "Epoch 287/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2423 - accuracy: 0.6091\n",
      "Epoch 288/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.2416 - accuracy: 0.6084\n",
      "Epoch 289/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2410 - accuracy: 0.6091\n",
      "Epoch 290/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2402 - accuracy: 0.6084\n",
      "Epoch 291/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2397 - accuracy: 0.6091\n",
      "Epoch 292/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2390 - accuracy: 0.6095\n",
      "Epoch 293/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2384 - accuracy: 0.6088\n",
      "Epoch 294/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2377 - accuracy: 0.6095\n",
      "Epoch 295/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2372 - accuracy: 0.6088\n",
      "Epoch 296/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2365 - accuracy: 0.6095\n",
      "Epoch 297/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2359 - accuracy: 0.6091\n",
      "Epoch 298/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2353 - accuracy: 0.6099\n",
      "Epoch 299/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2347 - accuracy: 0.6099\n",
      "Epoch 300/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2341 - accuracy: 0.6095\n",
      "Epoch 301/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2336 - accuracy: 0.6095\n",
      "Epoch 302/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2330 - accuracy: 0.6095\n",
      "Epoch 303/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2325 - accuracy: 0.6095\n",
      "Epoch 304/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2319 - accuracy: 0.6095\n",
      "Epoch 305/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2314 - accuracy: 0.6095\n",
      "Epoch 306/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2307 - accuracy: 0.6099\n",
      "Epoch 307/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2302 - accuracy: 0.6095\n",
      "Epoch 308/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2296 - accuracy: 0.6099\n",
      "Epoch 309/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2292 - accuracy: 0.6099\n",
      "Epoch 310/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2286 - accuracy: 0.6099\n",
      "Epoch 311/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2282 - accuracy: 0.6091\n",
      "Epoch 312/400\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.2276 - accuracy: 0.6099\n",
      "Epoch 313/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2270 - accuracy: 0.6099\n",
      "Epoch 314/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2266 - accuracy: 0.6095\n",
      "Epoch 315/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2260 - accuracy: 0.6095\n",
      "Epoch 316/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2255 - accuracy: 0.6099\n",
      "Epoch 317/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2250 - accuracy: 0.6095\n",
      "Epoch 318/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2246 - accuracy: 0.6095\n",
      "Epoch 319/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2240 - accuracy: 0.6095\n",
      "Epoch 320/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2236 - accuracy: 0.6095\n",
      "Epoch 321/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2230 - accuracy: 0.6099\n",
      "Epoch 322/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2226 - accuracy: 0.6099\n",
      "Epoch 323/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2221 - accuracy: 0.6099\n",
      "Epoch 324/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2217 - accuracy: 0.6099\n",
      "Epoch 325/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2211 - accuracy: 0.6095\n",
      "Epoch 326/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2207 - accuracy: 0.6095\n",
      "Epoch 327/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2202 - accuracy: 0.6099\n",
      "Epoch 328/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2198 - accuracy: 0.6095\n",
      "Epoch 329/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2193 - accuracy: 0.6095\n",
      "Epoch 330/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2188 - accuracy: 0.6099\n",
      "Epoch 331/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2184 - accuracy: 0.6095\n",
      "Epoch 332/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2179 - accuracy: 0.6099\n",
      "Epoch 333/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2175 - accuracy: 0.6095\n",
      "Epoch 334/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2170 - accuracy: 0.6099\n",
      "Epoch 335/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2166 - accuracy: 0.6091\n",
      "Epoch 336/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2161 - accuracy: 0.6091\n",
      "Epoch 337/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2158 - accuracy: 0.6095\n",
      "Epoch 338/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2152 - accuracy: 0.6095\n",
      "Epoch 339/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2148 - accuracy: 0.6095\n",
      "Epoch 340/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2144 - accuracy: 0.6095\n",
      "Epoch 341/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2138 - accuracy: 0.6095\n",
      "Epoch 342/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2135 - accuracy: 0.6088\n",
      "Epoch 343/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2128 - accuracy: 0.6099\n",
      "Epoch 344/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2125 - accuracy: 0.6095\n",
      "Epoch 345/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2118 - accuracy: 0.6091\n",
      "Epoch 346/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2115 - accuracy: 0.6091\n",
      "Epoch 347/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2111 - accuracy: 0.6095\n",
      "Epoch 348/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2105 - accuracy: 0.6095\n",
      "Epoch 349/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2101 - accuracy: 0.6091\n",
      "Epoch 350/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2096 - accuracy: 0.6095\n",
      "Epoch 351/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2091 - accuracy: 0.6095\n",
      "Epoch 352/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2087 - accuracy: 0.6095\n",
      "Epoch 353/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2082 - accuracy: 0.6099\n",
      "Epoch 354/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2077 - accuracy: 0.6099\n",
      "Epoch 355/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2074 - accuracy: 0.6095\n",
      "Epoch 356/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2069 - accuracy: 0.6095\n",
      "Epoch 357/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2066 - accuracy: 0.6095\n",
      "Epoch 358/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2062 - accuracy: 0.6099\n",
      "Epoch 359/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2058 - accuracy: 0.6095\n",
      "Epoch 360/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2055 - accuracy: 0.6099\n",
      "Epoch 361/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2050 - accuracy: 0.6095\n",
      "Epoch 362/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2046 - accuracy: 0.6095\n",
      "Epoch 363/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2042 - accuracy: 0.6095\n",
      "Epoch 364/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2039 - accuracy: 0.6099\n",
      "Epoch 365/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2035 - accuracy: 0.6099\n",
      "Epoch 366/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2031 - accuracy: 0.6095\n",
      "Epoch 367/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2027 - accuracy: 0.6099\n",
      "Epoch 368/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2024 - accuracy: 0.6099\n",
      "Epoch 369/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2020 - accuracy: 0.6099\n",
      "Epoch 370/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2017 - accuracy: 0.6099\n",
      "Epoch 371/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2013 - accuracy: 0.6099\n",
      "Epoch 372/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2009 - accuracy: 0.6099\n",
      "Epoch 373/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2006 - accuracy: 0.6099\n",
      "Epoch 374/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2002 - accuracy: 0.6099\n",
      "Epoch 375/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1999 - accuracy: 0.6099\n",
      "Epoch 376/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1995 - accuracy: 0.6099\n",
      "Epoch 377/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1991 - accuracy: 0.6099\n",
      "Epoch 378/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1988 - accuracy: 0.6099\n",
      "Epoch 379/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1984 - accuracy: 0.6099\n",
      "Epoch 380/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1981 - accuracy: 0.6099\n",
      "Epoch 381/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1978 - accuracy: 0.6099\n",
      "Epoch 382/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1974 - accuracy: 0.6099\n",
      "Epoch 383/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1971 - accuracy: 0.6099\n",
      "Epoch 384/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1967 - accuracy: 0.6099\n",
      "Epoch 385/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1964 - accuracy: 0.6099\n",
      "Epoch 386/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1960 - accuracy: 0.6099\n",
      "Epoch 387/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1957 - accuracy: 0.6099\n",
      "Epoch 388/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1954 - accuracy: 0.6099\n",
      "Epoch 389/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1951 - accuracy: 0.6099\n",
      "Epoch 390/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1947 - accuracy: 0.6099\n",
      "Epoch 391/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1944 - accuracy: 0.6099\n",
      "Epoch 392/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1941 - accuracy: 0.6099\n",
      "Epoch 393/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1938 - accuracy: 0.6099\n",
      "Epoch 394/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1935 - accuracy: 0.6099\n",
      "Epoch 395/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1931 - accuracy: 0.6099\n",
      "Epoch 396/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1928 - accuracy: 0.6099\n",
      "Epoch 397/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1925 - accuracy: 0.6099\n",
      "Epoch 398/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1922 - accuracy: 0.6099\n",
      "Epoch 399/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1919 - accuracy: 0.6099\n",
      "Epoch 400/400\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1915 - accuracy: 0.6099\n"
     ]
    }
   ],
   "source": [
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps=20000,\n",
    "    decay_rate=0.9)\n",
    "optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(200, input_dim=np.shape(compoundDataTrain)[1], activation='softmax', kernel_regularizer = keras.regularizers.L2(0.001)))\n",
    "model1.add(Dense(150, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model1.add(Dense(75, activation='relu', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model1.add(Dense(100, activation='relu', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model1.add(Dense(100, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model1.add(Dense(1, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "\n",
    "model1.compile(loss='MeanSquaredError', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model1.fit(compoundDataTrain, toClassification(activitiesTrain),epochs=400, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9dd37d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 1.0603 - accuracy: 0.4026\n",
      "Epoch 2/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.4133 - accuracy: 0.5370\n",
      "Epoch 3/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.3582 - accuracy: 0.5462\n",
      "Epoch 4/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.3242 - accuracy: 0.5565\n",
      "Epoch 5/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.2993 - accuracy: 0.5594\n",
      "Epoch 6/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.2772 - accuracy: 0.5668\n",
      "Epoch 7/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.2527 - accuracy: 0.5709\n",
      "Epoch 8/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.2325 - accuracy: 0.5793\n",
      "Epoch 9/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.2391 - accuracy: 0.5767\n",
      "Epoch 10/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.2306 - accuracy: 0.5797\n",
      "Epoch 11/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.2053 - accuracy: 0.5859\n",
      "Epoch 12/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.2047 - accuracy: 0.5867\n",
      "Epoch 13/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1977 - accuracy: 0.5893\n",
      "Epoch 14/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1912 - accuracy: 0.5896\n",
      "Epoch 15/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1893 - accuracy: 0.5881\n",
      "Epoch 16/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1908 - accuracy: 0.5904\n",
      "Epoch 17/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1860 - accuracy: 0.5900\n",
      "Epoch 18/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1796 - accuracy: 0.5911\n",
      "Epoch 19/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1813 - accuracy: 0.5907\n",
      "Epoch 20/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1796 - accuracy: 0.5929\n",
      "Epoch 21/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1751 - accuracy: 0.5926\n",
      "Epoch 22/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1677 - accuracy: 0.5940\n",
      "Epoch 23/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1679 - accuracy: 0.5951\n",
      "Epoch 24/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1667 - accuracy: 0.5944\n",
      "Epoch 25/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1640 - accuracy: 0.5951\n",
      "Epoch 26/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1677 - accuracy: 0.5929\n",
      "Epoch 27/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1553 - accuracy: 0.5959\n",
      "Epoch 28/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1608 - accuracy: 0.5959\n",
      "Epoch 29/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1624 - accuracy: 0.5962\n",
      "Epoch 30/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1611 - accuracy: 0.5959\n",
      "Epoch 31/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1641 - accuracy: 0.5922\n",
      "Epoch 32/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1602 - accuracy: 0.5955\n",
      "Epoch 33/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1416 - accuracy: 0.5992\n",
      "Epoch 34/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1510 - accuracy: 0.5988\n",
      "Epoch 35/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1461 - accuracy: 0.5966\n",
      "Epoch 36/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1306 - accuracy: 0.6021\n",
      "Epoch 37/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1506 - accuracy: 0.5966\n",
      "Epoch 38/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1398 - accuracy: 0.5996\n",
      "Epoch 39/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1510 - accuracy: 0.5951\n",
      "Epoch 40/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1383 - accuracy: 0.5996\n",
      "Epoch 41/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1302 - accuracy: 0.6010\n",
      "Epoch 42/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1287 - accuracy: 0.6047\n",
      "Epoch 43/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1306 - accuracy: 0.6003\n",
      "Epoch 44/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1207 - accuracy: 0.6043\n",
      "Epoch 45/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1250 - accuracy: 0.6032\n",
      "Epoch 46/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1368 - accuracy: 0.5981\n",
      "Epoch 47/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1312 - accuracy: 0.6040\n",
      "Epoch 48/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1214 - accuracy: 0.6036\n",
      "Epoch 49/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1307 - accuracy: 0.6018\n",
      "Epoch 50/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1427 - accuracy: 0.5996\n",
      "Epoch 51/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1256 - accuracy: 0.6018\n",
      "Epoch 52/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1277 - accuracy: 0.6032\n",
      "Epoch 53/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1248 - accuracy: 0.6040\n",
      "Epoch 54/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1343 - accuracy: 0.6007\n",
      "Epoch 55/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1223 - accuracy: 0.6043\n",
      "Epoch 56/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1181 - accuracy: 0.6054\n",
      "Epoch 57/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.1319 - accuracy: 0.6014\n",
      "Epoch 58/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1209 - accuracy: 0.6051\n",
      "Epoch 59/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1164 - accuracy: 0.6062\n",
      "Epoch 60/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1171 - accuracy: 0.6036\n",
      "Epoch 61/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1128 - accuracy: 0.6066\n",
      "Epoch 62/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1127 - accuracy: 0.6073\n",
      "Epoch 63/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1183 - accuracy: 0.6054\n",
      "Epoch 64/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1168 - accuracy: 0.6036\n",
      "Epoch 65/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1144 - accuracy: 0.6066\n",
      "Epoch 66/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1049 - accuracy: 0.6091\n",
      "Epoch 67/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1152 - accuracy: 0.6043\n",
      "Epoch 68/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1150 - accuracy: 0.6040\n",
      "Epoch 69/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1006 - accuracy: 0.6099\n",
      "Epoch 70/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1074 - accuracy: 0.6069\n",
      "Epoch 71/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1092 - accuracy: 0.6073\n",
      "Epoch 72/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1117 - accuracy: 0.6058\n",
      "Epoch 73/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1049 - accuracy: 0.6077\n",
      "Epoch 74/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1154 - accuracy: 0.6066\n",
      "Epoch 75/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1022 - accuracy: 0.6088\n",
      "Epoch 76/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1230 - accuracy: 0.6025\n",
      "Epoch 77/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1104 - accuracy: 0.6069\n",
      "Epoch 78/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1130 - accuracy: 0.6073\n",
      "Epoch 79/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1147 - accuracy: 0.6054\n",
      "Epoch 80/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1090 - accuracy: 0.6073\n",
      "Epoch 81/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1129 - accuracy: 0.6058\n",
      "Epoch 82/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.1095 - accuracy: 0.6066\n",
      "Epoch 83/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1090 - accuracy: 0.6077\n",
      "Epoch 84/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1047 - accuracy: 0.6069\n",
      "Epoch 85/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1103 - accuracy: 0.6080\n",
      "Epoch 86/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0976 - accuracy: 0.6095\n",
      "Epoch 87/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1180 - accuracy: 0.6043\n",
      "Epoch 88/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1113 - accuracy: 0.6073\n",
      "Epoch 89/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0961 - accuracy: 0.6110\n",
      "Epoch 90/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0982 - accuracy: 0.6091\n",
      "Epoch 91/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1006 - accuracy: 0.6099\n",
      "Epoch 92/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0891 - accuracy: 0.6132\n",
      "Epoch 93/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1083 - accuracy: 0.6073\n",
      "Epoch 94/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1231 - accuracy: 0.6062\n",
      "Epoch 95/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1153 - accuracy: 0.6040\n",
      "Epoch 96/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0967 - accuracy: 0.6095\n",
      "Epoch 97/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0878 - accuracy: 0.6110\n",
      "Epoch 98/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0950 - accuracy: 0.6106\n",
      "Epoch 99/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1114 - accuracy: 0.6051\n",
      "Epoch 100/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1160 - accuracy: 0.6036\n",
      "Epoch 101/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1057 - accuracy: 0.6084\n",
      "Epoch 102/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1062 - accuracy: 0.6066\n",
      "Epoch 103/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1145 - accuracy: 0.6051\n",
      "Epoch 104/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1114 - accuracy: 0.6051\n",
      "Epoch 105/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0980 - accuracy: 0.6091\n",
      "Epoch 106/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0859 - accuracy: 0.6117\n",
      "Epoch 107/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1179 - accuracy: 0.6062\n",
      "Epoch 108/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1120 - accuracy: 0.6040\n",
      "Epoch 109/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1017 - accuracy: 0.6102\n",
      "Epoch 110/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0858 - accuracy: 0.6132\n",
      "Epoch 111/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0974 - accuracy: 0.6088\n",
      "Epoch 112/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1207 - accuracy: 0.6054\n",
      "Epoch 113/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1023 - accuracy: 0.6084\n",
      "Epoch 114/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0992 - accuracy: 0.6091\n",
      "Epoch 115/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0993 - accuracy: 0.6099\n",
      "Epoch 116/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1054 - accuracy: 0.6069\n",
      "Epoch 117/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0993 - accuracy: 0.6095\n",
      "Epoch 118/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1120 - accuracy: 0.6047\n",
      "Epoch 119/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1021 - accuracy: 0.6069\n",
      "Epoch 120/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0872 - accuracy: 0.6124\n",
      "Epoch 121/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0819 - accuracy: 0.6128\n",
      "Epoch 122/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0832 - accuracy: 0.6110\n",
      "Epoch 123/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1154 - accuracy: 0.6043\n",
      "Epoch 124/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1086 - accuracy: 0.6054\n",
      "Epoch 125/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0899 - accuracy: 0.6099\n",
      "Epoch 126/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1060 - accuracy: 0.6054\n",
      "Epoch 127/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0947 - accuracy: 0.6099\n",
      "Epoch 128/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0902 - accuracy: 0.6088\n",
      "Epoch 129/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1161 - accuracy: 0.6073\n",
      "Epoch 130/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0931 - accuracy: 0.6088\n",
      "Epoch 131/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1004 - accuracy: 0.6066\n",
      "Epoch 132/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1066 - accuracy: 0.6058\n",
      "Epoch 133/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1040 - accuracy: 0.6054\n",
      "Epoch 134/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0934 - accuracy: 0.6088\n",
      "Epoch 135/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0950 - accuracy: 0.6099\n",
      "Epoch 136/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0992 - accuracy: 0.6088\n",
      "Epoch 137/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0870 - accuracy: 0.6099\n",
      "Epoch 138/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.6029\n",
      "Epoch 139/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0956 - accuracy: 0.6073\n",
      "Epoch 140/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0993 - accuracy: 0.6084\n",
      "Epoch 141/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1005 - accuracy: 0.6054\n",
      "Epoch 142/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1000 - accuracy: 0.6084\n",
      "Epoch 143/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0987 - accuracy: 0.6084\n",
      "Epoch 144/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0898 - accuracy: 0.6106\n",
      "Epoch 145/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0935 - accuracy: 0.6102\n",
      "Epoch 146/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0877 - accuracy: 0.6102\n",
      "Epoch 147/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1076 - accuracy: 0.6066\n",
      "Epoch 148/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1068 - accuracy: 0.6066\n",
      "Epoch 149/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1053 - accuracy: 0.6069\n",
      "Epoch 150/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1010 - accuracy: 0.6066\n",
      "Epoch 151/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1025 - accuracy: 0.6073\n",
      "Epoch 152/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1024 - accuracy: 0.6062\n",
      "Epoch 153/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0887 - accuracy: 0.6110\n",
      "Epoch 154/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1040 - accuracy: 0.6069\n",
      "Epoch 155/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0944 - accuracy: 0.6095\n",
      "Epoch 156/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0916 - accuracy: 0.6106\n",
      "Epoch 157/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0964 - accuracy: 0.6084\n",
      "Epoch 158/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0956 - accuracy: 0.6099\n",
      "Epoch 159/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1013 - accuracy: 0.6066\n",
      "Epoch 160/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0888 - accuracy: 0.6110\n",
      "Epoch 161/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0933 - accuracy: 0.6095\n",
      "Epoch 162/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0852 - accuracy: 0.6128\n",
      "Epoch 163/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1125 - accuracy: 0.6047\n",
      "Epoch 164/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0955 - accuracy: 0.6077\n",
      "Epoch 165/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0901 - accuracy: 0.6091\n",
      "Epoch 166/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0961 - accuracy: 0.6091\n",
      "Epoch 167/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0978 - accuracy: 0.6088\n",
      "Epoch 168/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0922 - accuracy: 0.6077\n",
      "Epoch 169/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0921 - accuracy: 0.6091\n",
      "Epoch 170/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1064 - accuracy: 0.6080\n",
      "Epoch 171/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0943 - accuracy: 0.6080\n",
      "Epoch 172/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0906 - accuracy: 0.6106\n",
      "Epoch 173/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1105 - accuracy: 0.6062\n",
      "Epoch 174/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0944 - accuracy: 0.6099\n",
      "Epoch 175/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1042 - accuracy: 0.6080\n",
      "Epoch 176/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0897 - accuracy: 0.6113\n",
      "Epoch 177/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0946 - accuracy: 0.6095\n",
      "Epoch 178/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1057 - accuracy: 0.6084\n",
      "Epoch 179/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0895 - accuracy: 0.6117\n",
      "Epoch 180/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0845 - accuracy: 0.6128\n",
      "Epoch 181/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0996 - accuracy: 0.6073\n",
      "Epoch 182/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0837 - accuracy: 0.6117\n",
      "Epoch 183/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0999 - accuracy: 0.6080\n",
      "Epoch 184/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1002 - accuracy: 0.6073\n",
      "Epoch 185/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0895 - accuracy: 0.6110\n",
      "Epoch 186/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0890 - accuracy: 0.6099\n",
      "Epoch 187/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1100 - accuracy: 0.6091\n",
      "Epoch 188/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0973 - accuracy: 0.6077\n",
      "Epoch 189/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0911 - accuracy: 0.6110\n",
      "Epoch 190/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1037 - accuracy: 0.6084\n",
      "Epoch 191/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0957 - accuracy: 0.6088\n",
      "Epoch 192/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1030 - accuracy: 0.6077\n",
      "Epoch 193/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0921 - accuracy: 0.6095\n",
      "Epoch 194/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0958 - accuracy: 0.6102\n",
      "Epoch 195/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0886 - accuracy: 0.6095\n",
      "Epoch 196/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1058 - accuracy: 0.6080\n",
      "Epoch 197/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0900 - accuracy: 0.6110\n",
      "Epoch 198/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1007 - accuracy: 0.6073\n",
      "Epoch 199/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0789 - accuracy: 0.6128\n",
      "Epoch 200/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1027 - accuracy: 0.6066\n",
      "Epoch 201/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.0998 - accuracy: 0.6080\n",
      "Epoch 202/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.0898 - accuracy: 0.6091\n",
      "Epoch 203/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1116 - accuracy: 0.6080\n",
      "Epoch 204/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0952 - accuracy: 0.6099\n",
      "Epoch 205/400\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.0911 - accuracy: 0.6099\n",
      "Epoch 206/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0944 - accuracy: 0.6102\n",
      "Epoch 207/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0859 - accuracy: 0.6128\n",
      "Epoch 208/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0827 - accuracy: 0.6121\n",
      "Epoch 209/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.0935 - accuracy: 0.6106\n",
      "Epoch 210/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.0965 - accuracy: 0.6080\n",
      "Epoch 211/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.1059 - accuracy: 0.6073\n",
      "Epoch 212/400\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.0934 - accuracy: 0.6095\n",
      "Epoch 213/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.0991 - accuracy: 0.6091\n",
      "Epoch 214/400\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.0851 - accuracy: 0.6117\n",
      "Epoch 215/400\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.0974 - accuracy: 0.6091\n",
      "Epoch 216/400\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.1043 - accuracy: 0.6077\n",
      "Epoch 217/400\n",
      "680/680 [==============================] - 2s 3ms/step - loss: 0.0921 - accuracy: 0.6099\n",
      "Epoch 218/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.0950 - accuracy: 0.6095\n",
      "Epoch 219/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0891 - accuracy: 0.6084\n",
      "Epoch 220/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.1041 - accuracy: 0.6069\n",
      "Epoch 221/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1008 - accuracy: 0.6091\n",
      "Epoch 222/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0966 - accuracy: 0.6084\n",
      "Epoch 223/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0851 - accuracy: 0.6110\n",
      "Epoch 224/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0926 - accuracy: 0.6124\n",
      "Epoch 225/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0909 - accuracy: 0.6095\n",
      "Epoch 226/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0977 - accuracy: 0.6077\n",
      "Epoch 227/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0957 - accuracy: 0.6099\n",
      "Epoch 228/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0875 - accuracy: 0.6113\n",
      "Epoch 229/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0973 - accuracy: 0.6088\n",
      "Epoch 230/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0879 - accuracy: 0.6113\n",
      "Epoch 231/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0918 - accuracy: 0.6088\n",
      "Epoch 232/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0934 - accuracy: 0.6095\n",
      "Epoch 233/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0845 - accuracy: 0.6128\n",
      "Epoch 234/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1026 - accuracy: 0.6091\n",
      "Epoch 235/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1031 - accuracy: 0.6077\n",
      "Epoch 236/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1108 - accuracy: 0.6073\n",
      "Epoch 237/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0960 - accuracy: 0.6088\n",
      "Epoch 238/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0934 - accuracy: 0.6102\n",
      "Epoch 239/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0929 - accuracy: 0.6091\n",
      "Epoch 240/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0886 - accuracy: 0.6102\n",
      "Epoch 241/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0949 - accuracy: 0.6091\n",
      "Epoch 242/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.0922 - accuracy: 0.6102\n",
      "Epoch 243/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0985 - accuracy: 0.6073\n",
      "Epoch 244/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0880 - accuracy: 0.6117\n",
      "Epoch 245/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0850 - accuracy: 0.6121\n",
      "Epoch 246/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0993 - accuracy: 0.6099\n",
      "Epoch 247/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0949 - accuracy: 0.6110\n",
      "Epoch 248/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0925 - accuracy: 0.6117\n",
      "Epoch 249/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1003 - accuracy: 0.6099\n",
      "Epoch 250/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0967 - accuracy: 0.6077\n",
      "Epoch 251/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0969 - accuracy: 0.6066\n",
      "Epoch 252/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0923 - accuracy: 0.6102\n",
      "Epoch 253/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0856 - accuracy: 0.6110\n",
      "Epoch 254/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0982 - accuracy: 0.6073\n",
      "Epoch 255/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0874 - accuracy: 0.6124\n",
      "Epoch 256/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0987 - accuracy: 0.6077\n",
      "Epoch 257/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0909 - accuracy: 0.6106\n",
      "Epoch 258/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0913 - accuracy: 0.6091\n",
      "Epoch 259/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0958 - accuracy: 0.6084\n",
      "Epoch 260/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0867 - accuracy: 0.6117\n",
      "Epoch 261/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0972 - accuracy: 0.6073\n",
      "Epoch 262/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1037 - accuracy: 0.6062\n",
      "Epoch 263/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1003 - accuracy: 0.6084\n",
      "Epoch 264/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0856 - accuracy: 0.6110\n",
      "Epoch 265/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0860 - accuracy: 0.6117\n",
      "Epoch 266/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1033 - accuracy: 0.6099\n",
      "Epoch 267/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1032 - accuracy: 0.6073\n",
      "Epoch 268/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1051 - accuracy: 0.6077\n",
      "Epoch 269/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0907 - accuracy: 0.6106\n",
      "Epoch 270/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0920 - accuracy: 0.6091\n",
      "Epoch 271/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1041 - accuracy: 0.6069\n",
      "Epoch 272/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0822 - accuracy: 0.6132\n",
      "Epoch 273/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0832 - accuracy: 0.6128\n",
      "Epoch 274/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0937 - accuracy: 0.6088\n",
      "Epoch 275/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1066 - accuracy: 0.6054\n",
      "Epoch 276/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0867 - accuracy: 0.6113\n",
      "Epoch 277/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0803 - accuracy: 0.6128\n",
      "Epoch 278/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0915 - accuracy: 0.6102\n",
      "Epoch 279/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1052 - accuracy: 0.6073\n",
      "Epoch 280/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.1009 - accuracy: 0.6058\n",
      "Epoch 281/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0918 - accuracy: 0.6110\n",
      "Epoch 282/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0840 - accuracy: 0.6117\n",
      "Epoch 283/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0920 - accuracy: 0.6102\n",
      "Epoch 284/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0947 - accuracy: 0.6099\n",
      "Epoch 285/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0948 - accuracy: 0.6080\n",
      "Epoch 286/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0850 - accuracy: 0.6113\n",
      "Epoch 287/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0838 - accuracy: 0.6124\n",
      "Epoch 288/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0804 - accuracy: 0.6128\n",
      "Epoch 289/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1064 - accuracy: 0.6073\n",
      "Epoch 290/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0909 - accuracy: 0.6080\n",
      "Epoch 291/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0863 - accuracy: 0.6124\n",
      "Epoch 292/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0861 - accuracy: 0.6106\n",
      "Epoch 293/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0900 - accuracy: 0.6102\n",
      "Epoch 294/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1139 - accuracy: 0.6043\n",
      "Epoch 295/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0994 - accuracy: 0.6088\n",
      "Epoch 296/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0981 - accuracy: 0.6069\n",
      "Epoch 297/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0902 - accuracy: 0.6106\n",
      "Epoch 298/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0804 - accuracy: 0.6128\n",
      "Epoch 299/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0923 - accuracy: 0.6095\n",
      "Epoch 300/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1066 - accuracy: 0.6047\n",
      "Epoch 301/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1037 - accuracy: 0.6066\n",
      "Epoch 302/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.0929 - accuracy: 0.6110\n",
      "Epoch 303/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0880 - accuracy: 0.6110\n",
      "Epoch 304/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0882 - accuracy: 0.6110\n",
      "Epoch 305/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0965 - accuracy: 0.6058\n",
      "Epoch 306/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0873 - accuracy: 0.6124\n",
      "Epoch 307/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0900 - accuracy: 0.6102\n",
      "Epoch 308/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1018 - accuracy: 0.6066\n",
      "Epoch 309/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0892 - accuracy: 0.6102\n",
      "Epoch 310/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0861 - accuracy: 0.6110\n",
      "Epoch 311/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0836 - accuracy: 0.6124\n",
      "Epoch 312/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0931 - accuracy: 0.6077\n",
      "Epoch 313/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1126 - accuracy: 0.6040\n",
      "Epoch 314/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0891 - accuracy: 0.6106\n",
      "Epoch 315/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0960 - accuracy: 0.6095\n",
      "Epoch 316/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.0821 - accuracy: 0.6132\n",
      "Epoch 317/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0863 - accuracy: 0.6113\n",
      "Epoch 318/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1039 - accuracy: 0.6073\n",
      "Epoch 319/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0882 - accuracy: 0.6088\n",
      "Epoch 320/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0953 - accuracy: 0.6084\n",
      "Epoch 321/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0998 - accuracy: 0.6080\n",
      "Epoch 322/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.0840 - accuracy: 0.6106\n",
      "Epoch 323/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0917 - accuracy: 0.6102\n",
      "Epoch 324/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0853 - accuracy: 0.6121\n",
      "Epoch 325/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1001 - accuracy: 0.6084\n",
      "Epoch 326/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0939 - accuracy: 0.6113\n",
      "Epoch 327/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0892 - accuracy: 0.6102\n",
      "Epoch 328/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0898 - accuracy: 0.6110\n",
      "Epoch 329/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0954 - accuracy: 0.6091\n",
      "Epoch 330/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0860 - accuracy: 0.6106\n",
      "Epoch 331/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1021 - accuracy: 0.6069\n",
      "Epoch 332/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0906 - accuracy: 0.6099\n",
      "Epoch 333/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0974 - accuracy: 0.6128\n",
      "Epoch 334/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1013 - accuracy: 0.6080\n",
      "Epoch 335/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1024 - accuracy: 0.6069\n",
      "Epoch 336/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0915 - accuracy: 0.6099\n",
      "Epoch 337/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0925 - accuracy: 0.6099\n",
      "Epoch 338/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0874 - accuracy: 0.6121\n",
      "Epoch 339/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0849 - accuracy: 0.6113\n",
      "Epoch 340/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1010 - accuracy: 0.6084\n",
      "Epoch 341/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0999 - accuracy: 0.6073\n",
      "Epoch 342/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0951 - accuracy: 0.6080\n",
      "Epoch 343/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0943 - accuracy: 0.6106\n",
      "Epoch 344/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0833 - accuracy: 0.6124\n",
      "Epoch 345/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0979 - accuracy: 0.6095\n",
      "Epoch 346/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0927 - accuracy: 0.6095\n",
      "Epoch 347/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1010 - accuracy: 0.6073\n",
      "Epoch 348/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0852 - accuracy: 0.6110\n",
      "Epoch 349/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0925 - accuracy: 0.6113\n",
      "Epoch 350/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0879 - accuracy: 0.6113\n",
      "Epoch 351/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0832 - accuracy: 0.6102\n",
      "Epoch 352/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1206 - accuracy: 0.6043\n",
      "Epoch 353/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1012 - accuracy: 0.6080\n",
      "Epoch 354/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0930 - accuracy: 0.6106\n",
      "Epoch 355/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0898 - accuracy: 0.6102\n",
      "Epoch 356/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1013 - accuracy: 0.6073\n",
      "Epoch 357/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0863 - accuracy: 0.6113\n",
      "Epoch 358/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0967 - accuracy: 0.6091\n",
      "Epoch 359/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0982 - accuracy: 0.6088\n",
      "Epoch 360/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0869 - accuracy: 0.6095\n",
      "Epoch 361/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1018 - accuracy: 0.6077\n",
      "Epoch 362/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0960 - accuracy: 0.6080\n",
      "Epoch 363/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0844 - accuracy: 0.6106\n",
      "Epoch 364/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0944 - accuracy: 0.6102\n",
      "Epoch 365/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1142 - accuracy: 0.6058\n",
      "Epoch 366/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0859 - accuracy: 0.6110\n",
      "Epoch 367/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.0900 - accuracy: 0.6095\n",
      "Epoch 368/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0834 - accuracy: 0.6124\n",
      "Epoch 369/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0813 - accuracy: 0.6117\n",
      "Epoch 370/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1034 - accuracy: 0.6066\n",
      "Epoch 371/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.1016 - accuracy: 0.6069\n",
      "Epoch 372/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.0822 - accuracy: 0.6132\n",
      "Epoch 373/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0902 - accuracy: 0.6117\n",
      "Epoch 374/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0941 - accuracy: 0.6106\n",
      "Epoch 375/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1136 - accuracy: 0.6058\n",
      "Epoch 376/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0924 - accuracy: 0.6091\n",
      "Epoch 377/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0908 - accuracy: 0.6110\n",
      "Epoch 378/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0878 - accuracy: 0.6099\n",
      "Epoch 379/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0945 - accuracy: 0.6091\n",
      "Epoch 380/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0977 - accuracy: 0.6084\n",
      "Epoch 381/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0935 - accuracy: 0.6095\n",
      "Epoch 382/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.0807 - accuracy: 0.6117\n",
      "Epoch 383/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0939 - accuracy: 0.6095\n",
      "Epoch 384/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1011 - accuracy: 0.6084\n",
      "Epoch 385/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0886 - accuracy: 0.6106\n",
      "Epoch 386/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0882 - accuracy: 0.6117\n",
      "Epoch 387/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0947 - accuracy: 0.6091\n",
      "Epoch 388/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.0973 - accuracy: 0.6073\n",
      "Epoch 389/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0959 - accuracy: 0.6077\n",
      "Epoch 390/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0907 - accuracy: 0.6117\n",
      "Epoch 391/400\n",
      "680/680 [==============================] - 2s 2ms/step - loss: 0.0822 - accuracy: 0.6113\n",
      "Epoch 392/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0822 - accuracy: 0.6121\n",
      "Epoch 393/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1005 - accuracy: 0.6054\n",
      "Epoch 394/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0905 - accuracy: 0.6106\n",
      "Epoch 395/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0863 - accuracy: 0.6106\n",
      "Epoch 396/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0836 - accuracy: 0.6117\n",
      "Epoch 397/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1007 - accuracy: 0.6077\n",
      "Epoch 398/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.1059 - accuracy: 0.6066\n",
      "Epoch 399/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0844 - accuracy: 0.6117\n",
      "Epoch 400/400\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.0865 - accuracy: 0.6110\n"
     ]
    }
   ],
   "source": [
    "classTrain = Transformer.toClassification(activitiesTrain)\n",
    "classVal = Transformer.toClassification(activitiesValidate)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.01,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False\n",
    ")\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "l1Reg = 0.001\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(200, input_dim=np.shape(compoundDataTrain)[1], activation='softmax', kernel_regularizer = keras.regularizers.L2(l1Reg)))\n",
    "model2.add(Dense(150, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model2.add(Dense(75, activation='relu', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model2.add(Dense(100, activation='relu', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model2.add(Dense(100, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model2.add(Dense(1, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "\n",
    "model2.compile(loss='MeanSquaredError', optimizer=\"adam\", metrics=['accuracy'])\n",
    "history = model2.fit(compoundDataTrain, Transformer.toClassification(activitiesTrain), epochs=400, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "09e1d532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "85/85 [==============================] - 1s 2ms/step - loss: 1.8043 - accuracy: 0.6198\n",
      "Epoch 2/100\n",
      "85/85 [==============================] - 0s 4ms/step - loss: 1.5772 - accuracy: 0.6198\n",
      "Epoch 3/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.5297 - accuracy: 0.6198\n",
      "Epoch 4/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.5219 - accuracy: 0.6198\n",
      "Epoch 5/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5209 - accuracy: 0.6198\n",
      "Epoch 6/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 7/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 8/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 9/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 10/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 11/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 12/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 13/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 14/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 15/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 16/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 17/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 18/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 19/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 20/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 21/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 22/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 23/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 24/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 25/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 26/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 27/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 28/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 29/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 30/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 31/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 32/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 33/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 34/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 35/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 36/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 37/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 38/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 39/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 40/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 41/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 42/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 43/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 44/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 45/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 46/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 47/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 48/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 49/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 50/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 51/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 52/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 53/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 54/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 55/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 56/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 57/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 58/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 59/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 60/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 61/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 62/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 63/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 64/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 65/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 66/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 67/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 68/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 69/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 70/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 71/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 72/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 73/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 74/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 75/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 76/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 77/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 78/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 79/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 80/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 81/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 82/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 84/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 85/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 86/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 87/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 88/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 89/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 90/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 91/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 92/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 93/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 94/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 95/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 96/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 97/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 98/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 99/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n",
      "Epoch 100/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5208 - accuracy: 0.6198\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(50, input_dim=np.shape(compoundDataTrain)[1], activation='softmax', kernel_regularizer = keras.regularizers.L2(0.001)))\n",
    "model3.add(Dense(100, activation='softmax', kernel_regularizer = keras.regularizers.L2(0.001)))\n",
    "model3.add(Dense(50, activation='relu', kernel_regularizer = keras.regularizers.L2(0.001)))\n",
    "model3.add(Dense(75, activation='relu', kernel_regularizer = keras.regularizers.L2(0.001)))\n",
    "model3.add(Dense(100, activation='relu', kernel_regularizer = keras.regularizers.L2(0.001)))\n",
    "model3.add(Dense(75, activation='relu', kernel_regularizer = keras.regularizers.L2(0.001)))\n",
    "model3.add(Dense(100, activation='relu', kernel_regularizer = keras.regularizers.L2(0.001)))\n",
    "model3.add(Dense(1, activation='softmax', kernel_regularizer = keras.regularizers.L2(0.001)))\n",
    "\n",
    "model3.compile(loss='MeanSquaredError', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model3.fit(compoundDataTrain, toClassification(activitiesTrain), epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "92288cff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "Aggregation:  0.5276124567474049\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "0.5216955017301038\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "0.5223529411764706\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "0.611764705882353\n"
     ]
    }
   ],
   "source": [
    "print(\"Aggregation: \", np.mean(toClassification(activitiesValidate) == takeVote([model1,model2,model3], compoundDataValidate, toClassification(activitiesValidate))))\n",
    "print(np.mean(toClassification(activitiesValidate) == np.sign(model1.predict(compoundDataValidate))))\n",
    "print(np.mean(toClassification(activitiesValidate) == np.sign(model2.predict(compoundDataValidate))))\n",
    "print(np.mean(toClassification(activitiesValidate) == np.sign(model3.predict(compoundDataValidate))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded82795",
   "metadata": {},
   "source": [
    "# Using the PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "36ee06a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcut2d retention: [0.99364773]\n",
      "\ttotal: 99.36477273412339%\n",
      "chi retention: [0.9541968]\n",
      "\ttotal: 95.41967968597889%\n",
      "paoe retention: [0.31495127 0.19509321 0.1390042  0.08838179 0.07155396 0.04590061\n",
      " 0.03243    0.02673268 0.02402293]\n",
      "\ttotal: 93.80706737004672%\n",
      "smr retention: [0.50634726 0.24568468 0.08792319 0.07435827]\n",
      "\ttotal: 91.43133907840063%\n",
      "slogp retention: [0.44620276 0.22134465 0.15753302 0.04213752 0.03159298 0.02850562]\n",
      "\ttotal: 92.73165471968964%\n",
      "estate_vsa retention: [0.29224011 0.18964809 0.14361318 0.10642656 0.07394495 0.06391846\n",
      " 0.05513575]\n",
      "\ttotal: 92.49271047439834%\n",
      "vsa_estate retention: [0.49255304 0.32866098 0.09719245]\n",
      "\ttotal: 91.84064669216797%\n",
      "fr retention: [0.32153498 0.12578177 0.09944384 0.0570485  0.05494644 0.04503917\n",
      " 0.03646285 0.02896244 0.02597271 0.02289905 0.01957315 0.01726897\n",
      " 0.01514305 0.01174696 0.01137838 0.00991623]\n",
      "\ttotal: 90.31185029428742%\n",
      "(2717, 95)\n",
      "0 \b:\t docking_score_max \t [1.8401924  1.40725282 0.10843408]\n",
      "1 \b:\t fusion_score_max \t [0.04179651 0.94715309 0.36833644]\n",
      "2 \b:\t maxestateindex \t [ 0.47106969 -0.39012844 -0.01682807]\n",
      "3 \b:\t minestateindex \t [-0.82162669  0.66805633  0.68625441]\n",
      "4 \b:\t maxabsestateindex \t [ 0.47106969 -0.39012844 -0.01682807]\n",
      "5 \b:\t minabsestateindex \t [0.21141959 1.09317326 0.04827083]\n",
      "6 \b:\t qed \t [-1.15591823 -0.59280689  0.85035599]\n",
      "7 \b:\t molwt \t [ 1.99801031 -0.06443776 -0.71012253]\n",
      "8 \b:\t heavyatommolwt \t [ 1.93030512 -0.11695211 -0.65392929]\n",
      "9 \b:\t exactmolwt \t [ 1.99574335 -0.06216303 -0.71565145]\n",
      "10 \b:\t numvalenceelectrons \t [ 1.96121366  0.07013939 -0.91837671]\n",
      "11 \b:\t numradicalelectrons \t [0. 0. 0.]\n",
      "12 \b:\t maxpartialcharge \t [-0.01918824 -0.01918824 -0.01918824]\n",
      "13 \b:\t minpartialcharge \t [-0.32451733 -0.4218597  -0.86702629]\n",
      "14 \b:\t maxabspartialcharge \t [-0.01918824 -0.01918824 -0.01918824]\n",
      "15 \b:\t minabspartialcharge \t [ 1.7818052   0.75171384 -0.76437674]\n",
      "16 \b:\t fpdensitymorgan1 \t [-0.17784179 -1.04453865  0.26762505]\n",
      "17 \b:\t fpdensitymorgan2 \t [-0.08737051 -0.78050444  0.63589968]\n",
      "18 \b:\t fpdensitymorgan3 \t [-0.05967969 -0.3359487   1.12143217]\n",
      "19 \b:\t bcut2d_0 \t [ 0.56823354 -1.10778048  0.56153227]\n",
      "20 \b:\t balabanj \t [-1.13058364 -0.32511406 -0.01486306]\n",
      "21 \b:\t bertzct \t [ 0.83984051  0.79847155 -0.40031737]\n",
      "22 \b:\t chi_0 \t [ 2.09115283  0.18997116 -0.87708089]\n",
      "23 \b:\t hallkieralpha \t [0.35978559 0.04680885 0.7770879 ]\n",
      "24 \b:\t ipc \t [-0.01922516 -0.01922525 -0.01922525]\n",
      "25 \b:\t kappa1 \t [ 1.91913447 -0.12207164 -0.95788224]\n",
      "26 \b:\t kappa2 \t [ 1.87390305 -0.2456708  -0.91775003]\n",
      "27 \b:\t kappa3 \t [ 1.88104706 -0.44739161 -0.98678511]\n",
      "28 \b:\t labuteasa \t [ 1.92861571  0.04690599 -0.72822654]\n",
      "29 \b:\t paoe_0 \t [ 1.7715528  -1.06384524  0.09761968]\n",
      "30 \b:\t paoe_1 \t [ 1.57532802  1.82127068 -2.07120945]\n",
      "31 \b:\t paoe_2 \t [-0.13695413  0.47732132  0.01573759]\n",
      "32 \b:\t paoe_3 \t [-0.14671787  1.95577123  0.42694656]\n",
      "33 \b:\t paoe_4 \t [-0.02969803 -0.8198314   1.23168777]\n",
      "34 \b:\t paoe_5 \t [ 0.53363795 -0.39443345 -0.42896462]\n",
      "35 \b:\t paoe_6 \t [ 0.04218307  1.19519689 -0.01025929]\n",
      "36 \b:\t paoe_7 \t [ 2.39130787  1.4642419  -0.26943893]\n",
      "37 \b:\t paoe_8 \t [-1.10108457  1.12829391 -0.03790073]\n",
      "38 \b:\t smr_0 \t [ 2.3494224  -0.36872345 -0.7931771 ]\n",
      "39 \b:\t smr_1 \t [ 0.84729418 -0.17927662 -0.34441144]\n",
      "40 \b:\t smr_2 \t [0.90558858 0.71730896 0.08903048]\n",
      "41 \b:\t smr_3 \t [-0.57758111  1.71922476 -1.23408327]\n",
      "42 \b:\t slogp_0 \t [ 2.34317973 -0.19238882 -1.06083782]\n",
      "43 \b:\t slogp_1 \t [ 0.48454409 -0.39095267 -0.6095749 ]\n",
      "44 \b:\t slogp_2 \t [-0.17521691  0.49342969 -0.66728402]\n",
      "45 \b:\t slogp_3 \t [ 1.28267141 -2.31144628  1.03428237]\n",
      "46 \b:\t slogp_4 \t [0.06385772 1.84262372 1.2797733 ]\n",
      "47 \b:\t slogp_5 \t [ 0.08483765 -0.32767368  0.40323626]\n",
      "48 \b:\t tpsa \t [ 1.2501606  -0.30653067 -1.11606705]\n",
      "49 \b:\t estate_vsa_0 \t [ 0.59394763 -1.30948619 -0.80599062]\n",
      "50 \b:\t estate_vsa_1 \t [ 1.63009257  0.91865585 -0.80749988]\n",
      "51 \b:\t estate_vsa_2 \t [ 0.28357391  2.41729163 -0.73671368]\n",
      "52 \b:\t estate_vsa_3 \t [2.68278065 2.22507577 0.05200186]\n",
      "53 \b:\t estate_vsa_4 \t [-0.49342337 -1.13144319 -0.84380262]\n",
      "54 \b:\t estate_vsa_5 \t [-1.83034    -0.04888996 -0.88537305]\n",
      "55 \b:\t estate_vsa_6 \t [ 0.58401521  1.46321122 -0.51464791]\n",
      "56 \b:\t vsa_estate_0 \t [ 0.42276422 -0.1287576  -0.38442719]\n",
      "57 \b:\t vsa_estate_1 \t [ 1.30131507 -0.63381469 -0.96981614]\n",
      "58 \b:\t vsa_estate_2 \t [-0.42725676  0.317791   -0.27063666]\n",
      "59 \b:\t fractioncsp3 \t [ 1.79907176  0.14664095 -1.01949257]\n",
      "60 \b:\t heavyatomcount \t [ 1.77698155  0.11426325 -0.83586149]\n",
      "61 \b:\t nhohcount \t [ 0.58956448  0.58956448 -0.68073424]\n",
      "62 \b:\t nocount \t [ 1.3230271  -0.12658057 -1.21378633]\n",
      "63 \b:\t numaliphaticcarbocycles \t [ 2.49426594 -0.52172007 -0.52172007]\n",
      "64 \b:\t numaliphaticheterocycles \t [1.43572912 0.10754525 0.10754525]\n",
      "65 \b:\t numaliphaticrings \t [ 2.55668969 -0.24772125 -0.24772125]\n",
      "66 \b:\t numaromaticcarbocycles \t [-0.6783444  -0.6783444   0.54061178]\n",
      "67 \b:\t numaromaticheterocycles \t [-1.22685468  2.71174025  0.08601029]\n",
      "68 \b:\t numaromaticrings \t [-1.33342254  1.34955132  0.4552267 ]\n",
      "69 \b:\t numhacceptors \t [ 0.79090621 -0.12624794 -1.04340208]\n",
      "70 \b:\t numhdonors \t [ 0.68607865  0.68607865 -0.66911996]\n",
      "71 \b:\t numheteroatoms \t [ 1.37863436 -0.56300688 -0.88661375]\n",
      "72 \b:\t numrotatablebonds \t [ 1.46014819 -0.42158094 -1.22803628]\n",
      "73 \b:\t numsaturatedcarbocycles \t [ 2.74355908 -0.50306549 -0.50306549]\n",
      "74 \b:\t numsaturatedheterocycles \t [ 1.93455293  0.51817451 -0.89820391]\n",
      "75 \b:\t numsaturatedrings \t [ 2.88453723  0.05280617 -0.89110418]\n",
      "76 \b:\t ringcount \t [1.02512286 1.02512286 0.20108179]\n",
      "77 \b:\t mollogp \t [ 0.42279694 -1.50976599  0.96203042]\n",
      "78 \b:\t molmr \t [ 1.89356375  0.19369986 -0.70163143]\n",
      "79 \b:\t fr_0 \t [ 1.44263174 -0.95429867 -0.55803743]\n",
      "80 \b:\t fr_1 \t [-0.16649715 -1.17342811 -0.53915678]\n",
      "81 \b:\t fr_2 \t [ 0.10048759 -0.21746896  0.70765117]\n",
      "82 \b:\t fr_3 \t [ 0.27308047 -1.30120346 -0.48403785]\n",
      "83 \b:\t fr_4 \t [-0.67198613 -0.39193125  1.10729028]\n",
      "84 \b:\t fr_5 \t [-0.41927633  0.75862047  0.19625445]\n",
      "85 \b:\t fr_6 \t [-0.18861713  1.42095493 -0.45381476]\n",
      "86 \b:\t fr_7 \t [0.05433694 0.80557447 0.04596197]\n",
      "87 \b:\t fr_8 \t [-0.39468002 -0.20130347 -0.37106788]\n",
      "88 \b:\t fr_9 \t [-0.0991188   0.79935111  0.07799913]\n",
      "89 \b:\t fr_10 \t [ 0.15272125 -0.91387713 -0.46979395]\n",
      "90 \b:\t fr_11 \t [-0.16007092  0.62335832 -1.14311677]\n",
      "91 \b:\t fr_12 \t [-0.26588733  0.91860171 -0.57174036]\n",
      "92 \b:\t fr_13 \t [-0.5495537  -4.30726115 -0.37486007]\n",
      "93 \b:\t fr_14 \t [ 0.35791729  2.71423905 -0.05892239]\n",
      "94 \b:\t fr_15 \t [ 1.7689255   0.33653336 -0.03693436]\n"
     ]
    }
   ],
   "source": [
    "compoundsTrain, smilesTrain, labelsTrain, compoundDataTrain, activitiesTrain = Loader.getTrain(defaultValue=0)\n",
    "compoundsTest, smilesTest, labelsTest, compoundDataTest, activitiesTest = Loader.getTest(defaultValue=0)\n",
    "compoundsValidate, smilesValidate, labelsValidate, compoundDataValidate, activitiesValidate = Loader.getValidate(defaultValue=0)\n",
    "\n",
    "\n",
    "labelsPCA, trainPCA, testPCA, valPCA = Transformer.applyPCA(labelsTrain,  compoundDataTrain, \n",
    "                                                            compoundDataTest, compoundDataValidate,\n",
    "                                                            endDims=[1,1,9,4,6,7,3,16])\n",
    "\n",
    "labelsMeanPCA, trainMeanPCA = Transformer.useAverageFD(labelsPCA, trainPCA)\n",
    "_, testMeanPCA = Transformer.useAverageFD(labelsPCA, testPCA)\n",
    "_, valMeanPCA = Transformer.useAverageFD(labelsPCA, valPCA)\n",
    "\n",
    "labelsMaxPCA, trainMaxPCA = Transformer.useMaxFD(labelsPCA, trainPCA)\n",
    "_, testMaxPCA = Transformer.useMaxFD(labelsPCA, testPCA)\n",
    "_, valMaxPCA = Transformer.useMaxFD(labelsPCA, valPCA)\n",
    "\n",
    "#after transformations are done assign data\n",
    "dataLabels = labelsMaxPCA\n",
    "trainData = trainMaxPCA\n",
    "testData = testMaxPCA\n",
    "valData = valMaxPCA\n",
    "\n",
    "trainData, testData, valData = Transformer.normalizeData(trainData, testData, valData, newMean=0, newStd=1)\n",
    "\n",
    "print(np.shape(trainData))\n",
    "for i in range(len(dataLabels)):\n",
    "    print(i, \"\\b:\\t\", dataLabels[i], \"\\t\", trainData[0:3,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14325779",
   "metadata": {},
   "source": [
    "# Training on Fr data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cfcceee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "85/85 [==============================] - 1s 2ms/step - loss: 2.5404 - accuracy: 0.3890\n",
      "Epoch 2/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.7911 - accuracy: 0.4921\n",
      "Epoch 3/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.3597 - accuracy: 0.5127\n",
      "Epoch 4/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.1017 - accuracy: 0.5171\n",
      "Epoch 5/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.9131 - accuracy: 0.5234\n",
      "Epoch 6/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.7799 - accuracy: 0.5230\n",
      "Epoch 7/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.6949 - accuracy: 0.5271\n",
      "Epoch 8/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.6241 - accuracy: 0.5278\n",
      "Epoch 9/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5795 - accuracy: 0.5274\n",
      "Epoch 10/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.5348\n",
      "Epoch 11/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.5366\n",
      "Epoch 12/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4830 - accuracy: 0.5385\n",
      "Epoch 13/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4625 - accuracy: 0.5388\n",
      "Epoch 14/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4535 - accuracy: 0.5436\n",
      "Epoch 15/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4369 - accuracy: 0.5385\n",
      "Epoch 16/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4138 - accuracy: 0.5455\n",
      "Epoch 17/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3995 - accuracy: 0.5462\n",
      "Epoch 18/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.5451\n",
      "Epoch 19/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3735 - accuracy: 0.5547\n",
      "Epoch 20/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3700 - accuracy: 0.5506\n",
      "Epoch 21/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3587 - accuracy: 0.5521\n",
      "Epoch 22/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3573 - accuracy: 0.5517\n",
      "Epoch 23/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3484 - accuracy: 0.5572\n",
      "Epoch 24/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3508 - accuracy: 0.5532\n",
      "Epoch 25/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3445 - accuracy: 0.5539\n",
      "Epoch 26/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3287 - accuracy: 0.5631\n",
      "Epoch 27/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3177 - accuracy: 0.5628\n",
      "Epoch 28/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3146 - accuracy: 0.5672\n",
      "Epoch 29/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3102 - accuracy: 0.5661\n",
      "Epoch 30/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3013 - accuracy: 0.5675\n",
      "Epoch 31/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3031 - accuracy: 0.5650\n",
      "Epoch 32/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3103 - accuracy: 0.5686\n",
      "Epoch 33/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2903 - accuracy: 0.5734\n",
      "Epoch 34/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2848 - accuracy: 0.5709\n",
      "Epoch 35/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2897 - accuracy: 0.5709\n",
      "Epoch 36/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2761 - accuracy: 0.5797\n",
      "Epoch 37/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2755 - accuracy: 0.5749\n",
      "Epoch 38/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2677 - accuracy: 0.5789\n",
      "Epoch 39/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2595 - accuracy: 0.5808\n",
      "Epoch 40/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2627 - accuracy: 0.5801\n",
      "Epoch 41/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2590 - accuracy: 0.5756\n",
      "Epoch 42/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2531 - accuracy: 0.5786\n",
      "Epoch 43/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2571 - accuracy: 0.5812\n",
      "Epoch 44/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2575 - accuracy: 0.5797\n",
      "Epoch 45/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2465 - accuracy: 0.5845\n",
      "Epoch 46/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2416 - accuracy: 0.5867\n",
      "Epoch 47/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2411 - accuracy: 0.5841\n",
      "Epoch 48/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2323 - accuracy: 0.5867\n",
      "Epoch 49/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2333 - accuracy: 0.5881\n",
      "Epoch 50/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2414 - accuracy: 0.5823\n",
      "Epoch 51/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2377 - accuracy: 0.5837\n",
      "Epoch 52/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2199 - accuracy: 0.5937\n",
      "Epoch 53/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2297 - accuracy: 0.5900\n",
      "Epoch 54/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2205 - accuracy: 0.5915\n",
      "Epoch 55/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2247 - accuracy: 0.5900\n",
      "Epoch 56/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2274 - accuracy: 0.5881\n",
      "Epoch 57/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2207 - accuracy: 0.5948\n",
      "Epoch 58/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2175 - accuracy: 0.5922\n",
      "Epoch 59/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2241 - accuracy: 0.5893\n",
      "Epoch 60/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2227 - accuracy: 0.5918\n",
      "Epoch 61/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2119 - accuracy: 0.5962\n",
      "Epoch 62/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2087 - accuracy: 0.5962\n",
      "Epoch 63/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2178 - accuracy: 0.5915\n",
      "Epoch 64/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2136 - accuracy: 0.5951\n",
      "Epoch 65/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2053 - accuracy: 0.5974\n",
      "Epoch 66/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2030 - accuracy: 0.5966\n",
      "Epoch 67/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2036 - accuracy: 0.5974\n",
      "Epoch 68/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2015 - accuracy: 0.5977\n",
      "Epoch 69/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2091 - accuracy: 0.5966\n",
      "Epoch 70/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2073 - accuracy: 0.5962\n",
      "Epoch 71/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1960 - accuracy: 0.5988\n",
      "Epoch 72/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1910 - accuracy: 0.6021\n",
      "Epoch 73/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2018 - accuracy: 0.5966\n",
      "Epoch 74/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1953 - accuracy: 0.5981\n",
      "Epoch 75/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1966 - accuracy: 0.5981\n",
      "Epoch 76/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1927 - accuracy: 0.6003\n",
      "Epoch 77/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2033 - accuracy: 0.5977\n",
      "Epoch 78/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1987 - accuracy: 0.5977\n",
      "Epoch 79/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2023 - accuracy: 0.5962\n",
      "Epoch 80/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2021 - accuracy: 0.5974\n",
      "Epoch 81/100\n",
      "85/85 [==============================] - 0s 4ms/step - loss: 0.1893 - accuracy: 0.5999\n",
      "Epoch 82/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.1839 - accuracy: 0.6032\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1832 - accuracy: 0.6018\n",
      "Epoch 84/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1838 - accuracy: 0.6007\n",
      "Epoch 85/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1909 - accuracy: 0.5996\n",
      "Epoch 86/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1869 - accuracy: 0.5992\n",
      "Epoch 87/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1880 - accuracy: 0.5999\n",
      "Epoch 88/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1874 - accuracy: 0.6007\n",
      "Epoch 89/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1937 - accuracy: 0.5944\n",
      "Epoch 90/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1860 - accuracy: 0.5996\n",
      "Epoch 91/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1875 - accuracy: 0.6007\n",
      "Epoch 92/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1857 - accuracy: 0.6021\n",
      "Epoch 93/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1814 - accuracy: 0.6010\n",
      "Epoch 94/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1844 - accuracy: 0.5996\n",
      "Epoch 95/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2200 - accuracy: 0.5988\n",
      "Epoch 96/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1880 - accuracy: 0.6007\n",
      "Epoch 97/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2174 - accuracy: 0.5904\n",
      "Epoch 98/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1986 - accuracy: 0.5977\n",
      "Epoch 99/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1854 - accuracy: 0.5988\n",
      "Epoch 100/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1758 - accuracy: 0.6032\n"
     ]
    }
   ],
   "source": [
    "dataX = np.array(trainData)[:,79:len(trainData)]\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(200, input_dim=np.shape(dataX)[1], activation='relu', kernel_regularizer = keras.regularizers.L2(0.001)))\n",
    "model1.add(Dense(100, activation='relu', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model1.add(Dense(100, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model1.add(Dense(1, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "\n",
    "model1.compile(loss='MeanSquaredError', optimizer=\"adam\", metrics=['accuracy'])\n",
    "history = model1.fit(dataX, toClassification(activitiesTrain),epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1fa93b",
   "metadata": {},
   "source": [
    "# Training on Structure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "149c1c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "85/85 [==============================] - 1s 2ms/step - loss: 2.6477 - accuracy: 0.3198\n",
      "Epoch 2/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.9294 - accuracy: 0.4188\n",
      "Epoch 3/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5379 - accuracy: 0.4284\n",
      "Epoch 4/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.2741 - accuracy: 0.4457\n",
      "Epoch 5/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.1011 - accuracy: 0.4479\n",
      "Epoch 6/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.9612 - accuracy: 0.4575\n",
      "Epoch 7/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.8719 - accuracy: 0.4626\n",
      "Epoch 8/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.7876 - accuracy: 0.4682\n",
      "Epoch 9/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.7347 - accuracy: 0.4663\n",
      "Epoch 10/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.6951 - accuracy: 0.4744\n",
      "Epoch 11/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.6578 - accuracy: 0.4748\n",
      "Epoch 12/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.6273 - accuracy: 0.4847\n",
      "Epoch 13/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.6029 - accuracy: 0.4899\n",
      "Epoch 14/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5731 - accuracy: 0.4906\n",
      "Epoch 15/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5584 - accuracy: 0.4910\n",
      "Epoch 16/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5565 - accuracy: 0.4895\n",
      "Epoch 17/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5485 - accuracy: 0.4972\n",
      "Epoch 18/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.5031\n",
      "Epoch 19/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5052 - accuracy: 0.5072\n",
      "Epoch 20/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5057 - accuracy: 0.5068\n",
      "Epoch 21/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4983 - accuracy: 0.5064\n",
      "Epoch 22/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.5035\n",
      "Epoch 23/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4788 - accuracy: 0.5175\n",
      "Epoch 24/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4644 - accuracy: 0.5142\n",
      "Epoch 25/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4655 - accuracy: 0.5134\n",
      "Epoch 26/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4780 - accuracy: 0.5075\n",
      "Epoch 27/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4512 - accuracy: 0.5142\n",
      "Epoch 28/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4437 - accuracy: 0.5179\n",
      "Epoch 29/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4568 - accuracy: 0.5167\n",
      "Epoch 30/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4399 - accuracy: 0.5142\n",
      "Epoch 31/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4341 - accuracy: 0.5271\n",
      "Epoch 32/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4194 - accuracy: 0.5237\n",
      "Epoch 33/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4331 - accuracy: 0.5241\n",
      "Epoch 34/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4088 - accuracy: 0.5307\n",
      "Epoch 35/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4053 - accuracy: 0.5278\n",
      "Epoch 36/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4202 - accuracy: 0.5307\n",
      "Epoch 37/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4018 - accuracy: 0.5274\n",
      "Epoch 38/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.5326\n",
      "Epoch 39/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4029 - accuracy: 0.5329\n",
      "Epoch 40/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3970 - accuracy: 0.5271\n",
      "Epoch 41/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3800 - accuracy: 0.5377\n",
      "Epoch 42/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3759 - accuracy: 0.5392\n",
      "Epoch 43/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.3770 - accuracy: 0.5444\n",
      "Epoch 44/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.3692 - accuracy: 0.5432\n",
      "Epoch 45/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3637 - accuracy: 0.5466\n",
      "Epoch 46/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3572 - accuracy: 0.5414\n",
      "Epoch 47/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3572 - accuracy: 0.5429\n",
      "Epoch 48/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3695 - accuracy: 0.5473\n",
      "Epoch 49/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3489 - accuracy: 0.5491\n",
      "Epoch 50/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3532 - accuracy: 0.5466\n",
      "Epoch 51/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.5528\n",
      "Epoch 52/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3485 - accuracy: 0.5462\n",
      "Epoch 53/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.3588 - accuracy: 0.5477\n",
      "Epoch 54/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3500 - accuracy: 0.5491\n",
      "Epoch 55/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3470 - accuracy: 0.5491\n",
      "Epoch 56/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3485 - accuracy: 0.5517\n",
      "Epoch 57/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3315 - accuracy: 0.5576\n",
      "Epoch 58/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3531 - accuracy: 0.5473\n",
      "Epoch 59/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3441 - accuracy: 0.5521\n",
      "Epoch 60/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3132 - accuracy: 0.5591\n",
      "Epoch 61/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3193 - accuracy: 0.5565\n",
      "Epoch 62/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3291 - accuracy: 0.5521\n",
      "Epoch 63/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3276 - accuracy: 0.5558\n",
      "Epoch 64/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3154 - accuracy: 0.5602\n",
      "Epoch 65/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3293 - accuracy: 0.5569\n",
      "Epoch 66/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3195 - accuracy: 0.5609\n",
      "Epoch 67/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3071 - accuracy: 0.5616\n",
      "Epoch 68/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3115 - accuracy: 0.5642\n",
      "Epoch 69/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3037 - accuracy: 0.5646\n",
      "Epoch 70/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3370 - accuracy: 0.5536\n",
      "Epoch 71/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3148 - accuracy: 0.5602\n",
      "Epoch 72/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3096 - accuracy: 0.5624\n",
      "Epoch 73/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3081 - accuracy: 0.5583\n",
      "Epoch 74/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2992 - accuracy: 0.5653\n",
      "Epoch 75/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3240 - accuracy: 0.5521\n",
      "Epoch 76/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2959 - accuracy: 0.5679\n",
      "Epoch 77/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3015 - accuracy: 0.5635\n",
      "Epoch 78/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3052 - accuracy: 0.5598\n",
      "Epoch 79/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3166 - accuracy: 0.5609\n",
      "Epoch 80/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2990 - accuracy: 0.5646\n",
      "Epoch 81/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2832 - accuracy: 0.5675\n",
      "Epoch 82/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2805 - accuracy: 0.5690\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2969 - accuracy: 0.5668\n",
      "Epoch 84/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2821 - accuracy: 0.5705\n",
      "Epoch 85/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2734 - accuracy: 0.5686\n",
      "Epoch 86/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2782 - accuracy: 0.5709\n",
      "Epoch 87/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2745 - accuracy: 0.5720\n",
      "Epoch 88/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3017 - accuracy: 0.5690\n",
      "Epoch 89/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2840 - accuracy: 0.5734\n",
      "Epoch 90/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2931 - accuracy: 0.5675\n",
      "Epoch 91/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2805 - accuracy: 0.5675\n",
      "Epoch 92/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2763 - accuracy: 0.5675\n",
      "Epoch 93/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2760 - accuracy: 0.5701\n",
      "Epoch 94/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2701 - accuracy: 0.5738\n",
      "Epoch 95/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2758 - accuracy: 0.5672\n",
      "Epoch 96/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2649 - accuracy: 0.5778\n",
      "Epoch 97/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2716 - accuracy: 0.5723\n",
      "Epoch 98/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2753 - accuracy: 0.5716\n",
      "Epoch 99/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2745 - accuracy: 0.5701\n",
      "Epoch 100/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2789 - accuracy: 0.5731\n"
     ]
    }
   ],
   "source": [
    "dataX = np.array(trainData)[:,59:79]\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(200, input_dim=np.shape(dataX)[1], activation='relu', kernel_regularizer = keras.regularizers.L2(0.001)))\n",
    "model2.add(Dense(100, activation='relu', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model2.add(Dense(100, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model2.add(Dense(1, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "\n",
    "model2.compile(loss='MeanSquaredError', optimizer=\"adam\", metrics=['accuracy'])\n",
    "history = model2.fit(dataX, toClassification(activitiesTrain),epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e22e09",
   "metadata": {},
   "source": [
    "# Training on TPSA, VSA ESTATE and ESTATE VSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba9c8f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-77cc73b5d4f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m59\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "dataX = np.array(trainData)[:,48:59]\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(200, input_dim=np.shape(dataX)[1], activation='relu', kernel_regularizer = keras.regularizers.L2(0.001)))\n",
    "model3.add(Dense(100, activation='relu', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model3.add(Dense(100, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model3.add(Dense(1, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "\n",
    "model3.compile(loss='MeanSquaredError', optimizer=\"adam\", metrics=['accuracy'])\n",
    "history = model3.fit(dataX, toClassification(activitiesTrain),epochs=200, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c0e7cf",
   "metadata": {},
   "source": [
    "# Training on Slogp, SMR, and PAOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5db0ece1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "85/85 [==============================] - 1s 2ms/step - loss: 2.5360 - accuracy: 0.3993\n",
      "Epoch 2/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.8129 - accuracy: 0.4799\n",
      "Epoch 3/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.3916 - accuracy: 0.4906\n",
      "Epoch 4/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.1047 - accuracy: 0.5013\n",
      "Epoch 5/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.9185 - accuracy: 0.5050\n",
      "Epoch 6/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.7782 - accuracy: 0.5094\n",
      "Epoch 7/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.6980 - accuracy: 0.5116\n",
      "Epoch 8/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.6166 - accuracy: 0.5219\n",
      "Epoch 9/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5789 - accuracy: 0.5153\n",
      "Epoch 10/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5293 - accuracy: 0.5282\n",
      "Epoch 11/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4955 - accuracy: 0.5304\n",
      "Epoch 12/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4704 - accuracy: 0.5278\n",
      "Epoch 13/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4434 - accuracy: 0.5359\n",
      "Epoch 14/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4246 - accuracy: 0.5432\n",
      "Epoch 15/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4096 - accuracy: 0.5399\n",
      "Epoch 16/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3943 - accuracy: 0.5451\n",
      "Epoch 17/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3815 - accuracy: 0.5488\n",
      "Epoch 18/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3688 - accuracy: 0.5473\n",
      "Epoch 19/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3533 - accuracy: 0.5488\n",
      "Epoch 20/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.3516 - accuracy: 0.5510\n",
      "Epoch 21/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3269 - accuracy: 0.5628\n",
      "Epoch 22/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3174 - accuracy: 0.5613\n",
      "Epoch 23/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3133 - accuracy: 0.5628\n",
      "Epoch 24/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.3006 - accuracy: 0.5697\n",
      "Epoch 25/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.2899 - accuracy: 0.5705\n",
      "Epoch 26/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2866 - accuracy: 0.5694\n",
      "Epoch 27/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.2764 - accuracy: 0.5701\n",
      "Epoch 28/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2909 - accuracy: 0.5653\n",
      "Epoch 29/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.2809 - accuracy: 0.5690\n",
      "Epoch 30/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2732 - accuracy: 0.5742\n",
      "Epoch 31/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2685 - accuracy: 0.5727\n",
      "Epoch 32/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2585 - accuracy: 0.5819\n",
      "Epoch 33/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2511 - accuracy: 0.5830\n",
      "Epoch 34/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2533 - accuracy: 0.5815\n",
      "Epoch 35/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2553 - accuracy: 0.5793\n",
      "Epoch 36/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2436 - accuracy: 0.5848\n",
      "Epoch 37/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2347 - accuracy: 0.5874\n",
      "Epoch 38/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.2258 - accuracy: 0.5918\n",
      "Epoch 39/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2323 - accuracy: 0.5870\n",
      "Epoch 40/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2328 - accuracy: 0.5911\n",
      "Epoch 41/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2413 - accuracy: 0.5819\n",
      "Epoch 42/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2349 - accuracy: 0.5859\n",
      "Epoch 43/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2204 - accuracy: 0.5900\n",
      "Epoch 44/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.2126 - accuracy: 0.5962\n",
      "Epoch 45/100\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.2170 - accuracy: 0.5933\n",
      "Epoch 46/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2148 - accuracy: 0.5948\n",
      "Epoch 47/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2244 - accuracy: 0.5878\n",
      "Epoch 48/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2252 - accuracy: 0.5896\n",
      "Epoch 49/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2161 - accuracy: 0.5940\n",
      "Epoch 50/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2103 - accuracy: 0.5933\n",
      "Epoch 51/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1978 - accuracy: 0.5999\n",
      "Epoch 52/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1997 - accuracy: 0.5985\n",
      "Epoch 53/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1958 - accuracy: 0.5992\n",
      "Epoch 54/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1951 - accuracy: 0.5992\n",
      "Epoch 55/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1981 - accuracy: 0.5951\n",
      "Epoch 56/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1875 - accuracy: 0.6032\n",
      "Epoch 57/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1923 - accuracy: 0.5996\n",
      "Epoch 58/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1919 - accuracy: 0.5992\n",
      "Epoch 59/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1933 - accuracy: 0.5985\n",
      "Epoch 60/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2071 - accuracy: 0.5951\n",
      "Epoch 61/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2099 - accuracy: 0.5900\n",
      "Epoch 62/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2008 - accuracy: 0.5944\n",
      "Epoch 63/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1901 - accuracy: 0.6018\n",
      "Epoch 64/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1814 - accuracy: 0.6021\n",
      "Epoch 65/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1818 - accuracy: 0.6043\n",
      "Epoch 66/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1860 - accuracy: 0.5999\n",
      "Epoch 67/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1943 - accuracy: 0.5992\n",
      "Epoch 68/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2075 - accuracy: 0.5926\n",
      "Epoch 69/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1989 - accuracy: 0.5959\n",
      "Epoch 70/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1822 - accuracy: 0.6014\n",
      "Epoch 71/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1844 - accuracy: 0.6007\n",
      "Epoch 72/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1818 - accuracy: 0.6010\n",
      "Epoch 73/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1708 - accuracy: 0.6047\n",
      "Epoch 74/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1682 - accuracy: 0.6040\n",
      "Epoch 75/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1693 - accuracy: 0.6066\n",
      "Epoch 76/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1635 - accuracy: 0.6062\n",
      "Epoch 77/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1607 - accuracy: 0.6080\n",
      "Epoch 78/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1751 - accuracy: 0.6040\n",
      "Epoch 79/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1663 - accuracy: 0.6036\n",
      "Epoch 80/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1588 - accuracy: 0.6062\n",
      "Epoch 81/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1594 - accuracy: 0.6088\n",
      "Epoch 82/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1766 - accuracy: 0.5996\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1814 - accuracy: 0.5977\n",
      "Epoch 84/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1715 - accuracy: 0.6014\n",
      "Epoch 85/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1662 - accuracy: 0.6040\n",
      "Epoch 86/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1670 - accuracy: 0.6025\n",
      "Epoch 87/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1637 - accuracy: 0.6062\n",
      "Epoch 88/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1587 - accuracy: 0.6066\n",
      "Epoch 89/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1592 - accuracy: 0.6069\n",
      "Epoch 90/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1484 - accuracy: 0.6113\n",
      "Epoch 91/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1460 - accuracy: 0.6110\n",
      "Epoch 92/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1407 - accuracy: 0.6132\n",
      "Epoch 93/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1439 - accuracy: 0.6117\n",
      "Epoch 94/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1394 - accuracy: 0.6139\n",
      "Epoch 95/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1364 - accuracy: 0.6124\n",
      "Epoch 96/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1368 - accuracy: 0.6117\n",
      "Epoch 97/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1346 - accuracy: 0.6143\n",
      "Epoch 98/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1375 - accuracy: 0.6121\n",
      "Epoch 99/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1743 - accuracy: 0.5977\n",
      "Epoch 100/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.1846 - accuracy: 0.5962\n"
     ]
    }
   ],
   "source": [
    "dataX = np.array(trainData)[:,29:48]\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(200, input_dim=np.shape(dataX)[1], activation='relu', kernel_regularizer = keras.regularizers.L2(0.001)))\n",
    "model4.add(Dense(100, activation='relu', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model4.add(Dense(100, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model4.add(Dense(1, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "\n",
    "model4.compile(loss='MeanSquaredError', optimizer=\"adam\", metrics=['accuracy'])\n",
    "history = model4.fit(dataX, toClassification(activitiesTrain),epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f47d21b",
   "metadata": {},
   "source": [
    "# Training on the remaining Compound Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ddc95aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "85/85 [==============================] - 1s 2ms/step - loss: 2.6314 - accuracy: 0.3909\n",
      "Epoch 2/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.9729 - accuracy: 0.4479\n",
      "Epoch 3/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.5700 - accuracy: 0.4619\n",
      "Epoch 4/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.2946 - accuracy: 0.4737\n",
      "Epoch 5/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 1.1142 - accuracy: 0.4799\n",
      "Epoch 6/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.9671 - accuracy: 0.4825\n",
      "Epoch 7/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.8723 - accuracy: 0.4851\n",
      "Epoch 8/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.7988 - accuracy: 0.4917\n",
      "Epoch 9/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.7280 - accuracy: 0.4943\n",
      "Epoch 10/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.6700 - accuracy: 0.4987\n",
      "Epoch 11/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.6346 - accuracy: 0.4965\n",
      "Epoch 12/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.6051 - accuracy: 0.4947\n",
      "Epoch 13/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5837 - accuracy: 0.5064\n",
      "Epoch 14/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5614 - accuracy: 0.5035\n",
      "Epoch 15/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5386 - accuracy: 0.5072\n",
      "Epoch 16/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5174 - accuracy: 0.5145\n",
      "Epoch 17/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.5011 - accuracy: 0.5134\n",
      "Epoch 18/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4824 - accuracy: 0.5212\n",
      "Epoch 19/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4724 - accuracy: 0.5278\n",
      "Epoch 20/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4664 - accuracy: 0.5193\n",
      "Epoch 21/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4526 - accuracy: 0.5248\n",
      "Epoch 22/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4441 - accuracy: 0.5267\n",
      "Epoch 23/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4412 - accuracy: 0.5248\n",
      "Epoch 24/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.4444 - accuracy: 0.5271\n",
      "Epoch 25/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4314 - accuracy: 0.5241\n",
      "Epoch 26/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4211 - accuracy: 0.5333\n",
      "Epoch 27/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4056 - accuracy: 0.5370\n",
      "Epoch 28/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4080 - accuracy: 0.5388\n",
      "Epoch 29/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3968 - accuracy: 0.5392\n",
      "Epoch 30/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3761 - accuracy: 0.5425\n",
      "Epoch 31/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3743 - accuracy: 0.5480\n",
      "Epoch 32/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.5447\n",
      "Epoch 33/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3780 - accuracy: 0.5447\n",
      "Epoch 34/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3605 - accuracy: 0.5506\n",
      "Epoch 35/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.3575 - accuracy: 0.5499\n",
      "Epoch 36/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.3678 - accuracy: 0.5480\n",
      "Epoch 37/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3578 - accuracy: 0.5477\n",
      "Epoch 38/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3704 - accuracy: 0.5528\n",
      "Epoch 39/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3562 - accuracy: 0.5554\n",
      "Epoch 40/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3432 - accuracy: 0.5506\n",
      "Epoch 41/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.3368 - accuracy: 0.5550\n",
      "Epoch 42/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.3376 - accuracy: 0.5576\n",
      "Epoch 43/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.3280 - accuracy: 0.5616\n",
      "Epoch 44/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.3375 - accuracy: 0.5591\n",
      "Epoch 45/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.3245 - accuracy: 0.5572\n",
      "Epoch 46/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3232 - accuracy: 0.5620\n",
      "Epoch 47/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3028 - accuracy: 0.5679\n",
      "Epoch 48/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3149 - accuracy: 0.5683\n",
      "Epoch 49/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.3018 - accuracy: 0.5690\n",
      "Epoch 50/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3056 - accuracy: 0.5675\n",
      "Epoch 51/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3168 - accuracy: 0.5653\n",
      "Epoch 52/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3065 - accuracy: 0.5653\n",
      "Epoch 53/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.3016 - accuracy: 0.5668\n",
      "Epoch 54/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.3055 - accuracy: 0.5661\n",
      "Epoch 55/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2918 - accuracy: 0.5657\n",
      "Epoch 56/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.3035 - accuracy: 0.5653\n",
      "Epoch 57/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2934 - accuracy: 0.5734\n",
      "Epoch 58/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2896 - accuracy: 0.5705\n",
      "Epoch 59/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2854 - accuracy: 0.5716\n",
      "Epoch 60/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2873 - accuracy: 0.5749\n",
      "Epoch 61/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2696 - accuracy: 0.5786\n",
      "Epoch 62/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2819 - accuracy: 0.5742\n",
      "Epoch 63/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2641 - accuracy: 0.5782\n",
      "Epoch 64/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2683 - accuracy: 0.5767\n",
      "Epoch 65/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2656 - accuracy: 0.5775\n",
      "Epoch 66/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2579 - accuracy: 0.5782\n",
      "Epoch 67/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2583 - accuracy: 0.5823\n",
      "Epoch 68/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2639 - accuracy: 0.5808\n",
      "Epoch 69/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2616 - accuracy: 0.5775\n",
      "Epoch 70/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2568 - accuracy: 0.5775\n",
      "Epoch 71/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2462 - accuracy: 0.5845\n",
      "Epoch 72/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2525 - accuracy: 0.5837\n",
      "Epoch 73/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2619 - accuracy: 0.5801\n",
      "Epoch 74/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2590 - accuracy: 0.5797\n",
      "Epoch 75/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2511 - accuracy: 0.5819\n",
      "Epoch 76/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2442 - accuracy: 0.5859\n",
      "Epoch 77/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2662 - accuracy: 0.5826\n",
      "Epoch 78/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2417 - accuracy: 0.5848\n",
      "Epoch 79/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2511 - accuracy: 0.5848\n",
      "Epoch 80/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2380 - accuracy: 0.5874\n",
      "Epoch 81/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.5830\n",
      "Epoch 82/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2326 - accuracy: 0.5881\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2470 - accuracy: 0.5845\n",
      "Epoch 84/100\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.2314 - accuracy: 0.5863\n",
      "Epoch 85/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2187 - accuracy: 0.5951\n",
      "Epoch 86/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2281 - accuracy: 0.5918\n",
      "Epoch 87/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2326 - accuracy: 0.5863\n",
      "Epoch 88/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2379 - accuracy: 0.5867\n",
      "Epoch 89/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2575 - accuracy: 0.5819\n",
      "Epoch 90/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2359 - accuracy: 0.5881\n",
      "Epoch 91/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2452 - accuracy: 0.5830\n",
      "Epoch 92/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2271 - accuracy: 0.5911\n",
      "Epoch 93/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2128 - accuracy: 0.5929\n",
      "Epoch 94/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2172 - accuracy: 0.5948\n",
      "Epoch 95/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2125 - accuracy: 0.5962\n",
      "Epoch 96/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2193 - accuracy: 0.5911\n",
      "Epoch 97/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2093 - accuracy: 0.5948\n",
      "Epoch 98/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2318 - accuracy: 0.5878\n",
      "Epoch 99/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2186 - accuracy: 0.5915\n",
      "Epoch 100/100\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2210 - accuracy: 0.5889\n"
     ]
    }
   ],
   "source": [
    "dataX = np.array(trainData)[:,:29]\n",
    "\n",
    "model5 = Sequential()\n",
    "model5.add(Dense(200, input_dim=np.shape(dataX)[1], activation='relu', kernel_regularizer = keras.regularizers.L2(0.001)))\n",
    "model5.add(Dense(100, activation='relu', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model5.add(Dense(100, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model5.add(Dense(1, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "\n",
    "model5.compile(loss='MeanSquaredError', optimizer=\"adam\", metrics=['accuracy'])\n",
    "history = model5.fit(dataX, toClassification(activitiesTrain),epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d6dc8b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 978us/step\n",
      "[[-1. -1. -1. ... -1. -1. -1.]\n",
      " [ 1.  1.  1. ...  1.  1.  1.]\n",
      " [ 1.  1.  1. ...  1.  1.  1.]\n",
      " ...\n",
      " [-1. -1. -1. ... -1. -1. -1.]\n",
      " [-1. -1. -1. ... -1. -1. -1.]\n",
      " [ 1.  1.  1. ...  1.  1.  1.]]\n",
      "0.5216955017301038\n"
     ]
    }
   ],
   "source": [
    "dataInds = [[79,len(trainData)],[59,79],[48,59],[29,48],[0,29]]\n",
    "yAgg = takeVote([model1,model2,model3,model4,model5], valData, toClassification(activitiesValidate), False, dataInds)\n",
    "print(yAgg)\n",
    "print(np.mean(toClassification(activitiesValidate) == yAgg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8568036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 1ms/step\n",
      "0.5256401384083045\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "0.5256401384083045\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "0.5322145328719723\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "0.5282698961937716\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "0.5249826989619377\n"
     ]
    }
   ],
   "source": [
    "valData = np.array(valData)\n",
    "print(np.mean(toClassification(activitiesValidate) == np.sign(model1.predict(valData[:,79:]))))\n",
    "print(np.mean(toClassification(activitiesValidate) == np.sign(model2.predict(valData[:,59:79]))))\n",
    "print(np.mean(toClassification(activitiesValidate) == np.sign(model3.predict(valData[:,48:59]))))\n",
    "print(np.mean(toClassification(activitiesValidate) == np.sign(model4.predict(valData[:,29:48]))))\n",
    "print(np.mean(toClassification(activitiesValidate) == np.sign(model5.predict(valData[:,0:29]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4704e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
