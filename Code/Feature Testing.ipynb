{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5ba41ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import Loader\n",
    "import tensorflow as tf\n",
    "import ModelFile\n",
    "import Transformer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "#use for dark mode\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "#plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c7aebfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runBasicSVM(Xtrain, Ytrain, valData):\n",
    "    maxC=10\n",
    "    clf=svm.SVC(\n",
    "        C=maxC,                          # The regularization parameter\n",
    "        kernel='rbf',                   # The kernel type used \n",
    "        degree=4,                       # Degree of polynomial function \n",
    "        gamma='scale',                  # The kernel coefficient\n",
    "        coef0=0.0,                      # If kernel = 'poly'/'sigmoid'\n",
    "        shrinking=True,                 # To use shrinking heuristic\n",
    "        probability=False,              # Enable probability estimates\n",
    "        tol=0.001,                      # Stopping crierion\n",
    "        cache_size=200,                 # Size of kernel cache\n",
    "        class_weight=None,              # The weight of each class\n",
    "        verbose=False,                  # Enable verbose output\n",
    "        max_iter=- 1,                   # Hard limit on iterations\n",
    "        decision_function_shape='ovr',  # One-vs-rest or one-vs-one\n",
    "        break_ties=False,               # How to handle breaking ties\n",
    "        random_state=None               # Random state of the model\n",
    "    )\n",
    "    #Xtrain,_,valData = Transformer.normalizeData(Xtrain, compoundDataTest, compoundDataValidate, newMean=0, newStd=1)\n",
    "    clf.fit(Xtrain, Ytrain)\n",
    "    predictions = clf.predict(Xtrain)\n",
    "    val_pred = clf.predict(valData)\n",
    "    accuracy = [predictions, val_pred]\n",
    "    return accuracy, clf\n",
    "\n",
    "def get_data():\n",
    "    compoundsTrain, smilesTrain, labelsTrain, compoundDataTrain, activitiesTrain = Loader.getTrain(defaultValue=0)\n",
    "    compoundsTest, smilesTest, labelsTest, compoundDataTest, activitiesTest = Loader.getTest(defaultValue=0)\n",
    "    compoundsValidate, smilesValidate, labelsValidate, compoundDataValidate, activitiesValidate = Loader.getValidate(defaultValue=0)\n",
    "\n",
    "    labelsMean, trainMean = Transformer.useAverageFD(labelsTrain, compoundDataTrain)\n",
    "    _, testMean = Transformer.useAverageFD(labelsTest, compoundDataTest)\n",
    "    _, valMean = Transformer.useAverageFD(labelsValidate, compoundDataValidate)\n",
    "\n",
    "    labelsMax, trainMax = Transformer.useMaxFD(labelsTrain, compoundDataTrain)\n",
    "    _, testMax = Transformer.useMaxFD(labelsTest, compoundDataTest)\n",
    "    _, valMax = Transformer.useMaxFD(labelsValidate, compoundDataValidate)\n",
    "\n",
    "    #after transformations are done assign data\n",
    "    dataLabels = labelsMax\n",
    "    trainData = trainMax\n",
    "    testData = testMax\n",
    "    valData = valMax\n",
    "\n",
    "    trainData, testData, valData = Transformer.normalizeData(trainData, testData, valData, newMean=0, newStd=1)\n",
    "    \n",
    "    classTrain = Transformer.toBinaryClassification(activitiesTrain)\n",
    "    classVal = Transformer.toBinaryClassification(activitiesValidate)\n",
    "    classTest = Transformer.toBinaryClassification(activitiesTest)\n",
    "    \n",
    "    return trainData, testData, valData, classTrain, classTest, classVal, dataLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b988a0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 % complete\n",
      "20.0 % complete\n",
      "30.0 % complete\n",
      "40.0 % complete\n",
      "50.0 % complete\n",
      "60.0 % complete\n",
      "70.0 % complete\n",
      "80.0 % complete\n",
      "90.0 % complete\n",
      "100.0 % complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Probability Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>peoe_vsa12</td>\n",
       "      <td>419.3315715011778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fr_aniline</td>\n",
       "      <td>324.69374507667493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slogp_vsa12</td>\n",
       "      <td>295.17613188788624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>peoe_vsa9</td>\n",
       "      <td>260.3719141769857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fr_nh1</td>\n",
       "      <td>246.4113297336056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>fr_term_acetylene</td>\n",
       "      <td>23.438939584500012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>bcut2d_mrlow</td>\n",
       "      <td>23.204550188655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>smr_vsa8</td>\n",
       "      <td>22.51535184350178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>fr_arn</td>\n",
       "      <td>20.884095169789507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>fr_sulfide</td>\n",
       "      <td>20.46850167591071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Feature  Probability Weight\n",
       "0           peoe_vsa12   419.3315715011778\n",
       "1           fr_aniline  324.69374507667493\n",
       "2          slogp_vsa12  295.17613188788624\n",
       "3            peoe_vsa9   260.3719141769857\n",
       "4               fr_nh1   246.4113297336056\n",
       "..                 ...                 ...\n",
       "205  fr_term_acetylene  23.438939584500012\n",
       "206       bcut2d_mrlow     23.204550188655\n",
       "207           smr_vsa8   22.51535184350178\n",
       "208             fr_arn  20.884095169789507\n",
       "209         fr_sulfide   20.46850167591071\n",
       "\n",
       "[210 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.9503128450496872, 0.9205882352941176]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def parameter_rank(top_x, epochs, avg_std_runs):\n",
    "    \n",
    "    trainData, testData, valData, classTrain, classTest, classVal, dataLabels = get_data()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(trainData,classTrain,\n",
    "                                       random_state=104, \n",
    "                                       test_size=0.20, \n",
    "                                       shuffle=True)\n",
    "    col_idx = np.arange(0,210)\n",
    "    probs = []\n",
    "    for i in range(210):\n",
    "        probs.append(float(50))\n",
    "    probs = np.array(probs)\n",
    "    eval_set = []\n",
    "\n",
    "    for x in range(epochs):\n",
    "        \n",
    "        train_data_df = pd.DataFrame(X_train, columns = dataLabels)\n",
    "        train_temp_df = train_data_df.copy()\n",
    "        val_data_df = pd.DataFrame(X_test, columns = dataLabels)\n",
    "        val_temp_df = val_data_df.copy()\n",
    "\n",
    "        values = probs\n",
    "        \n",
    "        arr1 = values / values.min()\n",
    "        arr1 = arr1 / arr1.sum()\n",
    "        #print(arr1, arr1.sum())\n",
    "        #display(values, sum(values))\n",
    "        test_cols = np.random.choice(col_idx, top_x, replace=False, p = arr1)\n",
    "        # using set() to perform task\n",
    "        test_cols_set = set(test_cols)\n",
    "        col_idx_set = set(col_idx)\n",
    "        zero_cols = list(col_idx_set - test_cols_set)\n",
    "        \n",
    "        train_temp_df.iloc[:, zero_cols] *= 0\n",
    "        val_temp_df.iloc[:, zero_cols] *= 0\n",
    "        \n",
    "        \n",
    "        Xtrain = train_temp_df.values\n",
    "        Xval = val_temp_df.values\n",
    "        accuracy = runBasicSVM(Xtrain,y_train, Xval)\n",
    "        curr_val_acc = accuracy_score(y_test, accuracy[1])\n",
    "        \n",
    "        if(x < avg_std_runs):\n",
    "            eval_set.append(curr_val_acc)\n",
    "        else:\n",
    "            \n",
    "            if curr_val_acc > np.average(eval_set) + np.std(eval_set):\n",
    "                np.multiply.at(probs,test_cols,1.1)\n",
    "            elif curr_val_acc < np.average(eval_set) - np.std(eval_set):\n",
    "                np.multiply.at(probs,test_cols,.9)\n",
    "                prev_val_acc = accuracy[1]\n",
    "            else:\n",
    "                prev_val_acc = accuracy[1]\n",
    "                \n",
    "            eval_set.pop(0)\n",
    "            eval_set.append(curr_val_acc)\n",
    "        \n",
    "        if((((x+1)/epochs)*100)%10 == 0):\n",
    "            print(((x+1)/epochs)*100, \"% complete\")\n",
    "            #print(probs)\n",
    "\n",
    "    top_idx = np.flip(probs.argsort()[-210:])\n",
    "    #display(final_probs,final_probs.argsort()[-5:])\n",
    "    res_probs_list = [probs[i] for i in top_idx]\n",
    "    res_feats_list = [dataLabels[i] for i in top_idx]\n",
    "    result = [res_feats_list, res_probs_list]\n",
    "    #display(results, results.shape)\n",
    "    rslt_df = pd.DataFrame(data = np.transpose(result), columns=['Feature','Probability Weight'])\n",
    "    #sorted_rslt_df = rslt_df.sort_values(by='Probability Weight', ascending=False)\n",
    "    \n",
    "    train_data_df = pd.DataFrame(trainData, columns = dataLabels)\n",
    "    train_temp_df = train_data_df.copy()\n",
    "    val_data_df = pd.DataFrame(valData, columns = dataLabels)\n",
    "    val_temp_df = val_data_df.copy()\n",
    "    \n",
    "    top_50_idx = probs.argsort()[-50:]\n",
    "    #test_cols = np.random.choice(col_idx, top_50_idx, replace=False, p = arr1)\n",
    "    # using set() to perform task\n",
    "    test_cols_set = set(top_50_idx)\n",
    "    col_idx_set = set(col_idx)\n",
    "    zero_cols = list(col_idx_set - test_cols_set)\n",
    "    \n",
    "    train_temp_df.iloc[:, zero_cols] *= 0\n",
    "    val_temp_df.iloc[:, zero_cols] *= 0\n",
    "\n",
    "\n",
    "    Xtrain = train_temp_df.values\n",
    "    Xval = val_temp_df.values\n",
    "    accuracy = runBasicSVM(Xtrain,classTrain, Xval)\n",
    "    in_samp_acc = accuracy_score(classTrain, accuracy[0])\n",
    "    out_samp_acc = accuracy_score(classVal, accuracy[1])\n",
    "    \n",
    "    final_acc = [in_samp_acc, out_samp_acc]\n",
    "    \n",
    "    \n",
    "\n",
    "    return rslt_df, probs, dataLabels, final_acc\n",
    "#for x in rslt\n",
    "results_df, final_probs, features, final_acc = parameter_rank(50,300,20)\n",
    "display(results_df, final_acc)\n",
    "#listToStr = ' '.join([str(elem) for i,elem in enumerate(s)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ad689242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context('display.max_rows', None,'display.max_columns', None,\n",
    "#     'display.precision', 3,):\n",
    "#     display(results)\n",
    "trainData, testData, valData, classTrain, classTest, classVal, dataLabels = get_data()\n",
    "\n",
    "col_idx = np.arange(0,210)\n",
    "\n",
    "train_data_df = pd.DataFrame(trainData, columns = dataLabels)\n",
    "train_temp_df = train_data_df.copy()\n",
    "val_data_df = pd.DataFrame(valData, columns = dataLabels)\n",
    "val_temp_df = val_data_df.copy()\n",
    "\n",
    "top_50_idx = final_probs.argsort()[-50:]\n",
    "#test_cols = np.random.choice(col_idx, top_50_idx, replace=False, p = arr1)\n",
    "# using set() to perform task\n",
    "test_cols_set = set(top_50_idx)\n",
    "col_idx_set = set(col_idx)\n",
    "zero_cols = list(col_idx_set - test_cols_set)\n",
    "\n",
    "train_temp_df.iloc[:, zero_cols] *= 0\n",
    "val_temp_df.iloc[:, zero_cols] *= 0\n",
    "\n",
    "\n",
    "Xtrain = train_temp_df.values\n",
    "Xval = val_temp_df.values\n",
    "accuracy, clf = runBasicSVM(Xtrain,classTrain, Xval)\n",
    "feature_list = list(results_df.head(50)['Feature'].values)\n",
    "listToStr = ','.join([elem for i,elem in enumerate(feature_list)])\n",
    "description = \"Tim's state of the art feature reduction SVM. This model was generated in the Full Model Testing file. Features follows as comma separated list: \"\n",
    "description += listToStr\n",
    "ModelFile.save(clf, \"TSVM.pkl\", description)\n",
    "# print(\"In Sample ACC: \", final_acc[0])\n",
    "# print(\"Out of Sample ACC: \", final_acc[1])\n",
    "# display(\"Top 50 Features\",results_df.head(50), \"Bottom 50 Features\", results_df.tail(50))\n",
    "#display(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d11dd",
   "metadata": {},
   "source": [
    "# Push the limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86810408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   8  11  16  17  22  23  27  36  42  45  50  51  60  65  66  70\n",
      "  71  74  79  80  82  88  94  96 107 108 112 113 114 116 123 129 130 131\n",
      " 141 142 144 150 154 157 161 164 170 182 187 193 195 201] 50\n",
      "[  0   1   8  11  16  17  22  23  27  36  42  45  50  51  60  65  66  70\n",
      "  71  74  79  80  82  88  94  96 107 108 112 113 114 116 123 129 130 131\n",
      " 141 142 144 150] 40\n",
      "[  0   1   8  11  16  17  22  23  27  36  42  45  50  51  60  65  66  70\n",
      "  71  74  79  80  82  88  94  96 107 108 112 113] 30\n",
      "[ 0  1  8 11 16 17 22 23 27 36 42 45 50 51 60 65 66 70 71 74] 20\n",
      "[ 0  1  8 11 16 17 22 23 27 36] 10\n",
      "[ 0  1  8 11 16] 5\n"
     ]
    }
   ],
   "source": [
    "#for x in range(5):\n",
    "trainData, testData, valData, classTrain, classTest, classVal, full_features = get_data()\n",
    "fig, ax = plt.subplots(1, figsize=(18,16))\n",
    "colors = [\"cyan\",\"red\"]\n",
    "top_50 = results_df.head(50)\n",
    "top_50_features = top_50['Feature']\n",
    "#print(feature, full_features)\n",
    "full_feature_TF = np.isin(full_features, top_50_features)\n",
    "#print(np.isin(full_features, top_50_features))\n",
    "top_50_col_idx = np.where(full_feature_TF == True)[0]\n",
    "col_idx = np.arange(0,210)\n",
    "remove = [0 ,10, 20, 30, 40, 45]\n",
    "\n",
    "in_samp_acc = []\n",
    "out_samp_acc = []\n",
    "\n",
    "for x in remove:\n",
    "    \n",
    "    temp_col_idx = top_50_col_idx.copy()\n",
    "    end = 50 - x\n",
    "    idx = np.arange(end, 50)\n",
    "    temp_col_idx = np.delete(temp_col_idx, idx)\n",
    "    \n",
    "    train_data_df = pd.DataFrame(trainData, columns = full_features)\n",
    "    train_temp_df = train_data_df.copy()\n",
    "    val_data_df = pd.DataFrame(valData, columns = full_features)\n",
    "    val_temp_df = val_data_df.copy()\n",
    "    \n",
    "    zero_cols = list(col_idx - temp_col_idx)\n",
    "    \n",
    "    train_temp_df.iloc[:, zero_cols] *= 0\n",
    "    val_temp_df.iloc[:, zero_cols] *= 0\n",
    "    \n",
    "    Xtrain = train_temp_df.values\n",
    "    Xval = val_temp_df.values\n",
    "    accuracy = runBasicSVM(Xtrain,classTrain, Xval)\n",
    "    in_samp_acc.append(accuracy_score(classTrain, accuracy[0]))\n",
    "    out_samp_acc.append(accuracy_score(classVal, accuracy[1]))\n",
    "    \n",
    "    #print(\"Partition:\", 100*dataSegments[i], \"\\b%, Train Err: \", avrErr[-1], \"Val Err: \", avrErrVal[-1])\n",
    "    \n",
    "ax[0].plot(in_samp_acc, color=colors[0])\n",
    "ax[0].plot(out_samp_acc,  color=colors[1])\n",
    "    \n",
    "    \n",
    "ax[0].set_title(\"Train vs Val Error at Different Feature Numbers\", fontsize = 24)\n",
    "#ax[1].set_title(\"Val Error at Different Data Percentages\", fontsize = 24)\n",
    "ticks = [1,2,3,4,5,6]\n",
    "labels = [50,40,30,20,10,5]\n",
    "\n",
    "ax[0].xticks(ticks, labels, rotation = 'horizontal', labelsize=15)\n",
    "ax[0].legend(loc = 'lower right', fontsize = 15)\n",
    "ax[0].set_xlabel(\"Epochs\", fontsize = 20)\n",
    "ax[0].set_ylabel(\"Accuracy\", fontsize = 20)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(temp_col_idx, len(temp_col_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e048d524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 % complete\n",
      "20.0 % complete\n",
      "30.0 % complete\n",
      "40.0 % complete\n",
      "50.0 % complete\n",
      "60.0 % complete\n",
      "70.0 % complete\n",
      "80.0 % complete\n",
      "90.0 % complete\n",
      "100.0 % complete\n",
      "10.0 % complete\n",
      "20.0 % complete\n",
      "30.0 % complete\n",
      "40.0 % complete\n",
      "50.0 % complete\n",
      "60.0 % complete\n",
      "70.0 % complete\n",
      "80.0 % complete\n",
      "90.0 % complete\n",
      "100.0 % complete\n",
      "10.0 % complete\n",
      "20.0 % complete\n",
      "30.0 % complete\n",
      "40.0 % complete\n",
      "50.0 % complete\n",
      "60.0 % complete\n",
      "70.0 % complete\n",
      "80.0 % complete\n",
      "90.0 % complete\n",
      "100.0 % complete\n",
      "10.0 % complete\n",
      "20.0 % complete\n",
      "30.0 % complete\n",
      "40.0 % complete\n",
      "50.0 % complete\n",
      "60.0 % complete\n",
      "70.0 % complete\n",
      "80.0 % complete\n",
      "90.0 % complete\n",
      "100.0 % complete\n",
      "10.0 % complete\n",
      "20.0 % complete\n",
      "30.0 % complete\n",
      "40.0 % complete\n",
      "50.0 % complete\n",
      "60.0 % complete\n",
      "70.0 % complete\n",
      "80.0 % complete\n",
      "90.0 % complete\n",
      "100.0 % complete\n",
      "10.0 % complete\n",
      "20.0 % complete\n",
      "30.0 % complete\n",
      "40.0 % complete\n",
      "50.0 % complete\n",
      "60.0 % complete\n",
      "70.0 % complete\n",
      "80.0 % complete\n",
      "90.0 % complete\n",
      "100.0 % complete\n",
      "10.0 % complete\n",
      "20.0 % complete\n",
      "30.0 % complete\n",
      "40.0 % complete\n",
      "50.0 % complete\n",
      "60.0 % complete\n",
      "70.0 % complete\n",
      "80.0 % complete\n",
      "90.0 % complete\n",
      "100.0 % complete\n",
      "10.0 % complete\n",
      "20.0 % complete\n",
      "30.0 % complete\n",
      "40.0 % complete\n",
      "50.0 % complete\n",
      "60.0 % complete\n",
      "70.0 % complete\n",
      "80.0 % complete\n",
      "90.0 % complete\n",
      "100.0 % complete\n",
      "10.0 % complete\n",
      "20.0 % complete\n",
      "30.0 % complete\n",
      "40.0 % complete\n",
      "50.0 % complete\n",
      "60.0 % complete\n",
      "70.0 % complete\n",
      "80.0 % complete\n",
      "90.0 % complete\n",
      "100.0 % complete\n",
      "10.0 % complete\n",
      "20.0 % complete\n",
      "30.0 % complete\n",
      "40.0 % complete\n",
      "50.0 % complete\n",
      "60.0 % complete\n",
      "70.0 % complete\n",
      "80.0 % complete\n",
      "90.0 % complete\n",
      "100.0 % complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.951417004048583,\n",
       " 0.9447920500552079,\n",
       " 0.9396393080603607,\n",
       " 0.937799043062201,\n",
       " 0.9462642620537357,\n",
       " 0.9455281560544718,\n",
       " 0.9433198380566802,\n",
       " 0.9536253220463746,\n",
       " 0.9422156790577844,\n",
       " 0.9536253220463746]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9458225984541773"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.9088235294117647,\n",
       " 0.9088235294117647,\n",
       " 0.8970588235294118,\n",
       " 0.9088235294117647,\n",
       " 0.9,\n",
       " 0.9058823529411765,\n",
       " 0.9176470588235294,\n",
       " 0.9088235294117647,\n",
       " 0.9147058823529411,\n",
       " 0.9235294117647059]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9094117647058824"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_in_acc = []\n",
    "test_out_acc = []\n",
    "for x in range(10):\n",
    "    results_df_100, final_probs_100, features_100, final_acc_100 = parameter_rank(50,100,20)\n",
    "    test_in_acc.append(final_acc_100[0])\n",
    "    test_out_acc.append(final_acc_100[1])\n",
    "\n",
    "display(test_in_acc, np.mean(test_in_acc), test_out_acc, np.mean(test_out_acc))\n",
    "feat_100 = results_df_100.head(50)\n",
    "feat_300 = results_df.head(5)\n",
    "\n",
    "\n",
    "#print(feat_100['Feature'].values == feat_300['Feature'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fc98ac4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display(np.isin(feat_300['Feature'].values, feat_100['Feature'].values))\n",
    "# #display(feat_100, feat_300)\n",
    "\n",
    "test = [1,2,3]\n",
    "# display(final_acc_100)\n",
    "x = np.arange(50,50)\n",
    "np.delete(test,x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
