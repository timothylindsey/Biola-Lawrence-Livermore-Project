{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf02e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import Loader\n",
    "import Transformer\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.losses import MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "402c42c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "compoundsTrain, smilesTrain, labelsTrain, compoundDataTrain, activitiesTrain = Loader.getTrain(defaultValue=0)\n",
    "compoundsTest, smilesTest, labelsTest, compoundDataTest, activitiesTest = Loader.getTest(defaultValue=0)\n",
    "compoundsValidate, smilesValidate, labelsValidate, compoundDataValidate, activitiesValidate = Loader.getValidate(defaultValue=0)\n",
    "\n",
    "#print(labelsTrain)\n",
    "#print(compoundsTrain)\n",
    "#print(smilesTrain)\n",
    "#print(activitiesTrain)\n",
    "\n",
    "#for i in range(len(labelsTrain)):\n",
    "#    print(labelsTrain[i] + \": \", compoundDataTrain[0,i])\n",
    "\n",
    "# def toClassification(y): # The resulting array will contain values of -1 if it is below 4.5 and 1 if it is above\n",
    "#     y = np.array(y)\n",
    "#     classification = (y.astype(float)>4).astype(int)\n",
    "#     return classification * 2 - 1\n",
    "\n",
    "# def normalizeData(train,test,validate):\n",
    "#     for i in range(np.shape(train)[1]):\n",
    "#         std = np.std(train[:,i])\n",
    "#         mean = np.mean(train[:,i])\n",
    "#         if(std == 0):\n",
    "#             std = 1\n",
    "#         train[:,i] = (train[:,i] - mean) / std\n",
    "#         test[:,i] = (test[:,i] - mean) / std\n",
    "#         validate[:,i] = (validate[:,i] - mean) / std\n",
    "#     return train, test, validate\n",
    "\n",
    "# def normalize_2d(matrix):\n",
    "#     norm = np.linalg.norm(matrix,np.inf)\n",
    "#     matrix = matrix/norm  # normalized matrix\n",
    "#     return matrix\n",
    "\n",
    "# normalizeData(compoundDataTrain, compoundDataTest, compoundDataValidate)\n",
    "\n",
    "# print(len(compoundsTrain), len(compoundsTest), len(compoundsValidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b9dda4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcut2d retention: [0.99364773]\n",
      "\ttotal: 99.36477273412306%\n",
      "chi retention: [0.9541968]\n",
      "\ttotal: 95.41967968597896%\n",
      "paoe retention: [0.31495127 0.19509321 0.1390042  0.08838179]\n",
      "\ttotal: 73.74304791821307%\n",
      "smr retention: [0.50634726 0.24568468 0.08792319 0.07435827]\n",
      "\ttotal: 91.43133907840064%\n",
      "slogp retention: [0.44620276 0.22134465 0.15753302]\n",
      "\ttotal: 82.5080420439621%\n",
      "estate_vsa retention: [0.29224011 0.18964809 0.14361318]\n",
      "\ttotal: 62.55013868306437%\n",
      "vsa_estate retention: [0.49255304 0.32866098 0.09719245]\n",
      "\ttotal: 91.84064669216814%\n",
      "fr retention: [0.32153498 0.12578177 0.09944384]\n",
      "\ttotal: 54.676059360511985%\n"
     ]
    }
   ],
   "source": [
    "labelsPCA, trainPCA, testPCA, valPCA = Transformer.applyPCA(labelsTrain,  compoundDataTrain, \n",
    "                                                            compoundDataTest, compoundDataValidate,\n",
    "                                                            endDims=[1,1,4,4,3,3,3,3])\n",
    "\n",
    "labelsMeanPCA, trainMeanPCA = Transformer.useAverageFD(labelsPCA, trainPCA)\n",
    "_, testMeanPCA = Transformer.useAverageFD(labelsPCA, testPCA)\n",
    "_, valMeanPCA = Transformer.useAverageFD(labelsPCA, valPCA)\n",
    "\n",
    "labelsMaxPCA, trainMaxPCA = Transformer.useMaxFD(labelsPCA, trainPCA)\n",
    "_, testMaxPCA = Transformer.useMaxFD(labelsPCA, testPCA)\n",
    "_, valMaxPCA = Transformer.useMaxFD(labelsPCA, valPCA)\n",
    "\n",
    "#after transformations are done assign data\n",
    "dataLabels = labelsMaxPCA\n",
    "trainData = trainMaxPCA\n",
    "testData = testMaxPCA\n",
    "valData = valMaxPCA\n",
    "\n",
    "trainData, testData, valData = Transformer.normalizeData(trainData, testData, valData, newMean=0, newStd=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941488f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = Transformer.toClassification(activitiesTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4c3b173",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 1.2877 - accuracy: 0.2205 - val_loss: 0.6534 - val_accuracy: 0.4029\n",
      "Epoch 2/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5906 - accuracy: 0.4520 - val_loss: 0.5538 - val_accuracy: 0.4500\n",
      "Epoch 3/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.5214 - accuracy: 0.4833 - val_loss: 0.5281 - val_accuracy: 0.4324\n",
      "Epoch 4/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.4808 - accuracy: 0.4914 - val_loss: 0.5160 - val_accuracy: 0.4647\n",
      "Epoch 5/1600\n",
      "680/680 [==============================] - 1s 938us/step - loss: 0.4544 - accuracy: 0.5009 - val_loss: 0.4712 - val_accuracy: 0.4765\n",
      "Epoch 6/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.4377 - accuracy: 0.5024 - val_loss: 0.4759 - val_accuracy: 0.4676\n",
      "Epoch 7/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.5086 - val_loss: 0.4479 - val_accuracy: 0.5088\n",
      "Epoch 8/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.4027 - accuracy: 0.5164 - val_loss: 0.4597 - val_accuracy: 0.4647\n",
      "Epoch 9/1600\n",
      "680/680 [==============================] - 1s 939us/step - loss: 0.3892 - accuracy: 0.5193 - val_loss: 0.4497 - val_accuracy: 0.5000\n",
      "Epoch 10/1600\n",
      "680/680 [==============================] - 1s 960us/step - loss: 0.3839 - accuracy: 0.5256 - val_loss: 0.4549 - val_accuracy: 0.5088\n",
      "Epoch 11/1600\n",
      "680/680 [==============================] - 1s 941us/step - loss: 0.3666 - accuracy: 0.5278 - val_loss: 0.4327 - val_accuracy: 0.4941\n",
      "Epoch 12/1600\n",
      "680/680 [==============================] - 1s 956us/step - loss: 0.3633 - accuracy: 0.5318 - val_loss: 0.4209 - val_accuracy: 0.5000\n",
      "Epoch 13/1600\n",
      "680/680 [==============================] - 1s 977us/step - loss: 0.3556 - accuracy: 0.5293 - val_loss: 0.4155 - val_accuracy: 0.5118\n",
      "Epoch 14/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.3481 - accuracy: 0.5337 - val_loss: 0.4250 - val_accuracy: 0.5000\n",
      "Epoch 15/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.3420 - accuracy: 0.5374 - val_loss: 0.4258 - val_accuracy: 0.5088\n",
      "Epoch 16/1600\n",
      "680/680 [==============================] - 1s 935us/step - loss: 0.3403 - accuracy: 0.5388 - val_loss: 0.4094 - val_accuracy: 0.5235\n",
      "Epoch 17/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.3300 - accuracy: 0.5396 - val_loss: 0.4106 - val_accuracy: 0.5059\n",
      "Epoch 18/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.3337 - accuracy: 0.5392 - val_loss: 0.4250 - val_accuracy: 0.5118\n",
      "Epoch 19/1600\n",
      "680/680 [==============================] - 1s 903us/step - loss: 0.3211 - accuracy: 0.5425 - val_loss: 0.4109 - val_accuracy: 0.5147\n",
      "Epoch 20/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.3155 - accuracy: 0.5447 - val_loss: 0.4107 - val_accuracy: 0.5412\n",
      "Epoch 21/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.3103 - accuracy: 0.5462 - val_loss: 0.4269 - val_accuracy: 0.5353\n",
      "Epoch 22/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.3049 - accuracy: 0.5499 - val_loss: 0.4171 - val_accuracy: 0.5412\n",
      "Epoch 23/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.3028 - accuracy: 0.5477 - val_loss: 0.4072 - val_accuracy: 0.5353\n",
      "Epoch 24/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.3004 - accuracy: 0.5510 - val_loss: 0.4211 - val_accuracy: 0.5265\n",
      "Epoch 25/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.3012 - accuracy: 0.5513 - val_loss: 0.4071 - val_accuracy: 0.5176\n",
      "Epoch 26/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.2901 - accuracy: 0.5543 - val_loss: 0.3973 - val_accuracy: 0.5206\n",
      "Epoch 27/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.2951 - accuracy: 0.5499 - val_loss: 0.4022 - val_accuracy: 0.5176\n",
      "Epoch 28/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.2840 - accuracy: 0.5513 - val_loss: 0.4082 - val_accuracy: 0.5382\n",
      "Epoch 29/1600\n",
      "680/680 [==============================] - 1s 935us/step - loss: 0.2844 - accuracy: 0.5521 - val_loss: 0.4166 - val_accuracy: 0.5235\n",
      "Epoch 30/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.2783 - accuracy: 0.5609 - val_loss: 0.4138 - val_accuracy: 0.5235\n",
      "Epoch 31/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.2819 - accuracy: 0.5554 - val_loss: 0.4324 - val_accuracy: 0.5088\n",
      "Epoch 32/1600\n",
      "680/680 [==============================] - 1s 903us/step - loss: 0.2818 - accuracy: 0.5528 - val_loss: 0.4054 - val_accuracy: 0.5294\n",
      "Epoch 33/1600\n",
      "680/680 [==============================] - 1s 907us/step - loss: 0.2835 - accuracy: 0.5561 - val_loss: 0.3888 - val_accuracy: 0.5206\n",
      "Epoch 34/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.2880 - accuracy: 0.5532 - val_loss: 0.3821 - val_accuracy: 0.5176\n",
      "Epoch 35/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.2673 - accuracy: 0.5569 - val_loss: 0.4024 - val_accuracy: 0.5059\n",
      "Epoch 36/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.2712 - accuracy: 0.5565 - val_loss: 0.3947 - val_accuracy: 0.5441\n",
      "Epoch 37/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.2688 - accuracy: 0.5624 - val_loss: 0.3790 - val_accuracy: 0.5353\n",
      "Epoch 38/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.2662 - accuracy: 0.5613 - val_loss: 0.3725 - val_accuracy: 0.5294\n",
      "Epoch 39/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.2684 - accuracy: 0.5646 - val_loss: 0.3945 - val_accuracy: 0.5176\n",
      "Epoch 40/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.2700 - accuracy: 0.5583 - val_loss: 0.3825 - val_accuracy: 0.5441\n",
      "Epoch 41/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.2606 - accuracy: 0.5635 - val_loss: 0.3713 - val_accuracy: 0.5324\n",
      "Epoch 42/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.2538 - accuracy: 0.5628 - val_loss: 0.3791 - val_accuracy: 0.5088\n",
      "Epoch 43/1600\n",
      "680/680 [==============================] - 1s 904us/step - loss: 0.2639 - accuracy: 0.5639 - val_loss: 0.3735 - val_accuracy: 0.5088\n",
      "Epoch 44/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2545 - accuracy: 0.5620 - val_loss: 0.3874 - val_accuracy: 0.5441\n",
      "Epoch 45/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2542 - accuracy: 0.5661 - val_loss: 0.3686 - val_accuracy: 0.5235\n",
      "Epoch 46/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2596 - accuracy: 0.5620 - val_loss: 0.3997 - val_accuracy: 0.5176\n",
      "Epoch 47/1600\n",
      "680/680 [==============================] - 1s 970us/step - loss: 0.2539 - accuracy: 0.5646 - val_loss: 0.3759 - val_accuracy: 0.5324\n",
      "Epoch 48/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2460 - accuracy: 0.5716 - val_loss: 0.3830 - val_accuracy: 0.5147\n",
      "Epoch 49/1600\n",
      "680/680 [==============================] - 1s 968us/step - loss: 0.2494 - accuracy: 0.5657 - val_loss: 0.3666 - val_accuracy: 0.5382\n",
      "Epoch 50/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.2496 - accuracy: 0.5657 - val_loss: 0.3812 - val_accuracy: 0.5382\n",
      "Epoch 51/1600\n",
      "680/680 [==============================] - 1s 946us/step - loss: 0.2492 - accuracy: 0.5672 - val_loss: 0.4327 - val_accuracy: 0.4941\n",
      "Epoch 52/1600\n",
      "680/680 [==============================] - 1s 983us/step - loss: 0.2489 - accuracy: 0.5683 - val_loss: 0.4119 - val_accuracy: 0.5088\n",
      "Epoch 53/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2514 - accuracy: 0.5650 - val_loss: 0.3758 - val_accuracy: 0.5324\n",
      "Epoch 54/1600\n",
      "680/680 [==============================] - 1s 989us/step - loss: 0.2402 - accuracy: 0.5690 - val_loss: 0.4086 - val_accuracy: 0.5500\n",
      "Epoch 55/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.2421 - accuracy: 0.5712 - val_loss: 0.3851 - val_accuracy: 0.5382\n",
      "Epoch 56/1600\n",
      "680/680 [==============================] - 1s 993us/step - loss: 0.2438 - accuracy: 0.5694 - val_loss: 0.3904 - val_accuracy: 0.5529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.2450 - accuracy: 0.5731 - val_loss: 0.3980 - val_accuracy: 0.5559\n",
      "Epoch 58/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.2405 - accuracy: 0.5672 - val_loss: 0.3835 - val_accuracy: 0.5559\n",
      "Epoch 59/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.2523 - accuracy: 0.5697 - val_loss: 0.3894 - val_accuracy: 0.5353\n",
      "Epoch 60/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2396 - accuracy: 0.5716 - val_loss: 0.3919 - val_accuracy: 0.5353\n",
      "Epoch 61/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2439 - accuracy: 0.5709 - val_loss: 0.3919 - val_accuracy: 0.5324\n",
      "Epoch 62/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2408 - accuracy: 0.5701 - val_loss: 0.3771 - val_accuracy: 0.5441\n",
      "Epoch 63/1600\n",
      "680/680 [==============================] - 1s 971us/step - loss: 0.2315 - accuracy: 0.5731 - val_loss: 0.3942 - val_accuracy: 0.5324\n",
      "Epoch 64/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2440 - accuracy: 0.5723 - val_loss: 0.3813 - val_accuracy: 0.5353\n",
      "Epoch 65/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2279 - accuracy: 0.5745 - val_loss: 0.3933 - val_accuracy: 0.5382\n",
      "Epoch 66/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2352 - accuracy: 0.5712 - val_loss: 0.3706 - val_accuracy: 0.5412\n",
      "Epoch 67/1600\n",
      "680/680 [==============================] - 1s 981us/step - loss: 0.2288 - accuracy: 0.5731 - val_loss: 0.3968 - val_accuracy: 0.5324\n",
      "Epoch 68/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2288 - accuracy: 0.5767 - val_loss: 0.3934 - val_accuracy: 0.5324\n",
      "Epoch 69/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2341 - accuracy: 0.5705 - val_loss: 0.3987 - val_accuracy: 0.5147\n",
      "Epoch 70/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2277 - accuracy: 0.5778 - val_loss: 0.4196 - val_accuracy: 0.5235\n",
      "Epoch 71/1600\n",
      "680/680 [==============================] - 1s 968us/step - loss: 0.2240 - accuracy: 0.5727 - val_loss: 0.4148 - val_accuracy: 0.5412\n",
      "Epoch 72/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2411 - accuracy: 0.5709 - val_loss: 0.4175 - val_accuracy: 0.5412\n",
      "Epoch 73/1600\n",
      "680/680 [==============================] - 1s 947us/step - loss: 0.2295 - accuracy: 0.5749 - val_loss: 0.3920 - val_accuracy: 0.5471\n",
      "Epoch 74/1600\n",
      "680/680 [==============================] - 1s 866us/step - loss: 0.2319 - accuracy: 0.5760 - val_loss: 0.3982 - val_accuracy: 0.5500\n",
      "Epoch 75/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.2223 - accuracy: 0.5760 - val_loss: 0.4255 - val_accuracy: 0.5265\n",
      "Epoch 76/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.2210 - accuracy: 0.5778 - val_loss: 0.3999 - val_accuracy: 0.5353\n",
      "Epoch 77/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.2277 - accuracy: 0.5793 - val_loss: 0.3890 - val_accuracy: 0.5265\n",
      "Epoch 78/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.2303 - accuracy: 0.5756 - val_loss: 0.3811 - val_accuracy: 0.5412\n",
      "Epoch 79/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.2243 - accuracy: 0.5767 - val_loss: 0.4064 - val_accuracy: 0.5588\n",
      "Epoch 80/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2309 - accuracy: 0.5801 - val_loss: 0.4360 - val_accuracy: 0.5206\n",
      "Epoch 81/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2280 - accuracy: 0.5745 - val_loss: 0.3803 - val_accuracy: 0.5294\n",
      "Epoch 82/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.2138 - accuracy: 0.5789 - val_loss: 0.4330 - val_accuracy: 0.5206\n",
      "Epoch 83/1600\n",
      "680/680 [==============================] - 1s 988us/step - loss: 0.2222 - accuracy: 0.5786 - val_loss: 0.4036 - val_accuracy: 0.5559\n",
      "Epoch 84/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2247 - accuracy: 0.5801 - val_loss: 0.4080 - val_accuracy: 0.5353\n",
      "Epoch 85/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.2195 - accuracy: 0.5804 - val_loss: 0.4153 - val_accuracy: 0.5382\n",
      "Epoch 86/1600\n",
      "680/680 [==============================] - 1s 960us/step - loss: 0.2177 - accuracy: 0.5804 - val_loss: 0.3840 - val_accuracy: 0.5559\n",
      "Epoch 87/1600\n",
      "680/680 [==============================] - 1s 972us/step - loss: 0.2190 - accuracy: 0.5801 - val_loss: 0.3825 - val_accuracy: 0.5412\n",
      "Epoch 88/1600\n",
      "680/680 [==============================] - 1s 944us/step - loss: 0.2271 - accuracy: 0.5819 - val_loss: 0.4033 - val_accuracy: 0.5353\n",
      "Epoch 89/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2214 - accuracy: 0.5812 - val_loss: 0.3837 - val_accuracy: 0.5529\n",
      "Epoch 90/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.2239 - accuracy: 0.5808 - val_loss: 0.4272 - val_accuracy: 0.5500\n",
      "Epoch 91/1600\n",
      "680/680 [==============================] - 1s 937us/step - loss: 0.2136 - accuracy: 0.5793 - val_loss: 0.4014 - val_accuracy: 0.5471\n",
      "Epoch 92/1600\n",
      "680/680 [==============================] - 1s 956us/step - loss: 0.2184 - accuracy: 0.5815 - val_loss: 0.3698 - val_accuracy: 0.5412\n",
      "Epoch 93/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.2112 - accuracy: 0.5812 - val_loss: 0.4056 - val_accuracy: 0.5588\n",
      "Epoch 94/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.2226 - accuracy: 0.5801 - val_loss: 0.4066 - val_accuracy: 0.5353\n",
      "Epoch 95/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.2193 - accuracy: 0.5841 - val_loss: 0.4043 - val_accuracy: 0.5294\n",
      "Epoch 96/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.2155 - accuracy: 0.5808 - val_loss: 0.3946 - val_accuracy: 0.5441\n",
      "Epoch 97/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.2222 - accuracy: 0.5826 - val_loss: 0.4214 - val_accuracy: 0.5353\n",
      "Epoch 98/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.2149 - accuracy: 0.5819 - val_loss: 0.4042 - val_accuracy: 0.5324\n",
      "Epoch 99/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.2045 - accuracy: 0.5845 - val_loss: 0.4109 - val_accuracy: 0.5529\n",
      "Epoch 100/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.2122 - accuracy: 0.5859 - val_loss: 0.3907 - val_accuracy: 0.5412\n",
      "Epoch 101/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.2276 - accuracy: 0.5819 - val_loss: 0.3877 - val_accuracy: 0.5618\n",
      "Epoch 102/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.2059 - accuracy: 0.5834 - val_loss: 0.3941 - val_accuracy: 0.5618\n",
      "Epoch 103/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.2043 - accuracy: 0.5885 - val_loss: 0.4215 - val_accuracy: 0.5324\n",
      "Epoch 104/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.2141 - accuracy: 0.5834 - val_loss: 0.3926 - val_accuracy: 0.5500\n",
      "Epoch 105/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.2183 - accuracy: 0.5830 - val_loss: 0.4122 - val_accuracy: 0.5412\n",
      "Epoch 106/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.2093 - accuracy: 0.5834 - val_loss: 0.3987 - val_accuracy: 0.5588\n",
      "Epoch 107/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.2099 - accuracy: 0.5823 - val_loss: 0.4078 - val_accuracy: 0.5471\n",
      "Epoch 108/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.2029 - accuracy: 0.5841 - val_loss: 0.3944 - val_accuracy: 0.5588\n",
      "Epoch 109/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.2032 - accuracy: 0.5870 - val_loss: 0.4047 - val_accuracy: 0.5353\n",
      "Epoch 110/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.2129 - accuracy: 0.5863 - val_loss: 0.4032 - val_accuracy: 0.5471\n",
      "Epoch 111/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.2046 - accuracy: 0.5874 - val_loss: 0.4193 - val_accuracy: 0.5441\n",
      "Epoch 112/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.2114 - accuracy: 0.5819 - val_loss: 0.4086 - val_accuracy: 0.5441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/1600\n",
      "680/680 [==============================] - 1s 868us/step - loss: 0.2045 - accuracy: 0.5863 - val_loss: 0.3872 - val_accuracy: 0.5471\n",
      "Epoch 114/1600\n",
      "680/680 [==============================] - 1s 930us/step - loss: 0.2097 - accuracy: 0.5863 - val_loss: 0.3985 - val_accuracy: 0.5382\n",
      "Epoch 115/1600\n",
      "680/680 [==============================] - 1s 898us/step - loss: 0.2003 - accuracy: 0.5867 - val_loss: 0.3902 - val_accuracy: 0.5412\n",
      "Epoch 116/1600\n",
      "680/680 [==============================] - 1s 930us/step - loss: 0.1988 - accuracy: 0.5874 - val_loss: 0.3910 - val_accuracy: 0.5647\n",
      "Epoch 117/1600\n",
      "680/680 [==============================] - 1s 944us/step - loss: 0.1955 - accuracy: 0.5904 - val_loss: 0.4120 - val_accuracy: 0.5382\n",
      "Epoch 118/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.2250 - accuracy: 0.5830 - val_loss: 0.4140 - val_accuracy: 0.5529\n",
      "Epoch 119/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.2115 - accuracy: 0.5834 - val_loss: 0.4037 - val_accuracy: 0.5382\n",
      "Epoch 120/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.2042 - accuracy: 0.5867 - val_loss: 0.4339 - val_accuracy: 0.5324\n",
      "Epoch 121/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.2031 - accuracy: 0.5845 - val_loss: 0.3895 - val_accuracy: 0.5588\n",
      "Epoch 122/1600\n",
      "680/680 [==============================] - 1s 906us/step - loss: 0.2105 - accuracy: 0.5863 - val_loss: 0.4214 - val_accuracy: 0.5353\n",
      "Epoch 123/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.2090 - accuracy: 0.5859 - val_loss: 0.3894 - val_accuracy: 0.5353\n",
      "Epoch 124/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.2160 - accuracy: 0.5808 - val_loss: 0.3942 - val_accuracy: 0.5647\n",
      "Epoch 125/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.2058 - accuracy: 0.5874 - val_loss: 0.3905 - val_accuracy: 0.5412\n",
      "Epoch 126/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.2015 - accuracy: 0.5856 - val_loss: 0.3975 - val_accuracy: 0.5441\n",
      "Epoch 127/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.2222 - accuracy: 0.5823 - val_loss: 0.3354 - val_accuracy: 0.5500\n",
      "Epoch 128/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.2022 - accuracy: 0.5863 - val_loss: 0.3624 - val_accuracy: 0.5559\n",
      "Epoch 129/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1950 - accuracy: 0.5881 - val_loss: 0.3808 - val_accuracy: 0.5647\n",
      "Epoch 130/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.2038 - accuracy: 0.5863 - val_loss: 0.3874 - val_accuracy: 0.5324\n",
      "Epoch 131/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1936 - accuracy: 0.5881 - val_loss: 0.3657 - val_accuracy: 0.5441\n",
      "Epoch 132/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.2162 - accuracy: 0.5848 - val_loss: 0.3897 - val_accuracy: 0.5618\n",
      "Epoch 133/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1995 - accuracy: 0.5885 - val_loss: 0.3645 - val_accuracy: 0.5588\n",
      "Epoch 134/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1922 - accuracy: 0.5922 - val_loss: 0.3732 - val_accuracy: 0.5618\n",
      "Epoch 135/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.2032 - accuracy: 0.5874 - val_loss: 0.3922 - val_accuracy: 0.5471\n",
      "Epoch 136/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1978 - accuracy: 0.5870 - val_loss: 0.3879 - val_accuracy: 0.5441\n",
      "Epoch 137/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.2031 - accuracy: 0.5881 - val_loss: 0.4776 - val_accuracy: 0.5559\n",
      "Epoch 138/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.2122 - accuracy: 0.5874 - val_loss: 0.3801 - val_accuracy: 0.5353\n",
      "Epoch 139/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1976 - accuracy: 0.5896 - val_loss: 0.3680 - val_accuracy: 0.5559\n",
      "Epoch 140/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.2087 - accuracy: 0.5881 - val_loss: 0.3864 - val_accuracy: 0.5353\n",
      "Epoch 141/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.2061 - accuracy: 0.5870 - val_loss: 0.3692 - val_accuracy: 0.5471\n",
      "Epoch 142/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1980 - accuracy: 0.5878 - val_loss: 0.3753 - val_accuracy: 0.5588\n",
      "Epoch 143/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1995 - accuracy: 0.5878 - val_loss: 0.3745 - val_accuracy: 0.5529\n",
      "Epoch 144/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.2062 - accuracy: 0.5900 - val_loss: 0.3782 - val_accuracy: 0.5588\n",
      "Epoch 145/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1980 - accuracy: 0.5904 - val_loss: 0.3789 - val_accuracy: 0.5441\n",
      "Epoch 146/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.2078 - accuracy: 0.5878 - val_loss: 0.4264 - val_accuracy: 0.5353\n",
      "Epoch 147/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1980 - accuracy: 0.5881 - val_loss: 0.3960 - val_accuracy: 0.5559\n",
      "Epoch 148/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1996 - accuracy: 0.5874 - val_loss: 0.3872 - val_accuracy: 0.5441\n",
      "Epoch 149/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.2074 - accuracy: 0.5863 - val_loss: 0.3916 - val_accuracy: 0.5559\n",
      "Epoch 150/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.2135 - accuracy: 0.5863 - val_loss: 0.4011 - val_accuracy: 0.5441\n",
      "Epoch 151/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.2043 - accuracy: 0.5885 - val_loss: 0.3854 - val_accuracy: 0.5559\n",
      "Epoch 152/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1891 - accuracy: 0.5926 - val_loss: 0.4497 - val_accuracy: 0.5382\n",
      "Epoch 153/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1964 - accuracy: 0.5870 - val_loss: 0.4163 - val_accuracy: 0.5412\n",
      "Epoch 154/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1961 - accuracy: 0.5915 - val_loss: 0.3860 - val_accuracy: 0.5559\n",
      "Epoch 155/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.2259 - accuracy: 0.5837 - val_loss: 0.3902 - val_accuracy: 0.5500\n",
      "Epoch 156/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1960 - accuracy: 0.5881 - val_loss: 0.3838 - val_accuracy: 0.5735\n",
      "Epoch 157/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1856 - accuracy: 0.5940 - val_loss: 0.3835 - val_accuracy: 0.5500\n",
      "Epoch 158/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.2003 - accuracy: 0.5881 - val_loss: 0.4017 - val_accuracy: 0.5529\n",
      "Epoch 159/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1961 - accuracy: 0.5915 - val_loss: 0.3950 - val_accuracy: 0.5500\n",
      "Epoch 160/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.2058 - accuracy: 0.5863 - val_loss: 0.3833 - val_accuracy: 0.5529\n",
      "Epoch 161/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1999 - accuracy: 0.5915 - val_loss: 0.3982 - val_accuracy: 0.5412\n",
      "Epoch 162/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.2004 - accuracy: 0.5896 - val_loss: 0.3710 - val_accuracy: 0.5559\n",
      "Epoch 163/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1916 - accuracy: 0.5915 - val_loss: 0.4035 - val_accuracy: 0.5412\n",
      "Epoch 164/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.2098 - accuracy: 0.5874 - val_loss: 0.3712 - val_accuracy: 0.5382\n",
      "Epoch 165/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.2039 - accuracy: 0.5889 - val_loss: 0.3845 - val_accuracy: 0.5618\n",
      "Epoch 166/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.2048 - accuracy: 0.5896 - val_loss: 0.3991 - val_accuracy: 0.5647\n",
      "Epoch 167/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1985 - accuracy: 0.5881 - val_loss: 0.4466 - val_accuracy: 0.5294\n",
      "Epoch 168/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 918us/step - loss: 0.2016 - accuracy: 0.5885 - val_loss: 0.3979 - val_accuracy: 0.5441\n",
      "Epoch 169/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1957 - accuracy: 0.5900 - val_loss: 0.3822 - val_accuracy: 0.5588\n",
      "Epoch 170/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1969 - accuracy: 0.5885 - val_loss: 0.3816 - val_accuracy: 0.5500\n",
      "Epoch 171/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1992 - accuracy: 0.5904 - val_loss: 0.3883 - val_accuracy: 0.5353\n",
      "Epoch 172/1600\n",
      "680/680 [==============================] - 1s 936us/step - loss: 0.1966 - accuracy: 0.5900 - val_loss: 0.4127 - val_accuracy: 0.5441\n",
      "Epoch 173/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1911 - accuracy: 0.5911 - val_loss: 0.3939 - val_accuracy: 0.5559\n",
      "Epoch 174/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.2050 - accuracy: 0.5874 - val_loss: 0.4280 - val_accuracy: 0.5382\n",
      "Epoch 175/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1936 - accuracy: 0.5907 - val_loss: 0.3839 - val_accuracy: 0.5382\n",
      "Epoch 176/1600\n",
      "680/680 [==============================] - 1s 868us/step - loss: 0.1906 - accuracy: 0.5904 - val_loss: 0.3910 - val_accuracy: 0.5559\n",
      "Epoch 177/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.2076 - accuracy: 0.5893 - val_loss: 0.3966 - val_accuracy: 0.5618\n",
      "Epoch 178/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1938 - accuracy: 0.5896 - val_loss: 0.3793 - val_accuracy: 0.5353\n",
      "Epoch 179/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.1896 - accuracy: 0.5915 - val_loss: 0.4130 - val_accuracy: 0.5412\n",
      "Epoch 180/1600\n",
      "680/680 [==============================] - 1s 937us/step - loss: 0.1893 - accuracy: 0.5933 - val_loss: 0.3753 - val_accuracy: 0.5500\n",
      "Epoch 181/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1968 - accuracy: 0.5896 - val_loss: 0.3578 - val_accuracy: 0.5588\n",
      "Epoch 182/1600\n",
      "680/680 [==============================] - 1s 934us/step - loss: 0.1970 - accuracy: 0.5904 - val_loss: 0.4275 - val_accuracy: 0.5706\n",
      "Epoch 183/1600\n",
      "680/680 [==============================] - 1s 936us/step - loss: 0.1985 - accuracy: 0.5904 - val_loss: 0.3834 - val_accuracy: 0.5500\n",
      "Epoch 184/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1900 - accuracy: 0.5911 - val_loss: 0.3757 - val_accuracy: 0.5559\n",
      "Epoch 185/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1954 - accuracy: 0.5900 - val_loss: 0.3953 - val_accuracy: 0.5529\n",
      "Epoch 186/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1840 - accuracy: 0.5944 - val_loss: 0.3900 - val_accuracy: 0.5471\n",
      "Epoch 187/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.1929 - accuracy: 0.5915 - val_loss: 0.4100 - val_accuracy: 0.5324\n",
      "Epoch 188/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1962 - accuracy: 0.5900 - val_loss: 0.3981 - val_accuracy: 0.5676\n",
      "Epoch 189/1600\n",
      "680/680 [==============================] - 1s 930us/step - loss: 0.1994 - accuracy: 0.5904 - val_loss: 0.4235 - val_accuracy: 0.5412\n",
      "Epoch 190/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1945 - accuracy: 0.5915 - val_loss: 0.4167 - val_accuracy: 0.5353\n",
      "Epoch 191/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.2030 - accuracy: 0.5878 - val_loss: 0.4345 - val_accuracy: 0.5471\n",
      "Epoch 192/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.2032 - accuracy: 0.5893 - val_loss: 0.4059 - val_accuracy: 0.5353\n",
      "Epoch 193/1600\n",
      "680/680 [==============================] - 1s 901us/step - loss: 0.1891 - accuracy: 0.5926 - val_loss: 0.4146 - val_accuracy: 0.5382\n",
      "Epoch 194/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1943 - accuracy: 0.5907 - val_loss: 0.4119 - val_accuracy: 0.5441\n",
      "Epoch 195/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.2009 - accuracy: 0.5885 - val_loss: 0.3838 - val_accuracy: 0.5529\n",
      "Epoch 196/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1911 - accuracy: 0.5926 - val_loss: 0.4254 - val_accuracy: 0.5559\n",
      "Epoch 197/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.2072 - accuracy: 0.5878 - val_loss: 0.3762 - val_accuracy: 0.5618\n",
      "Epoch 198/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1826 - accuracy: 0.5944 - val_loss: 0.4219 - val_accuracy: 0.5412\n",
      "Epoch 199/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1911 - accuracy: 0.5904 - val_loss: 0.4229 - val_accuracy: 0.5412\n",
      "Epoch 200/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.2023 - accuracy: 0.5900 - val_loss: 0.4404 - val_accuracy: 0.5382\n",
      "Epoch 201/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1990 - accuracy: 0.5893 - val_loss: 0.3863 - val_accuracy: 0.5500\n",
      "Epoch 202/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1923 - accuracy: 0.5907 - val_loss: 0.3625 - val_accuracy: 0.5618\n",
      "Epoch 203/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1815 - accuracy: 0.5929 - val_loss: 0.3833 - val_accuracy: 0.5559\n",
      "Epoch 204/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1927 - accuracy: 0.5900 - val_loss: 0.4124 - val_accuracy: 0.5441\n",
      "Epoch 205/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.2014 - accuracy: 0.5922 - val_loss: 0.3603 - val_accuracy: 0.5559\n",
      "Epoch 206/1600\n",
      "680/680 [==============================] - 1s 868us/step - loss: 0.1897 - accuracy: 0.5904 - val_loss: 0.4021 - val_accuracy: 0.5471\n",
      "Epoch 207/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1934 - accuracy: 0.5904 - val_loss: 0.3756 - val_accuracy: 0.5559\n",
      "Epoch 208/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1917 - accuracy: 0.5915 - val_loss: 0.3947 - val_accuracy: 0.5559\n",
      "Epoch 209/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.2082 - accuracy: 0.5874 - val_loss: 0.3886 - val_accuracy: 0.5353\n",
      "Epoch 210/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1948 - accuracy: 0.5889 - val_loss: 0.4285 - val_accuracy: 0.5324\n",
      "Epoch 211/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1940 - accuracy: 0.5896 - val_loss: 0.4531 - val_accuracy: 0.5412\n",
      "Epoch 212/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1955 - accuracy: 0.5926 - val_loss: 0.3800 - val_accuracy: 0.5559\n",
      "Epoch 213/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1964 - accuracy: 0.5915 - val_loss: 0.4021 - val_accuracy: 0.5441\n",
      "Epoch 214/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1855 - accuracy: 0.5926 - val_loss: 0.3654 - val_accuracy: 0.5500\n",
      "Epoch 215/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.2094 - accuracy: 0.5881 - val_loss: 0.4247 - val_accuracy: 0.5471\n",
      "Epoch 216/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1963 - accuracy: 0.5918 - val_loss: 0.3962 - val_accuracy: 0.5412\n",
      "Epoch 217/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1968 - accuracy: 0.5918 - val_loss: 0.3861 - val_accuracy: 0.5618\n",
      "Epoch 218/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1761 - accuracy: 0.5929 - val_loss: 0.3851 - val_accuracy: 0.5618\n",
      "Epoch 219/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1949 - accuracy: 0.5918 - val_loss: 0.4183 - val_accuracy: 0.5382\n",
      "Epoch 220/1600\n",
      "680/680 [==============================] - 1s 945us/step - loss: 0.1941 - accuracy: 0.5922 - val_loss: 0.4122 - val_accuracy: 0.5353\n",
      "Epoch 221/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1906 - accuracy: 0.5915 - val_loss: 0.3943 - val_accuracy: 0.5382\n",
      "Epoch 222/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1978 - accuracy: 0.5878 - val_loss: 0.4033 - val_accuracy: 0.5412\n",
      "Epoch 223/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 893us/step - loss: 0.1918 - accuracy: 0.5955 - val_loss: 0.3575 - val_accuracy: 0.5647\n",
      "Epoch 224/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1986 - accuracy: 0.5889 - val_loss: 0.4055 - val_accuracy: 0.5382\n",
      "Epoch 225/1600\n",
      "680/680 [==============================] - 1s 936us/step - loss: 0.1862 - accuracy: 0.5918 - val_loss: 0.3966 - val_accuracy: 0.5412\n",
      "Epoch 226/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.2007 - accuracy: 0.5889 - val_loss: 0.4059 - val_accuracy: 0.5441\n",
      "Epoch 227/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1900 - accuracy: 0.5922 - val_loss: 0.3566 - val_accuracy: 0.5588\n",
      "Epoch 228/1600\n",
      "680/680 [==============================] - 1s 901us/step - loss: 0.1778 - accuracy: 0.5966 - val_loss: 0.3464 - val_accuracy: 0.5588\n",
      "Epoch 229/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1970 - accuracy: 0.5918 - val_loss: 0.4444 - val_accuracy: 0.5206\n",
      "Epoch 230/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1863 - accuracy: 0.5926 - val_loss: 0.4104 - val_accuracy: 0.5441\n",
      "Epoch 231/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1875 - accuracy: 0.5940 - val_loss: 0.4229 - val_accuracy: 0.5471\n",
      "Epoch 232/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1925 - accuracy: 0.5900 - val_loss: 0.4041 - val_accuracy: 0.5471\n",
      "Epoch 233/1600\n",
      "680/680 [==============================] - 1s 904us/step - loss: 0.1966 - accuracy: 0.5937 - val_loss: 0.4135 - val_accuracy: 0.5500\n",
      "Epoch 234/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1884 - accuracy: 0.5937 - val_loss: 0.3991 - val_accuracy: 0.5441\n",
      "Epoch 235/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1821 - accuracy: 0.5922 - val_loss: 0.3904 - val_accuracy: 0.5559\n",
      "Epoch 236/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1978 - accuracy: 0.5929 - val_loss: 0.3734 - val_accuracy: 0.5588\n",
      "Epoch 237/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1897 - accuracy: 0.5918 - val_loss: 0.4235 - val_accuracy: 0.5412\n",
      "Epoch 238/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1931 - accuracy: 0.5929 - val_loss: 0.4222 - val_accuracy: 0.5412\n",
      "Epoch 239/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1886 - accuracy: 0.5926 - val_loss: 0.4650 - val_accuracy: 0.5147\n",
      "Epoch 240/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1836 - accuracy: 0.5933 - val_loss: 0.4173 - val_accuracy: 0.5382\n",
      "Epoch 241/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1888 - accuracy: 0.5922 - val_loss: 0.4000 - val_accuracy: 0.5559\n",
      "Epoch 242/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1954 - accuracy: 0.5929 - val_loss: 0.3609 - val_accuracy: 0.5588\n",
      "Epoch 243/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.1928 - accuracy: 0.5896 - val_loss: 0.3852 - val_accuracy: 0.5529\n",
      "Epoch 244/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1772 - accuracy: 0.5966 - val_loss: 0.4154 - val_accuracy: 0.5353\n",
      "Epoch 245/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1925 - accuracy: 0.5929 - val_loss: 0.4211 - val_accuracy: 0.5441\n",
      "Epoch 246/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1911 - accuracy: 0.5922 - val_loss: 0.4092 - val_accuracy: 0.5735\n",
      "Epoch 247/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1914 - accuracy: 0.5951 - val_loss: 0.3937 - val_accuracy: 0.5529\n",
      "Epoch 248/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1888 - accuracy: 0.5948 - val_loss: 0.4001 - val_accuracy: 0.5647\n",
      "Epoch 249/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1879 - accuracy: 0.5900 - val_loss: 0.4054 - val_accuracy: 0.5382\n",
      "Epoch 250/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.2009 - accuracy: 0.5896 - val_loss: 0.4114 - val_accuracy: 0.5441\n",
      "Epoch 251/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1873 - accuracy: 0.5911 - val_loss: 0.3837 - val_accuracy: 0.5441\n",
      "Epoch 252/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1790 - accuracy: 0.5940 - val_loss: 0.4081 - val_accuracy: 0.5559\n",
      "Epoch 253/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1993 - accuracy: 0.5918 - val_loss: 0.3984 - val_accuracy: 0.5529\n",
      "Epoch 254/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1943 - accuracy: 0.5915 - val_loss: 0.4138 - val_accuracy: 0.5471\n",
      "Epoch 255/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1927 - accuracy: 0.5915 - val_loss: 0.4004 - val_accuracy: 0.5588\n",
      "Epoch 256/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1969 - accuracy: 0.5893 - val_loss: 0.4085 - val_accuracy: 0.5441\n",
      "Epoch 257/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1799 - accuracy: 0.5929 - val_loss: 0.4127 - val_accuracy: 0.5735\n",
      "Epoch 258/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1846 - accuracy: 0.5959 - val_loss: 0.4018 - val_accuracy: 0.5500\n",
      "Epoch 259/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1866 - accuracy: 0.5970 - val_loss: 0.4116 - val_accuracy: 0.5382\n",
      "Epoch 260/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1961 - accuracy: 0.5915 - val_loss: 0.3725 - val_accuracy: 0.5441\n",
      "Epoch 261/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1840 - accuracy: 0.5944 - val_loss: 0.3933 - val_accuracy: 0.5559\n",
      "Epoch 262/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1904 - accuracy: 0.5951 - val_loss: 0.3737 - val_accuracy: 0.5588\n",
      "Epoch 263/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1868 - accuracy: 0.5933 - val_loss: 0.3845 - val_accuracy: 0.5500\n",
      "Epoch 264/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1737 - accuracy: 0.5974 - val_loss: 0.3805 - val_accuracy: 0.5588\n",
      "Epoch 265/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1994 - accuracy: 0.5929 - val_loss: 0.4052 - val_accuracy: 0.5412\n",
      "Epoch 266/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.1951 - accuracy: 0.5900 - val_loss: 0.3894 - val_accuracy: 0.5676\n",
      "Epoch 267/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1895 - accuracy: 0.5911 - val_loss: 0.3722 - val_accuracy: 0.5441\n",
      "Epoch 268/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1994 - accuracy: 0.5863 - val_loss: 0.3893 - val_accuracy: 0.5441\n",
      "Epoch 269/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1885 - accuracy: 0.5915 - val_loss: 0.3903 - val_accuracy: 0.5529\n",
      "Epoch 270/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1792 - accuracy: 0.5970 - val_loss: 0.3960 - val_accuracy: 0.5471\n",
      "Epoch 271/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1976 - accuracy: 0.5940 - val_loss: 0.3798 - val_accuracy: 0.5441\n",
      "Epoch 272/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1884 - accuracy: 0.5926 - val_loss: 0.4006 - val_accuracy: 0.5412\n",
      "Epoch 273/1600\n",
      "680/680 [==============================] - 1s 944us/step - loss: 0.1814 - accuracy: 0.5937 - val_loss: 0.3642 - val_accuracy: 0.5471\n",
      "Epoch 274/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.2124 - accuracy: 0.5848 - val_loss: 0.3969 - val_accuracy: 0.5412\n",
      "Epoch 275/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1959 - accuracy: 0.5904 - val_loss: 0.3689 - val_accuracy: 0.5471\n",
      "Epoch 276/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1942 - accuracy: 0.5911 - val_loss: 0.3881 - val_accuracy: 0.5559\n",
      "Epoch 277/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.1860 - accuracy: 0.5959 - val_loss: 0.4036 - val_accuracy: 0.5559\n",
      "Epoch 278/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 937us/step - loss: 0.1998 - accuracy: 0.5911 - val_loss: 0.3852 - val_accuracy: 0.5265\n",
      "Epoch 279/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1921 - accuracy: 0.5915 - val_loss: 0.4065 - val_accuracy: 0.5412\n",
      "Epoch 280/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1962 - accuracy: 0.5937 - val_loss: 0.3910 - val_accuracy: 0.5471\n",
      "Epoch 281/1600\n",
      "680/680 [==============================] - 1s 942us/step - loss: 0.1867 - accuracy: 0.5937 - val_loss: 0.4078 - val_accuracy: 0.5500\n",
      "Epoch 282/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1858 - accuracy: 0.5929 - val_loss: 0.3702 - val_accuracy: 0.5529\n",
      "Epoch 283/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1878 - accuracy: 0.5900 - val_loss: 0.4068 - val_accuracy: 0.5441\n",
      "Epoch 284/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1824 - accuracy: 0.5926 - val_loss: 0.3938 - val_accuracy: 0.5559\n",
      "Epoch 285/1600\n",
      "680/680 [==============================] - 1s 907us/step - loss: 0.1949 - accuracy: 0.5911 - val_loss: 0.3760 - val_accuracy: 0.5618\n",
      "Epoch 286/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1880 - accuracy: 0.5926 - val_loss: 0.3998 - val_accuracy: 0.5441\n",
      "Epoch 287/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.2003 - accuracy: 0.5907 - val_loss: 0.4234 - val_accuracy: 0.5441\n",
      "Epoch 288/1600\n",
      "680/680 [==============================] - 1s 935us/step - loss: 0.1831 - accuracy: 0.5944 - val_loss: 0.3721 - val_accuracy: 0.5588\n",
      "Epoch 289/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.1781 - accuracy: 0.5959 - val_loss: 0.3982 - val_accuracy: 0.5441\n",
      "Epoch 290/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1826 - accuracy: 0.5922 - val_loss: 0.3545 - val_accuracy: 0.5529\n",
      "Epoch 291/1600\n",
      "680/680 [==============================] - 1s 930us/step - loss: 0.2043 - accuracy: 0.5900 - val_loss: 0.3715 - val_accuracy: 0.5529\n",
      "Epoch 292/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1880 - accuracy: 0.5933 - val_loss: 0.3814 - val_accuracy: 0.5471\n",
      "Epoch 293/1600\n",
      "680/680 [==============================] - 1s 937us/step - loss: 0.1803 - accuracy: 0.5926 - val_loss: 0.3901 - val_accuracy: 0.5529\n",
      "Epoch 294/1600\n",
      "680/680 [==============================] - 1s 902us/step - loss: 0.1913 - accuracy: 0.5933 - val_loss: 0.3901 - val_accuracy: 0.5441\n",
      "Epoch 295/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1837 - accuracy: 0.5948 - val_loss: 0.4000 - val_accuracy: 0.5382\n",
      "Epoch 296/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1899 - accuracy: 0.5926 - val_loss: 0.3737 - val_accuracy: 0.5529\n",
      "Epoch 297/1600\n",
      "680/680 [==============================] - 1s 938us/step - loss: 0.1874 - accuracy: 0.5904 - val_loss: 0.3792 - val_accuracy: 0.5559\n",
      "Epoch 298/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.2016 - accuracy: 0.5896 - val_loss: 0.3923 - val_accuracy: 0.5353\n",
      "Epoch 299/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1846 - accuracy: 0.5922 - val_loss: 0.4066 - val_accuracy: 0.5353\n",
      "Epoch 300/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1811 - accuracy: 0.5911 - val_loss: 0.3899 - val_accuracy: 0.5382\n",
      "Epoch 301/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1850 - accuracy: 0.5911 - val_loss: 0.4124 - val_accuracy: 0.5441\n",
      "Epoch 302/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1936 - accuracy: 0.5915 - val_loss: 0.3852 - val_accuracy: 0.5529\n",
      "Epoch 303/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1871 - accuracy: 0.5911 - val_loss: 0.4034 - val_accuracy: 0.5471\n",
      "Epoch 304/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1811 - accuracy: 0.5951 - val_loss: 0.4038 - val_accuracy: 0.5588\n",
      "Epoch 305/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1790 - accuracy: 0.5948 - val_loss: 0.4258 - val_accuracy: 0.5382\n",
      "Epoch 306/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1937 - accuracy: 0.5926 - val_loss: 0.3780 - val_accuracy: 0.5529\n",
      "Epoch 307/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1841 - accuracy: 0.5926 - val_loss: 0.3741 - val_accuracy: 0.5529\n",
      "Epoch 308/1600\n",
      "680/680 [==============================] - 1s 901us/step - loss: 0.1857 - accuracy: 0.5937 - val_loss: 0.3835 - val_accuracy: 0.5588\n",
      "Epoch 309/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1808 - accuracy: 0.5944 - val_loss: 0.3795 - val_accuracy: 0.5559\n",
      "Epoch 310/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1950 - accuracy: 0.5929 - val_loss: 0.3588 - val_accuracy: 0.5529\n",
      "Epoch 311/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1873 - accuracy: 0.5922 - val_loss: 0.4257 - val_accuracy: 0.5412\n",
      "Epoch 312/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1808 - accuracy: 0.5937 - val_loss: 0.3926 - val_accuracy: 0.5529\n",
      "Epoch 313/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1882 - accuracy: 0.5933 - val_loss: 0.3937 - val_accuracy: 0.5441\n",
      "Epoch 314/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1822 - accuracy: 0.5933 - val_loss: 0.3908 - val_accuracy: 0.5529\n",
      "Epoch 315/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1827 - accuracy: 0.5940 - val_loss: 0.3461 - val_accuracy: 0.5588\n",
      "Epoch 316/1600\n",
      "680/680 [==============================] - 1s 902us/step - loss: 0.1866 - accuracy: 0.5948 - val_loss: 0.3723 - val_accuracy: 0.5706\n",
      "Epoch 317/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1874 - accuracy: 0.5951 - val_loss: 0.3852 - val_accuracy: 0.5441\n",
      "Epoch 318/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1956 - accuracy: 0.5918 - val_loss: 0.3721 - val_accuracy: 0.5647\n",
      "Epoch 319/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1791 - accuracy: 0.5940 - val_loss: 0.3581 - val_accuracy: 0.5588\n",
      "Epoch 320/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1875 - accuracy: 0.5937 - val_loss: 0.3951 - val_accuracy: 0.5412\n",
      "Epoch 321/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.2086 - accuracy: 0.5900 - val_loss: 0.3685 - val_accuracy: 0.5412\n",
      "Epoch 322/1600\n",
      "680/680 [==============================] - 1s 904us/step - loss: 0.1893 - accuracy: 0.5929 - val_loss: 0.4105 - val_accuracy: 0.5265\n",
      "Epoch 323/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1843 - accuracy: 0.5937 - val_loss: 0.3839 - val_accuracy: 0.5588\n",
      "Epoch 324/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1872 - accuracy: 0.5937 - val_loss: 0.3734 - val_accuracy: 0.5529\n",
      "Epoch 325/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1895 - accuracy: 0.5944 - val_loss: 0.3655 - val_accuracy: 0.5559\n",
      "Epoch 326/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1908 - accuracy: 0.5926 - val_loss: 0.3692 - val_accuracy: 0.5529\n",
      "Epoch 327/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1835 - accuracy: 0.5929 - val_loss: 0.3657 - val_accuracy: 0.5559\n",
      "Epoch 328/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1815 - accuracy: 0.5929 - val_loss: 0.3957 - val_accuracy: 0.5529\n",
      "Epoch 329/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.1800 - accuracy: 0.5940 - val_loss: 0.3877 - val_accuracy: 0.5500\n",
      "Epoch 330/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.2048 - accuracy: 0.5893 - val_loss: 0.3868 - val_accuracy: 0.5382\n",
      "Epoch 331/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1753 - accuracy: 0.5955 - val_loss: 0.4092 - val_accuracy: 0.5324\n",
      "Epoch 332/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1965 - accuracy: 0.5907 - val_loss: 0.4008 - val_accuracy: 0.5471\n",
      "Epoch 333/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 878us/step - loss: 0.1706 - accuracy: 0.5962 - val_loss: 0.3606 - val_accuracy: 0.5471\n",
      "Epoch 334/1600\n",
      "680/680 [==============================] - 1s 898us/step - loss: 0.1797 - accuracy: 0.5959 - val_loss: 0.4249 - val_accuracy: 0.5324\n",
      "Epoch 335/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.2065 - accuracy: 0.5904 - val_loss: 0.3632 - val_accuracy: 0.5529\n",
      "Epoch 336/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1858 - accuracy: 0.5918 - val_loss: 0.3783 - val_accuracy: 0.5441\n",
      "Epoch 337/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1811 - accuracy: 0.5948 - val_loss: 0.4058 - val_accuracy: 0.5588\n",
      "Epoch 338/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1921 - accuracy: 0.5915 - val_loss: 0.3495 - val_accuracy: 0.5588\n",
      "Epoch 339/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1772 - accuracy: 0.5937 - val_loss: 0.4115 - val_accuracy: 0.5353\n",
      "Epoch 340/1600\n",
      "680/680 [==============================] - 1s 938us/step - loss: 0.1839 - accuracy: 0.5955 - val_loss: 0.3691 - val_accuracy: 0.5618\n",
      "Epoch 341/1600\n",
      "680/680 [==============================] - 1s 934us/step - loss: 0.1904 - accuracy: 0.5926 - val_loss: 0.3659 - val_accuracy: 0.5471\n",
      "Epoch 342/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1928 - accuracy: 0.5907 - val_loss: 0.4001 - val_accuracy: 0.5412\n",
      "Epoch 343/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1818 - accuracy: 0.5926 - val_loss: 0.4176 - val_accuracy: 0.5382\n",
      "Epoch 344/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.1859 - accuracy: 0.5926 - val_loss: 0.3840 - val_accuracy: 0.5471\n",
      "Epoch 345/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.1821 - accuracy: 0.5937 - val_loss: 0.4363 - val_accuracy: 0.5588\n",
      "Epoch 346/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.2020 - accuracy: 0.5922 - val_loss: 0.3601 - val_accuracy: 0.5529\n",
      "Epoch 347/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1749 - accuracy: 0.5962 - val_loss: 0.3986 - val_accuracy: 0.5324\n",
      "Epoch 348/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1745 - accuracy: 0.5970 - val_loss: 0.3881 - val_accuracy: 0.5500\n",
      "Epoch 349/1600\n",
      "680/680 [==============================] - 1s 934us/step - loss: 0.2002 - accuracy: 0.5915 - val_loss: 0.4014 - val_accuracy: 0.5441\n",
      "Epoch 350/1600\n",
      "680/680 [==============================] - 1s 938us/step - loss: 0.1807 - accuracy: 0.5926 - val_loss: 0.3888 - val_accuracy: 0.5471\n",
      "Epoch 351/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1882 - accuracy: 0.5929 - val_loss: 0.3659 - val_accuracy: 0.5500\n",
      "Epoch 352/1600\n",
      "680/680 [==============================] - 1s 934us/step - loss: 0.1788 - accuracy: 0.5966 - val_loss: 0.3830 - val_accuracy: 0.5559\n",
      "Epoch 353/1600\n",
      "680/680 [==============================] - 1s 937us/step - loss: 0.1868 - accuracy: 0.5940 - val_loss: 0.3670 - val_accuracy: 0.5529\n",
      "Epoch 354/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1927 - accuracy: 0.5907 - val_loss: 0.3862 - val_accuracy: 0.5353\n",
      "Epoch 355/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1992 - accuracy: 0.5896 - val_loss: 0.4490 - val_accuracy: 0.5324\n",
      "Epoch 356/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1920 - accuracy: 0.5944 - val_loss: 0.3731 - val_accuracy: 0.5441\n",
      "Epoch 357/1600\n",
      "680/680 [==============================] - 1s 938us/step - loss: 0.1862 - accuracy: 0.5940 - val_loss: 0.3327 - val_accuracy: 0.5559\n",
      "Epoch 358/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1807 - accuracy: 0.5937 - val_loss: 0.3821 - val_accuracy: 0.5559\n",
      "Epoch 359/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1841 - accuracy: 0.5937 - val_loss: 0.4111 - val_accuracy: 0.5559\n",
      "Epoch 360/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.2009 - accuracy: 0.5881 - val_loss: 0.3902 - val_accuracy: 0.5471\n",
      "Epoch 361/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.1797 - accuracy: 0.5955 - val_loss: 0.4074 - val_accuracy: 0.5559\n",
      "Epoch 362/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1940 - accuracy: 0.5915 - val_loss: 0.3616 - val_accuracy: 0.5559\n",
      "Epoch 363/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1795 - accuracy: 0.5955 - val_loss: 0.3569 - val_accuracy: 0.5441\n",
      "Epoch 364/1600\n",
      "680/680 [==============================] - 1s 864us/step - loss: 0.1879 - accuracy: 0.5948 - val_loss: 0.3768 - val_accuracy: 0.5529\n",
      "Epoch 365/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1837 - accuracy: 0.5959 - val_loss: 0.3696 - val_accuracy: 0.5588\n",
      "Epoch 366/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1989 - accuracy: 0.5915 - val_loss: 0.4547 - val_accuracy: 0.5382\n",
      "Epoch 367/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1832 - accuracy: 0.5922 - val_loss: 0.3806 - val_accuracy: 0.5588\n",
      "Epoch 368/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1815 - accuracy: 0.5951 - val_loss: 0.3988 - val_accuracy: 0.5382\n",
      "Epoch 369/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1839 - accuracy: 0.5907 - val_loss: 0.4112 - val_accuracy: 0.5412\n",
      "Epoch 370/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1988 - accuracy: 0.5911 - val_loss: 0.3967 - val_accuracy: 0.5412\n",
      "Epoch 371/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1790 - accuracy: 0.5948 - val_loss: 0.3605 - val_accuracy: 0.5559\n",
      "Epoch 372/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1814 - accuracy: 0.5951 - val_loss: 0.4045 - val_accuracy: 0.5382\n",
      "Epoch 373/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1842 - accuracy: 0.5918 - val_loss: 0.3926 - val_accuracy: 0.5471\n",
      "Epoch 374/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1922 - accuracy: 0.5918 - val_loss: 0.3699 - val_accuracy: 0.5559\n",
      "Epoch 375/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.2021 - accuracy: 0.5893 - val_loss: 0.4441 - val_accuracy: 0.5294\n",
      "Epoch 376/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1769 - accuracy: 0.5955 - val_loss: 0.4224 - val_accuracy: 0.5382\n",
      "Epoch 377/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1858 - accuracy: 0.5915 - val_loss: 0.4358 - val_accuracy: 0.5500\n",
      "Epoch 378/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1855 - accuracy: 0.5944 - val_loss: 0.3743 - val_accuracy: 0.5588\n",
      "Epoch 379/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1924 - accuracy: 0.5907 - val_loss: 0.3883 - val_accuracy: 0.5559\n",
      "Epoch 380/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1897 - accuracy: 0.5926 - val_loss: 0.4037 - val_accuracy: 0.5500\n",
      "Epoch 381/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1955 - accuracy: 0.5911 - val_loss: 0.4079 - val_accuracy: 0.5500\n",
      "Epoch 382/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1807 - accuracy: 0.5948 - val_loss: 0.3831 - val_accuracy: 0.5588\n",
      "Epoch 383/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1995 - accuracy: 0.5893 - val_loss: 0.3839 - val_accuracy: 0.5471\n",
      "Epoch 384/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1978 - accuracy: 0.5915 - val_loss: 0.3795 - val_accuracy: 0.5559\n",
      "Epoch 385/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1730 - accuracy: 0.5951 - val_loss: 0.4017 - val_accuracy: 0.5500\n",
      "Epoch 386/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1853 - accuracy: 0.5944 - val_loss: 0.3851 - val_accuracy: 0.5588\n",
      "Epoch 387/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1816 - accuracy: 0.5951 - val_loss: 0.3798 - val_accuracy: 0.5500\n",
      "Epoch 388/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 877us/step - loss: 0.1863 - accuracy: 0.5944 - val_loss: 0.3772 - val_accuracy: 0.5529\n",
      "Epoch 389/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1932 - accuracy: 0.5893 - val_loss: 0.3905 - val_accuracy: 0.5529\n",
      "Epoch 390/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1768 - accuracy: 0.5974 - val_loss: 0.3864 - val_accuracy: 0.5529\n",
      "Epoch 391/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1830 - accuracy: 0.5959 - val_loss: 0.4090 - val_accuracy: 0.5500\n",
      "Epoch 392/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1838 - accuracy: 0.5937 - val_loss: 0.3959 - val_accuracy: 0.5529\n",
      "Epoch 393/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1858 - accuracy: 0.5937 - val_loss: 0.4040 - val_accuracy: 0.5441\n",
      "Epoch 394/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.2082 - accuracy: 0.5881 - val_loss: 0.3743 - val_accuracy: 0.5500\n",
      "Epoch 395/1600\n",
      "680/680 [==============================] - 1s 906us/step - loss: 0.1784 - accuracy: 0.5937 - val_loss: 0.3959 - val_accuracy: 0.5588\n",
      "Epoch 396/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.2001 - accuracy: 0.5918 - val_loss: 0.3849 - val_accuracy: 0.5500\n",
      "Epoch 397/1600\n",
      "680/680 [==============================] - 1s 937us/step - loss: 0.1793 - accuracy: 0.5951 - val_loss: 0.3540 - val_accuracy: 0.5618\n",
      "Epoch 398/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1850 - accuracy: 0.5933 - val_loss: 0.4182 - val_accuracy: 0.5353\n",
      "Epoch 399/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1803 - accuracy: 0.5951 - val_loss: 0.3681 - val_accuracy: 0.5412\n",
      "Epoch 400/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.2002 - accuracy: 0.5889 - val_loss: 0.4097 - val_accuracy: 0.5500\n",
      "Epoch 401/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1761 - accuracy: 0.5951 - val_loss: 0.3709 - val_accuracy: 0.5529\n",
      "Epoch 402/1600\n",
      "680/680 [==============================] - 1s 898us/step - loss: 0.1883 - accuracy: 0.5926 - val_loss: 0.3670 - val_accuracy: 0.5529\n",
      "Epoch 403/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1828 - accuracy: 0.5940 - val_loss: 0.3746 - val_accuracy: 0.5412\n",
      "Epoch 404/1600\n",
      "680/680 [==============================] - 1s 939us/step - loss: 0.1834 - accuracy: 0.5929 - val_loss: 0.4290 - val_accuracy: 0.5382\n",
      "Epoch 405/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1829 - accuracy: 0.5929 - val_loss: 0.3935 - val_accuracy: 0.5412\n",
      "Epoch 406/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1710 - accuracy: 0.5959 - val_loss: 0.3801 - val_accuracy: 0.5559\n",
      "Epoch 407/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1894 - accuracy: 0.5922 - val_loss: 0.3800 - val_accuracy: 0.5500\n",
      "Epoch 408/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1819 - accuracy: 0.5966 - val_loss: 0.3884 - val_accuracy: 0.5529\n",
      "Epoch 409/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1891 - accuracy: 0.5940 - val_loss: 0.4000 - val_accuracy: 0.5471\n",
      "Epoch 410/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1880 - accuracy: 0.5915 - val_loss: 0.3669 - val_accuracy: 0.5382\n",
      "Epoch 411/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1855 - accuracy: 0.5929 - val_loss: 0.3882 - val_accuracy: 0.5412\n",
      "Epoch 412/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1862 - accuracy: 0.5918 - val_loss: 0.3861 - val_accuracy: 0.5529\n",
      "Epoch 413/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1919 - accuracy: 0.5904 - val_loss: 0.3956 - val_accuracy: 0.5559\n",
      "Epoch 414/1600\n",
      "680/680 [==============================] - 1s 967us/step - loss: 0.1760 - accuracy: 0.5951 - val_loss: 0.3764 - val_accuracy: 0.5441\n",
      "Epoch 415/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1807 - accuracy: 0.5937 - val_loss: 0.3618 - val_accuracy: 0.5559\n",
      "Epoch 416/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1792 - accuracy: 0.5948 - val_loss: 0.3967 - val_accuracy: 0.5529\n",
      "Epoch 417/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1787 - accuracy: 0.5970 - val_loss: 0.4128 - val_accuracy: 0.5471\n",
      "Epoch 418/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.2033 - accuracy: 0.5889 - val_loss: 0.4257 - val_accuracy: 0.5676\n",
      "Epoch 419/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1785 - accuracy: 0.5959 - val_loss: 0.4014 - val_accuracy: 0.5353\n",
      "Epoch 420/1600\n",
      "680/680 [==============================] - 1s 904us/step - loss: 0.1931 - accuracy: 0.5933 - val_loss: 0.3962 - val_accuracy: 0.5441\n",
      "Epoch 421/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1797 - accuracy: 0.5955 - val_loss: 0.3784 - val_accuracy: 0.5559\n",
      "Epoch 422/1600\n",
      "680/680 [==============================] - 1s 898us/step - loss: 0.1887 - accuracy: 0.5926 - val_loss: 0.4151 - val_accuracy: 0.5529\n",
      "Epoch 423/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1867 - accuracy: 0.5937 - val_loss: 0.4388 - val_accuracy: 0.5324\n",
      "Epoch 424/1600\n",
      "680/680 [==============================] - 1s 945us/step - loss: 0.1905 - accuracy: 0.5900 - val_loss: 0.3532 - val_accuracy: 0.5588\n",
      "Epoch 425/1600\n",
      "680/680 [==============================] - 1s 935us/step - loss: 0.1761 - accuracy: 0.5966 - val_loss: 0.3890 - val_accuracy: 0.5529\n",
      "Epoch 426/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1781 - accuracy: 0.5959 - val_loss: 0.3892 - val_accuracy: 0.5500\n",
      "Epoch 427/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1768 - accuracy: 0.5948 - val_loss: 0.3909 - val_accuracy: 0.5529\n",
      "Epoch 428/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1842 - accuracy: 0.5926 - val_loss: 0.3990 - val_accuracy: 0.5441\n",
      "Epoch 429/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.1819 - accuracy: 0.5933 - val_loss: 0.4123 - val_accuracy: 0.5382\n",
      "Epoch 430/1600\n",
      "680/680 [==============================] - 1s 935us/step - loss: 0.1849 - accuracy: 0.5962 - val_loss: 0.3624 - val_accuracy: 0.5441\n",
      "Epoch 431/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1823 - accuracy: 0.5940 - val_loss: 0.3992 - val_accuracy: 0.5412\n",
      "Epoch 432/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1821 - accuracy: 0.5922 - val_loss: 0.4020 - val_accuracy: 0.5559\n",
      "Epoch 433/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1930 - accuracy: 0.5896 - val_loss: 0.3553 - val_accuracy: 0.5559\n",
      "Epoch 434/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1788 - accuracy: 0.5944 - val_loss: 0.4541 - val_accuracy: 0.5265\n",
      "Epoch 435/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.1991 - accuracy: 0.5889 - val_loss: 0.4156 - val_accuracy: 0.5647\n",
      "Epoch 436/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1806 - accuracy: 0.5944 - val_loss: 0.3862 - val_accuracy: 0.5441\n",
      "Epoch 437/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1820 - accuracy: 0.5951 - val_loss: 0.4483 - val_accuracy: 0.5353\n",
      "Epoch 438/1600\n",
      "680/680 [==============================] - 1s 903us/step - loss: 0.1875 - accuracy: 0.5926 - val_loss: 0.3772 - val_accuracy: 0.5441\n",
      "Epoch 439/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1984 - accuracy: 0.5911 - val_loss: 0.3910 - val_accuracy: 0.5559\n",
      "Epoch 440/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1957 - accuracy: 0.5918 - val_loss: 0.4455 - val_accuracy: 0.5353\n",
      "Epoch 441/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1905 - accuracy: 0.5889 - val_loss: 0.3897 - val_accuracy: 0.5529\n",
      "Epoch 442/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1794 - accuracy: 0.5966 - val_loss: 0.4210 - val_accuracy: 0.5353\n",
      "Epoch 443/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 864us/step - loss: 0.1982 - accuracy: 0.5948 - val_loss: 0.3804 - val_accuracy: 0.5529\n",
      "Epoch 444/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.1785 - accuracy: 0.5962 - val_loss: 0.3828 - val_accuracy: 0.5559\n",
      "Epoch 445/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1900 - accuracy: 0.5940 - val_loss: 0.3946 - val_accuracy: 0.5382\n",
      "Epoch 446/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1851 - accuracy: 0.5940 - val_loss: 0.3982 - val_accuracy: 0.5441\n",
      "Epoch 447/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1775 - accuracy: 0.5948 - val_loss: 0.4004 - val_accuracy: 0.5471\n",
      "Epoch 448/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1812 - accuracy: 0.5966 - val_loss: 0.3846 - val_accuracy: 0.5529\n",
      "Epoch 449/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1873 - accuracy: 0.5937 - val_loss: 0.4189 - val_accuracy: 0.5265\n",
      "Epoch 450/1600\n",
      "680/680 [==============================] - 1s 902us/step - loss: 0.1856 - accuracy: 0.5922 - val_loss: 0.4652 - val_accuracy: 0.5765\n",
      "Epoch 451/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1876 - accuracy: 0.5948 - val_loss: 0.3856 - val_accuracy: 0.5529\n",
      "Epoch 452/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1879 - accuracy: 0.5929 - val_loss: 0.3812 - val_accuracy: 0.5500\n",
      "Epoch 453/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1877 - accuracy: 0.5904 - val_loss: 0.4097 - val_accuracy: 0.5500\n",
      "Epoch 454/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1756 - accuracy: 0.5977 - val_loss: 0.3840 - val_accuracy: 0.5471\n",
      "Epoch 455/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1885 - accuracy: 0.5922 - val_loss: 0.3922 - val_accuracy: 0.5471\n",
      "Epoch 456/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1901 - accuracy: 0.5918 - val_loss: 0.4038 - val_accuracy: 0.5500\n",
      "Epoch 457/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.1858 - accuracy: 0.5907 - val_loss: 0.3665 - val_accuracy: 0.5618\n",
      "Epoch 458/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1875 - accuracy: 0.5944 - val_loss: 0.3616 - val_accuracy: 0.5500\n",
      "Epoch 459/1600\n",
      "680/680 [==============================] - 1s 941us/step - loss: 0.1864 - accuracy: 0.5929 - val_loss: 0.3967 - val_accuracy: 0.5412\n",
      "Epoch 460/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1921 - accuracy: 0.5911 - val_loss: 0.4016 - val_accuracy: 0.5618\n",
      "Epoch 461/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1681 - accuracy: 0.5992 - val_loss: 0.4216 - val_accuracy: 0.5412\n",
      "Epoch 462/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1871 - accuracy: 0.5929 - val_loss: 0.3939 - val_accuracy: 0.5618\n",
      "Epoch 463/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1736 - accuracy: 0.5977 - val_loss: 0.3916 - val_accuracy: 0.5412\n",
      "Epoch 464/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1982 - accuracy: 0.5904 - val_loss: 0.3855 - val_accuracy: 0.5500\n",
      "Epoch 465/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1813 - accuracy: 0.5948 - val_loss: 0.3803 - val_accuracy: 0.5471\n",
      "Epoch 466/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1777 - accuracy: 0.5962 - val_loss: 0.3703 - val_accuracy: 0.5559\n",
      "Epoch 467/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1740 - accuracy: 0.5959 - val_loss: 0.3815 - val_accuracy: 0.5529\n",
      "Epoch 468/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1776 - accuracy: 0.5962 - val_loss: 0.3889 - val_accuracy: 0.5500\n",
      "Epoch 469/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1803 - accuracy: 0.5944 - val_loss: 0.3812 - val_accuracy: 0.5500\n",
      "Epoch 470/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1899 - accuracy: 0.5929 - val_loss: 0.3478 - val_accuracy: 0.5618\n",
      "Epoch 471/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1886 - accuracy: 0.5922 - val_loss: 0.4293 - val_accuracy: 0.5235\n",
      "Epoch 472/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1766 - accuracy: 0.5944 - val_loss: 0.3974 - val_accuracy: 0.5471\n",
      "Epoch 473/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.1833 - accuracy: 0.5944 - val_loss: 0.4609 - val_accuracy: 0.5294\n",
      "Epoch 474/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.1935 - accuracy: 0.5904 - val_loss: 0.4028 - val_accuracy: 0.5647\n",
      "Epoch 475/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1918 - accuracy: 0.5933 - val_loss: 0.3839 - val_accuracy: 0.5559\n",
      "Epoch 476/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1837 - accuracy: 0.5962 - val_loss: 0.4187 - val_accuracy: 0.5265\n",
      "Epoch 477/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1714 - accuracy: 0.5985 - val_loss: 0.4251 - val_accuracy: 0.5559\n",
      "Epoch 478/1600\n",
      "680/680 [==============================] - 1s 898us/step - loss: 0.1874 - accuracy: 0.5944 - val_loss: 0.4018 - val_accuracy: 0.5382\n",
      "Epoch 479/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.1857 - accuracy: 0.5926 - val_loss: 0.4051 - val_accuracy: 0.5471\n",
      "Epoch 480/1600\n",
      "680/680 [==============================] - 1s 906us/step - loss: 0.1911 - accuracy: 0.5911 - val_loss: 0.3713 - val_accuracy: 0.5529\n",
      "Epoch 481/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1910 - accuracy: 0.5933 - val_loss: 0.3900 - val_accuracy: 0.5500\n",
      "Epoch 482/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1815 - accuracy: 0.5948 - val_loss: 0.4020 - val_accuracy: 0.5353\n",
      "Epoch 483/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1724 - accuracy: 0.5970 - val_loss: 0.4114 - val_accuracy: 0.5382\n",
      "Epoch 484/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1830 - accuracy: 0.5937 - val_loss: 0.3673 - val_accuracy: 0.5529\n",
      "Epoch 485/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1951 - accuracy: 0.5937 - val_loss: 0.4253 - val_accuracy: 0.5382\n",
      "Epoch 486/1600\n",
      "680/680 [==============================] - 1s 953us/step - loss: 0.1766 - accuracy: 0.5962 - val_loss: 0.3756 - val_accuracy: 0.5441\n",
      "Epoch 487/1600\n",
      "680/680 [==============================] - 1s 939us/step - loss: 0.1874 - accuracy: 0.5937 - val_loss: 0.4081 - val_accuracy: 0.5471\n",
      "Epoch 488/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.1800 - accuracy: 0.5944 - val_loss: 0.4180 - val_accuracy: 0.5412\n",
      "Epoch 489/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1794 - accuracy: 0.5940 - val_loss: 0.4030 - val_accuracy: 0.5441\n",
      "Epoch 490/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1754 - accuracy: 0.5970 - val_loss: 0.4066 - val_accuracy: 0.5471\n",
      "Epoch 491/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1899 - accuracy: 0.5955 - val_loss: 0.4233 - val_accuracy: 0.5353\n",
      "Epoch 492/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1825 - accuracy: 0.5922 - val_loss: 0.3792 - val_accuracy: 0.5588\n",
      "Epoch 493/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1832 - accuracy: 0.5951 - val_loss: 0.3860 - val_accuracy: 0.5500\n",
      "Epoch 494/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1880 - accuracy: 0.5926 - val_loss: 0.4532 - val_accuracy: 0.5294\n",
      "Epoch 495/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1819 - accuracy: 0.5933 - val_loss: 0.4056 - val_accuracy: 0.5441\n",
      "Epoch 496/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1892 - accuracy: 0.5951 - val_loss: 0.4318 - val_accuracy: 0.5412\n",
      "Epoch 497/1600\n",
      "680/680 [==============================] - 1s 901us/step - loss: 0.1808 - accuracy: 0.5962 - val_loss: 0.3894 - val_accuracy: 0.5676\n",
      "Epoch 498/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 883us/step - loss: 0.1893 - accuracy: 0.5922 - val_loss: 0.4154 - val_accuracy: 0.5412\n",
      "Epoch 499/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1869 - accuracy: 0.5944 - val_loss: 0.3981 - val_accuracy: 0.5706\n",
      "Epoch 500/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.1930 - accuracy: 0.5940 - val_loss: 0.4119 - val_accuracy: 0.5265\n",
      "Epoch 501/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1789 - accuracy: 0.5948 - val_loss: 0.4227 - val_accuracy: 0.5412\n",
      "Epoch 502/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1846 - accuracy: 0.5940 - val_loss: 0.3809 - val_accuracy: 0.5588\n",
      "Epoch 503/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1804 - accuracy: 0.5940 - val_loss: 0.4253 - val_accuracy: 0.5412\n",
      "Epoch 504/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1796 - accuracy: 0.5944 - val_loss: 0.3801 - val_accuracy: 0.5618\n",
      "Epoch 505/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1891 - accuracy: 0.5915 - val_loss: 0.4317 - val_accuracy: 0.5471\n",
      "Epoch 506/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1927 - accuracy: 0.5922 - val_loss: 0.3984 - val_accuracy: 0.5618\n",
      "Epoch 507/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1838 - accuracy: 0.5922 - val_loss: 0.3895 - val_accuracy: 0.5500\n",
      "Epoch 508/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1850 - accuracy: 0.5926 - val_loss: 0.4122 - val_accuracy: 0.5441\n",
      "Epoch 509/1600\n",
      "680/680 [==============================] - 1s 936us/step - loss: 0.1875 - accuracy: 0.5926 - val_loss: 0.4051 - val_accuracy: 0.5382\n",
      "Epoch 510/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1788 - accuracy: 0.5966 - val_loss: 0.4038 - val_accuracy: 0.5559\n",
      "Epoch 511/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1675 - accuracy: 0.5981 - val_loss: 0.4000 - val_accuracy: 0.5412\n",
      "Epoch 512/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1839 - accuracy: 0.5944 - val_loss: 0.3940 - val_accuracy: 0.5588\n",
      "Epoch 513/1600\n",
      "680/680 [==============================] - 1s 943us/step - loss: 0.1803 - accuracy: 0.5937 - val_loss: 0.4071 - val_accuracy: 0.5441\n",
      "Epoch 514/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1929 - accuracy: 0.5922 - val_loss: 0.3973 - val_accuracy: 0.5647\n",
      "Epoch 515/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1754 - accuracy: 0.5951 - val_loss: 0.3987 - val_accuracy: 0.5441\n",
      "Epoch 516/1600\n",
      "680/680 [==============================] - 1s 960us/step - loss: 0.1689 - accuracy: 0.5974 - val_loss: 0.4350 - val_accuracy: 0.5441\n",
      "Epoch 517/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1839 - accuracy: 0.5959 - val_loss: 0.4018 - val_accuracy: 0.5676\n",
      "Epoch 518/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1901 - accuracy: 0.5915 - val_loss: 0.4015 - val_accuracy: 0.5618\n",
      "Epoch 519/1600\n",
      "680/680 [==============================] - 1s 903us/step - loss: 0.1915 - accuracy: 0.5933 - val_loss: 0.4135 - val_accuracy: 0.5324\n",
      "Epoch 520/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1872 - accuracy: 0.5948 - val_loss: 0.3756 - val_accuracy: 0.5588\n",
      "Epoch 521/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1842 - accuracy: 0.5955 - val_loss: 0.3829 - val_accuracy: 0.5529\n",
      "Epoch 522/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1721 - accuracy: 0.5977 - val_loss: 0.4065 - val_accuracy: 0.5471\n",
      "Epoch 523/1600\n",
      "680/680 [==============================] - 1s 930us/step - loss: 0.1728 - accuracy: 0.5962 - val_loss: 0.4083 - val_accuracy: 0.5588\n",
      "Epoch 524/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1869 - accuracy: 0.5926 - val_loss: 0.4097 - val_accuracy: 0.5471\n",
      "Epoch 525/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1858 - accuracy: 0.5955 - val_loss: 0.3977 - val_accuracy: 0.5500\n",
      "Epoch 526/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1785 - accuracy: 0.5940 - val_loss: 0.4014 - val_accuracy: 0.5471\n",
      "Epoch 527/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1733 - accuracy: 0.5977 - val_loss: 0.4128 - val_accuracy: 0.5441\n",
      "Epoch 528/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1717 - accuracy: 0.5981 - val_loss: 0.4581 - val_accuracy: 0.5294\n",
      "Epoch 529/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1992 - accuracy: 0.5937 - val_loss: 0.4043 - val_accuracy: 0.5559\n",
      "Epoch 530/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1937 - accuracy: 0.5907 - val_loss: 0.4161 - val_accuracy: 0.5324\n",
      "Epoch 531/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1725 - accuracy: 0.5974 - val_loss: 0.4183 - val_accuracy: 0.5529\n",
      "Epoch 532/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1798 - accuracy: 0.5959 - val_loss: 0.4016 - val_accuracy: 0.5529\n",
      "Epoch 533/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1770 - accuracy: 0.5940 - val_loss: 0.4268 - val_accuracy: 0.5382\n",
      "Epoch 534/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1928 - accuracy: 0.5918 - val_loss: 0.4386 - val_accuracy: 0.5353\n",
      "Epoch 535/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1979 - accuracy: 0.5900 - val_loss: 0.3949 - val_accuracy: 0.5529\n",
      "Epoch 536/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1889 - accuracy: 0.5926 - val_loss: 0.4072 - val_accuracy: 0.5500\n",
      "Epoch 537/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1703 - accuracy: 0.5955 - val_loss: 0.3662 - val_accuracy: 0.5529\n",
      "Epoch 538/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1865 - accuracy: 0.5915 - val_loss: 0.4131 - val_accuracy: 0.5471\n",
      "Epoch 539/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1764 - accuracy: 0.5951 - val_loss: 0.3963 - val_accuracy: 0.5588\n",
      "Epoch 540/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1919 - accuracy: 0.5940 - val_loss: 0.3690 - val_accuracy: 0.5382\n",
      "Epoch 541/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1846 - accuracy: 0.5926 - val_loss: 0.3775 - val_accuracy: 0.5676\n",
      "Epoch 542/1600\n",
      "680/680 [==============================] - 1s 864us/step - loss: 0.1877 - accuracy: 0.5933 - val_loss: 0.4201 - val_accuracy: 0.5441\n",
      "Epoch 543/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1711 - accuracy: 0.5948 - val_loss: 0.3740 - val_accuracy: 0.5500\n",
      "Epoch 544/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1827 - accuracy: 0.5966 - val_loss: 0.4228 - val_accuracy: 0.5382\n",
      "Epoch 545/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1933 - accuracy: 0.5948 - val_loss: 0.3842 - val_accuracy: 0.5441\n",
      "Epoch 546/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1698 - accuracy: 0.5974 - val_loss: 0.3838 - val_accuracy: 0.5500\n",
      "Epoch 547/1600\n",
      "680/680 [==============================] - 1s 866us/step - loss: 0.1757 - accuracy: 0.5937 - val_loss: 0.4147 - val_accuracy: 0.5441\n",
      "Epoch 548/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1794 - accuracy: 0.5929 - val_loss: 0.4076 - val_accuracy: 0.5471\n",
      "Epoch 549/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1945 - accuracy: 0.5933 - val_loss: 0.4207 - val_accuracy: 0.5412\n",
      "Epoch 550/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1825 - accuracy: 0.5948 - val_loss: 0.3794 - val_accuracy: 0.5382\n",
      "Epoch 551/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1782 - accuracy: 0.5951 - val_loss: 0.4110 - val_accuracy: 0.5412\n",
      "Epoch 552/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1798 - accuracy: 0.5951 - val_loss: 0.4119 - val_accuracy: 0.5382\n",
      "Epoch 553/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 925us/step - loss: 0.1773 - accuracy: 0.5966 - val_loss: 0.3940 - val_accuracy: 0.5441\n",
      "Epoch 554/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1708 - accuracy: 0.5985 - val_loss: 0.3792 - val_accuracy: 0.5500\n",
      "Epoch 555/1600\n",
      "680/680 [==============================] - 1s 904us/step - loss: 0.1842 - accuracy: 0.5959 - val_loss: 0.4092 - val_accuracy: 0.5412\n",
      "Epoch 556/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1840 - accuracy: 0.5929 - val_loss: 0.3899 - val_accuracy: 0.5559\n",
      "Epoch 557/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1823 - accuracy: 0.5922 - val_loss: 0.4387 - val_accuracy: 0.5471\n",
      "Epoch 558/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1762 - accuracy: 0.5959 - val_loss: 0.4443 - val_accuracy: 0.5441\n",
      "Epoch 559/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1873 - accuracy: 0.5937 - val_loss: 0.3941 - val_accuracy: 0.5412\n",
      "Epoch 560/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1878 - accuracy: 0.5922 - val_loss: 0.4205 - val_accuracy: 0.5529\n",
      "Epoch 561/1600\n",
      "680/680 [==============================] - 1s 939us/step - loss: 0.1931 - accuracy: 0.5915 - val_loss: 0.3642 - val_accuracy: 0.5529\n",
      "Epoch 562/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1735 - accuracy: 0.5962 - val_loss: 0.3766 - val_accuracy: 0.5647\n",
      "Epoch 563/1600\n",
      "680/680 [==============================] - 1s 934us/step - loss: 0.1774 - accuracy: 0.5948 - val_loss: 0.4131 - val_accuracy: 0.5559\n",
      "Epoch 564/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1858 - accuracy: 0.5933 - val_loss: 0.3639 - val_accuracy: 0.5706\n",
      "Epoch 565/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1868 - accuracy: 0.5948 - val_loss: 0.4167 - val_accuracy: 0.5588\n",
      "Epoch 566/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.1769 - accuracy: 0.5951 - val_loss: 0.4115 - val_accuracy: 0.5559\n",
      "Epoch 567/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1728 - accuracy: 0.5944 - val_loss: 0.4321 - val_accuracy: 0.5441\n",
      "Epoch 568/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1875 - accuracy: 0.5951 - val_loss: 0.4263 - val_accuracy: 0.5441\n",
      "Epoch 569/1600\n",
      "680/680 [==============================] - 1s 901us/step - loss: 0.1850 - accuracy: 0.5918 - val_loss: 0.4275 - val_accuracy: 0.5382\n",
      "Epoch 570/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1951 - accuracy: 0.5918 - val_loss: 0.4042 - val_accuracy: 0.5529\n",
      "Epoch 571/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1903 - accuracy: 0.5926 - val_loss: 0.3981 - val_accuracy: 0.5500\n",
      "Epoch 572/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1697 - accuracy: 0.5970 - val_loss: 0.4144 - val_accuracy: 0.5500\n",
      "Epoch 573/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1898 - accuracy: 0.5948 - val_loss: 0.3863 - val_accuracy: 0.5559\n",
      "Epoch 574/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1783 - accuracy: 0.5951 - val_loss: 0.4272 - val_accuracy: 0.5412\n",
      "Epoch 575/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1846 - accuracy: 0.5951 - val_loss: 0.3876 - val_accuracy: 0.5500\n",
      "Epoch 576/1600\n",
      "680/680 [==============================] - 1s 934us/step - loss: 0.1899 - accuracy: 0.5951 - val_loss: 0.3900 - val_accuracy: 0.5471\n",
      "Epoch 577/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1875 - accuracy: 0.5948 - val_loss: 0.3775 - val_accuracy: 0.5559\n",
      "Epoch 578/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1795 - accuracy: 0.5959 - val_loss: 0.4195 - val_accuracy: 0.5353\n",
      "Epoch 579/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1851 - accuracy: 0.5922 - val_loss: 0.4001 - val_accuracy: 0.5500\n",
      "Epoch 580/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1778 - accuracy: 0.5977 - val_loss: 0.4302 - val_accuracy: 0.5441\n",
      "Epoch 581/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1750 - accuracy: 0.5966 - val_loss: 0.3827 - val_accuracy: 0.5500\n",
      "Epoch 582/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1857 - accuracy: 0.5966 - val_loss: 0.4267 - val_accuracy: 0.5647\n",
      "Epoch 583/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1807 - accuracy: 0.5948 - val_loss: 0.4130 - val_accuracy: 0.5441\n",
      "Epoch 584/1600\n",
      "680/680 [==============================] - 1s 863us/step - loss: 0.1848 - accuracy: 0.5940 - val_loss: 0.4368 - val_accuracy: 0.5559\n",
      "Epoch 585/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1957 - accuracy: 0.5896 - val_loss: 0.4076 - val_accuracy: 0.5500\n",
      "Epoch 586/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1817 - accuracy: 0.5962 - val_loss: 0.4061 - val_accuracy: 0.5529\n",
      "Epoch 587/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1818 - accuracy: 0.5951 - val_loss: 0.4158 - val_accuracy: 0.5500\n",
      "Epoch 588/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1754 - accuracy: 0.5959 - val_loss: 0.4394 - val_accuracy: 0.5324\n",
      "Epoch 589/1600\n",
      "680/680 [==============================] - 1s 938us/step - loss: 0.1879 - accuracy: 0.5933 - val_loss: 0.4083 - val_accuracy: 0.5412\n",
      "Epoch 590/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1835 - accuracy: 0.5940 - val_loss: 0.4266 - val_accuracy: 0.5529\n",
      "Epoch 591/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1735 - accuracy: 0.5992 - val_loss: 0.3986 - val_accuracy: 0.5500\n",
      "Epoch 592/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1738 - accuracy: 0.5988 - val_loss: 0.4235 - val_accuracy: 0.5265\n",
      "Epoch 593/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1901 - accuracy: 0.5940 - val_loss: 0.3877 - val_accuracy: 0.5618\n",
      "Epoch 594/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1880 - accuracy: 0.5926 - val_loss: 0.3964 - val_accuracy: 0.5559\n",
      "Epoch 595/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1943 - accuracy: 0.5929 - val_loss: 0.3835 - val_accuracy: 0.5441\n",
      "Epoch 596/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1880 - accuracy: 0.5933 - val_loss: 0.3812 - val_accuracy: 0.5618\n",
      "Epoch 597/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1672 - accuracy: 0.5970 - val_loss: 0.4050 - val_accuracy: 0.5529\n",
      "Epoch 598/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1892 - accuracy: 0.5922 - val_loss: 0.3850 - val_accuracy: 0.5412\n",
      "Epoch 599/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1928 - accuracy: 0.5937 - val_loss: 0.3840 - val_accuracy: 0.5559\n",
      "Epoch 600/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1726 - accuracy: 0.5985 - val_loss: 0.4025 - val_accuracy: 0.5471\n",
      "Epoch 601/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1748 - accuracy: 0.5977 - val_loss: 0.3826 - val_accuracy: 0.5529\n",
      "Epoch 602/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1826 - accuracy: 0.5966 - val_loss: 0.3761 - val_accuracy: 0.5559\n",
      "Epoch 603/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1721 - accuracy: 0.5970 - val_loss: 0.3955 - val_accuracy: 0.5735\n",
      "Epoch 604/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.2006 - accuracy: 0.5933 - val_loss: 0.3470 - val_accuracy: 0.5618\n",
      "Epoch 605/1600\n",
      "680/680 [==============================] - 1s 966us/step - loss: 0.1926 - accuracy: 0.5918 - val_loss: 0.4202 - val_accuracy: 0.5382\n",
      "Epoch 606/1600\n",
      "680/680 [==============================] - 1s 862us/step - loss: 0.1792 - accuracy: 0.5940 - val_loss: 0.4043 - val_accuracy: 0.5441\n",
      "Epoch 607/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1808 - accuracy: 0.5970 - val_loss: 0.3398 - val_accuracy: 0.5706\n",
      "Epoch 608/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 925us/step - loss: 0.1910 - accuracy: 0.5926 - val_loss: 0.3744 - val_accuracy: 0.5618\n",
      "Epoch 609/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1832 - accuracy: 0.5951 - val_loss: 0.4409 - val_accuracy: 0.5294\n",
      "Epoch 610/1600\n",
      "680/680 [==============================] - 1s 906us/step - loss: 0.1844 - accuracy: 0.5959 - val_loss: 0.3943 - val_accuracy: 0.5529\n",
      "Epoch 611/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1786 - accuracy: 0.5951 - val_loss: 0.3731 - val_accuracy: 0.5529\n",
      "Epoch 612/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1810 - accuracy: 0.5966 - val_loss: 0.4133 - val_accuracy: 0.5382\n",
      "Epoch 613/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1778 - accuracy: 0.5951 - val_loss: 0.3829 - val_accuracy: 0.5588\n",
      "Epoch 614/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1705 - accuracy: 0.5988 - val_loss: 0.4150 - val_accuracy: 0.5441\n",
      "Epoch 615/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1848 - accuracy: 0.5937 - val_loss: 0.4289 - val_accuracy: 0.5471\n",
      "Epoch 616/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1713 - accuracy: 0.5977 - val_loss: 0.3868 - val_accuracy: 0.5618\n",
      "Epoch 617/1600\n",
      "680/680 [==============================] - 1s 866us/step - loss: 0.2057 - accuracy: 0.5911 - val_loss: 0.4208 - val_accuracy: 0.5618\n",
      "Epoch 618/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1769 - accuracy: 0.5962 - val_loss: 0.4279 - val_accuracy: 0.5412\n",
      "Epoch 619/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1802 - accuracy: 0.5948 - val_loss: 0.4145 - val_accuracy: 0.5588\n",
      "Epoch 620/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1889 - accuracy: 0.5940 - val_loss: 0.3969 - val_accuracy: 0.5529\n",
      "Epoch 621/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1781 - accuracy: 0.5944 - val_loss: 0.4486 - val_accuracy: 0.5353\n",
      "Epoch 622/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1838 - accuracy: 0.5922 - val_loss: 0.4366 - val_accuracy: 0.5588\n",
      "Epoch 623/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1952 - accuracy: 0.5918 - val_loss: 0.3944 - val_accuracy: 0.5471\n",
      "Epoch 624/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1802 - accuracy: 0.5955 - val_loss: 0.4481 - val_accuracy: 0.5412\n",
      "Epoch 625/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1705 - accuracy: 0.5955 - val_loss: 0.3938 - val_accuracy: 0.5559\n",
      "Epoch 626/1600\n",
      "680/680 [==============================] - 1s 936us/step - loss: 0.1780 - accuracy: 0.5940 - val_loss: 0.3975 - val_accuracy: 0.5500\n",
      "Epoch 627/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1817 - accuracy: 0.5966 - val_loss: 0.4194 - val_accuracy: 0.5441\n",
      "Epoch 628/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1936 - accuracy: 0.5900 - val_loss: 0.4227 - val_accuracy: 0.5588\n",
      "Epoch 629/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1797 - accuracy: 0.5926 - val_loss: 0.4209 - val_accuracy: 0.5441\n",
      "Epoch 630/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1818 - accuracy: 0.5955 - val_loss: 0.4382 - val_accuracy: 0.5382\n",
      "Epoch 631/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1958 - accuracy: 0.5918 - val_loss: 0.4478 - val_accuracy: 0.5382\n",
      "Epoch 632/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1843 - accuracy: 0.5948 - val_loss: 0.3896 - val_accuracy: 0.5588\n",
      "Epoch 633/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1770 - accuracy: 0.5959 - val_loss: 0.4123 - val_accuracy: 0.5559\n",
      "Epoch 634/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1763 - accuracy: 0.5966 - val_loss: 0.4045 - val_accuracy: 0.5412\n",
      "Epoch 635/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1745 - accuracy: 0.5970 - val_loss: 0.3918 - val_accuracy: 0.5441\n",
      "Epoch 636/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1865 - accuracy: 0.5933 - val_loss: 0.4277 - val_accuracy: 0.5441\n",
      "Epoch 637/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1854 - accuracy: 0.5929 - val_loss: 0.3868 - val_accuracy: 0.5353\n",
      "Epoch 638/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1807 - accuracy: 0.5922 - val_loss: 0.4147 - val_accuracy: 0.5471\n",
      "Epoch 639/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1905 - accuracy: 0.5933 - val_loss: 0.4121 - val_accuracy: 0.5441\n",
      "Epoch 640/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1738 - accuracy: 0.5970 - val_loss: 0.4197 - val_accuracy: 0.5559\n",
      "Epoch 641/1600\n",
      "680/680 [==============================] - 1s 898us/step - loss: 0.1859 - accuracy: 0.5944 - val_loss: 0.4146 - val_accuracy: 0.5471\n",
      "Epoch 642/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1841 - accuracy: 0.5922 - val_loss: 0.4140 - val_accuracy: 0.5500\n",
      "Epoch 643/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1733 - accuracy: 0.5955 - val_loss: 0.4404 - val_accuracy: 0.5382\n",
      "Epoch 644/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1745 - accuracy: 0.5948 - val_loss: 0.4091 - val_accuracy: 0.5588\n",
      "Epoch 645/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1839 - accuracy: 0.5955 - val_loss: 0.4308 - val_accuracy: 0.5441\n",
      "Epoch 646/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1769 - accuracy: 0.5955 - val_loss: 0.4061 - val_accuracy: 0.5500\n",
      "Epoch 647/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.1733 - accuracy: 0.5955 - val_loss: 0.4272 - val_accuracy: 0.5471\n",
      "Epoch 648/1600\n",
      "680/680 [==============================] - 1s 963us/step - loss: 0.1992 - accuracy: 0.5907 - val_loss: 0.4085 - val_accuracy: 0.5471\n",
      "Epoch 649/1600\n",
      "680/680 [==============================] - 1s 972us/step - loss: 0.1802 - accuracy: 0.5959 - val_loss: 0.4239 - val_accuracy: 0.5412\n",
      "Epoch 650/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1891 - accuracy: 0.5911 - val_loss: 0.4049 - val_accuracy: 0.5500\n",
      "Epoch 651/1600\n",
      "680/680 [==============================] - 1s 934us/step - loss: 0.1889 - accuracy: 0.5933 - val_loss: 0.4122 - val_accuracy: 0.5441\n",
      "Epoch 652/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1714 - accuracy: 0.5981 - val_loss: 0.4435 - val_accuracy: 0.5324\n",
      "Epoch 653/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1699 - accuracy: 0.5966 - val_loss: 0.3841 - val_accuracy: 0.5559\n",
      "Epoch 654/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1865 - accuracy: 0.5944 - val_loss: 0.3983 - val_accuracy: 0.5471\n",
      "Epoch 655/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1896 - accuracy: 0.5907 - val_loss: 0.4071 - val_accuracy: 0.5412\n",
      "Epoch 656/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1771 - accuracy: 0.5948 - val_loss: 0.4233 - val_accuracy: 0.5382\n",
      "Epoch 657/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1767 - accuracy: 0.5944 - val_loss: 0.4104 - val_accuracy: 0.5529\n",
      "Epoch 658/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1773 - accuracy: 0.5955 - val_loss: 0.4374 - val_accuracy: 0.5441\n",
      "Epoch 659/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1807 - accuracy: 0.5962 - val_loss: 0.3711 - val_accuracy: 0.5471\n",
      "Epoch 660/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1848 - accuracy: 0.5933 - val_loss: 0.4154 - val_accuracy: 0.5353\n",
      "Epoch 661/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1945 - accuracy: 0.5904 - val_loss: 0.4096 - val_accuracy: 0.5500\n",
      "Epoch 662/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1747 - accuracy: 0.5948 - val_loss: 0.4273 - val_accuracy: 0.5471\n",
      "Epoch 663/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 893us/step - loss: 0.1801 - accuracy: 0.5981 - val_loss: 0.4129 - val_accuracy: 0.5382\n",
      "Epoch 664/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1958 - accuracy: 0.5900 - val_loss: 0.4253 - val_accuracy: 0.5353\n",
      "Epoch 665/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1740 - accuracy: 0.5966 - val_loss: 0.4477 - val_accuracy: 0.5353\n",
      "Epoch 666/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.1941 - accuracy: 0.5922 - val_loss: 0.4137 - val_accuracy: 0.5441\n",
      "Epoch 667/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1821 - accuracy: 0.5940 - val_loss: 0.3914 - val_accuracy: 0.5559\n",
      "Epoch 668/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1787 - accuracy: 0.5966 - val_loss: 0.4045 - val_accuracy: 0.5559\n",
      "Epoch 669/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1808 - accuracy: 0.5940 - val_loss: 0.4017 - val_accuracy: 0.5618\n",
      "Epoch 670/1600\n",
      "680/680 [==============================] - 1s 964us/step - loss: 0.1852 - accuracy: 0.5944 - val_loss: 0.4082 - val_accuracy: 0.5471\n",
      "Epoch 671/1600\n",
      "680/680 [==============================] - 1s 864us/step - loss: 0.1784 - accuracy: 0.5962 - val_loss: 0.4381 - val_accuracy: 0.5441\n",
      "Epoch 672/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1961 - accuracy: 0.5911 - val_loss: 0.4136 - val_accuracy: 0.5324\n",
      "Epoch 673/1600\n",
      "680/680 [==============================] - 1s 947us/step - loss: 0.1699 - accuracy: 0.5974 - val_loss: 0.4370 - val_accuracy: 0.5412\n",
      "Epoch 674/1600\n",
      "680/680 [==============================] - 1s 906us/step - loss: 0.1802 - accuracy: 0.5955 - val_loss: 0.4063 - val_accuracy: 0.5588\n",
      "Epoch 675/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1780 - accuracy: 0.5940 - val_loss: 0.4095 - val_accuracy: 0.5353\n",
      "Epoch 676/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1888 - accuracy: 0.5944 - val_loss: 0.4017 - val_accuracy: 0.5441\n",
      "Epoch 677/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1776 - accuracy: 0.5944 - val_loss: 0.4017 - val_accuracy: 0.5735\n",
      "Epoch 678/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1658 - accuracy: 0.5996 - val_loss: 0.4171 - val_accuracy: 0.5353\n",
      "Epoch 679/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1881 - accuracy: 0.5940 - val_loss: 0.4184 - val_accuracy: 0.5353\n",
      "Epoch 680/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1812 - accuracy: 0.5933 - val_loss: 0.3982 - val_accuracy: 0.5618\n",
      "Epoch 681/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1906 - accuracy: 0.5948 - val_loss: 0.4049 - val_accuracy: 0.5471\n",
      "Epoch 682/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1826 - accuracy: 0.5937 - val_loss: 0.4223 - val_accuracy: 0.5471\n",
      "Epoch 683/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1889 - accuracy: 0.5926 - val_loss: 0.4093 - val_accuracy: 0.5471\n",
      "Epoch 684/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1790 - accuracy: 0.5940 - val_loss: 0.4216 - val_accuracy: 0.5500\n",
      "Epoch 685/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1852 - accuracy: 0.5915 - val_loss: 0.3924 - val_accuracy: 0.5382\n",
      "Epoch 686/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1764 - accuracy: 0.5959 - val_loss: 0.4189 - val_accuracy: 0.5382\n",
      "Epoch 687/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1772 - accuracy: 0.5966 - val_loss: 0.4013 - val_accuracy: 0.5441\n",
      "Epoch 688/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1817 - accuracy: 0.5951 - val_loss: 0.4232 - val_accuracy: 0.5529\n",
      "Epoch 689/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1832 - accuracy: 0.5948 - val_loss: 0.4238 - val_accuracy: 0.5618\n",
      "Epoch 690/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1842 - accuracy: 0.5944 - val_loss: 0.4091 - val_accuracy: 0.5500\n",
      "Epoch 691/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1835 - accuracy: 0.5940 - val_loss: 0.3907 - val_accuracy: 0.5588\n",
      "Epoch 692/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1690 - accuracy: 0.5959 - val_loss: 0.4183 - val_accuracy: 0.5412\n",
      "Epoch 693/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1791 - accuracy: 0.5955 - val_loss: 0.4106 - val_accuracy: 0.5500\n",
      "Epoch 694/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1848 - accuracy: 0.5926 - val_loss: 0.3836 - val_accuracy: 0.5529\n",
      "Epoch 695/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1627 - accuracy: 0.5999 - val_loss: 0.3974 - val_accuracy: 0.5471\n",
      "Epoch 696/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1887 - accuracy: 0.5933 - val_loss: 0.4017 - val_accuracy: 0.5588\n",
      "Epoch 697/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1791 - accuracy: 0.5940 - val_loss: 0.3995 - val_accuracy: 0.5529\n",
      "Epoch 698/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.1719 - accuracy: 0.5962 - val_loss: 0.3998 - val_accuracy: 0.5500\n",
      "Epoch 699/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1719 - accuracy: 0.5977 - val_loss: 0.4338 - val_accuracy: 0.5382\n",
      "Epoch 700/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1885 - accuracy: 0.5959 - val_loss: 0.3874 - val_accuracy: 0.5382\n",
      "Epoch 701/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1918 - accuracy: 0.5937 - val_loss: 0.4093 - val_accuracy: 0.5441\n",
      "Epoch 702/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1728 - accuracy: 0.5966 - val_loss: 0.4800 - val_accuracy: 0.5118\n",
      "Epoch 703/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1870 - accuracy: 0.5944 - val_loss: 0.4033 - val_accuracy: 0.5529\n",
      "Epoch 704/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1883 - accuracy: 0.5948 - val_loss: 0.3992 - val_accuracy: 0.5559\n",
      "Epoch 705/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1682 - accuracy: 0.5981 - val_loss: 0.4306 - val_accuracy: 0.5412\n",
      "Epoch 706/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1857 - accuracy: 0.5937 - val_loss: 0.4555 - val_accuracy: 0.5353\n",
      "Epoch 707/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1770 - accuracy: 0.5970 - val_loss: 0.4635 - val_accuracy: 0.5353\n",
      "Epoch 708/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1800 - accuracy: 0.5929 - val_loss: 0.4391 - val_accuracy: 0.5471\n",
      "Epoch 709/1600\n",
      "680/680 [==============================] - 1s 862us/step - loss: 0.1901 - accuracy: 0.5926 - val_loss: 0.4148 - val_accuracy: 0.5382\n",
      "Epoch 710/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1766 - accuracy: 0.5966 - val_loss: 0.4092 - val_accuracy: 0.5471\n",
      "Epoch 711/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1799 - accuracy: 0.5962 - val_loss: 0.4100 - val_accuracy: 0.5618\n",
      "Epoch 712/1600\n",
      "680/680 [==============================] - 1s 866us/step - loss: 0.1771 - accuracy: 0.5970 - val_loss: 0.4027 - val_accuracy: 0.5441\n",
      "Epoch 713/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1860 - accuracy: 0.5944 - val_loss: 0.4045 - val_accuracy: 0.5412\n",
      "Epoch 714/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.1697 - accuracy: 0.5981 - val_loss: 0.4092 - val_accuracy: 0.5441\n",
      "Epoch 715/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1893 - accuracy: 0.5955 - val_loss: 0.4179 - val_accuracy: 0.5441\n",
      "Epoch 716/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1838 - accuracy: 0.5929 - val_loss: 0.4078 - val_accuracy: 0.5500\n",
      "Epoch 717/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1781 - accuracy: 0.5948 - val_loss: 0.4318 - val_accuracy: 0.5500\n",
      "Epoch 718/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 942us/step - loss: 0.1769 - accuracy: 0.5959 - val_loss: 0.4143 - val_accuracy: 0.5471\n",
      "Epoch 719/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.1782 - accuracy: 0.5981 - val_loss: 0.3744 - val_accuracy: 0.5588\n",
      "Epoch 720/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1789 - accuracy: 0.5974 - val_loss: 0.4097 - val_accuracy: 0.5647\n",
      "Epoch 721/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1748 - accuracy: 0.5988 - val_loss: 0.4328 - val_accuracy: 0.5441\n",
      "Epoch 722/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1812 - accuracy: 0.5985 - val_loss: 0.4250 - val_accuracy: 0.5353\n",
      "Epoch 723/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1893 - accuracy: 0.5926 - val_loss: 0.4198 - val_accuracy: 0.5441\n",
      "Epoch 724/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1770 - accuracy: 0.5929 - val_loss: 0.4424 - val_accuracy: 0.5559\n",
      "Epoch 725/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1717 - accuracy: 0.5962 - val_loss: 0.4016 - val_accuracy: 0.5529\n",
      "Epoch 726/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1924 - accuracy: 0.5937 - val_loss: 0.5060 - val_accuracy: 0.5118\n",
      "Epoch 727/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1962 - accuracy: 0.5926 - val_loss: 0.4507 - val_accuracy: 0.5382\n",
      "Epoch 728/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1655 - accuracy: 0.6003 - val_loss: 0.4838 - val_accuracy: 0.5265\n",
      "Epoch 729/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1807 - accuracy: 0.5937 - val_loss: 0.4039 - val_accuracy: 0.5412\n",
      "Epoch 730/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1770 - accuracy: 0.5970 - val_loss: 0.4291 - val_accuracy: 0.5382\n",
      "Epoch 731/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1719 - accuracy: 0.5977 - val_loss: 0.4428 - val_accuracy: 0.5324\n",
      "Epoch 732/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1900 - accuracy: 0.5929 - val_loss: 0.4339 - val_accuracy: 0.5559\n",
      "Epoch 733/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1771 - accuracy: 0.5937 - val_loss: 0.3939 - val_accuracy: 0.5382\n",
      "Epoch 734/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1669 - accuracy: 0.5985 - val_loss: 0.3876 - val_accuracy: 0.5471\n",
      "Epoch 735/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1756 - accuracy: 0.5966 - val_loss: 0.3894 - val_accuracy: 0.5471\n",
      "Epoch 736/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1864 - accuracy: 0.5944 - val_loss: 0.4280 - val_accuracy: 0.5412\n",
      "Epoch 737/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1767 - accuracy: 0.5959 - val_loss: 0.4319 - val_accuracy: 0.5382\n",
      "Epoch 738/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1758 - accuracy: 0.5974 - val_loss: 0.4205 - val_accuracy: 0.5441\n",
      "Epoch 739/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1835 - accuracy: 0.5948 - val_loss: 0.4520 - val_accuracy: 0.5353\n",
      "Epoch 740/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1818 - accuracy: 0.5959 - val_loss: 0.4463 - val_accuracy: 0.5412\n",
      "Epoch 741/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1708 - accuracy: 0.5985 - val_loss: 0.4483 - val_accuracy: 0.5471\n",
      "Epoch 742/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1725 - accuracy: 0.5966 - val_loss: 0.4007 - val_accuracy: 0.5500\n",
      "Epoch 743/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1876 - accuracy: 0.5940 - val_loss: 0.4118 - val_accuracy: 0.5353\n",
      "Epoch 744/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1788 - accuracy: 0.5974 - val_loss: 0.4142 - val_accuracy: 0.5382\n",
      "Epoch 745/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1746 - accuracy: 0.5962 - val_loss: 0.4620 - val_accuracy: 0.5412\n",
      "Epoch 746/1600\n",
      "680/680 [==============================] - 1s 965us/step - loss: 0.1769 - accuracy: 0.5966 - val_loss: 0.4586 - val_accuracy: 0.5441\n",
      "Epoch 747/1600\n",
      "680/680 [==============================] - 1s 942us/step - loss: 0.1703 - accuracy: 0.5985 - val_loss: 0.4338 - val_accuracy: 0.5559\n",
      "Epoch 748/1600\n",
      "680/680 [==============================] - 1s 944us/step - loss: 0.1959 - accuracy: 0.5933 - val_loss: 0.4188 - val_accuracy: 0.5382\n",
      "Epoch 749/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1761 - accuracy: 0.5970 - val_loss: 0.4115 - val_accuracy: 0.5500\n",
      "Epoch 750/1600\n",
      "680/680 [==============================] - 1s 930us/step - loss: 0.1766 - accuracy: 0.5970 - val_loss: 0.3997 - val_accuracy: 0.5412\n",
      "Epoch 751/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1827 - accuracy: 0.5951 - val_loss: 0.4161 - val_accuracy: 0.5382\n",
      "Epoch 752/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.2083 - accuracy: 0.5893 - val_loss: 0.4454 - val_accuracy: 0.5353\n",
      "Epoch 753/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1784 - accuracy: 0.5951 - val_loss: 0.3817 - val_accuracy: 0.5382\n",
      "Epoch 754/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1745 - accuracy: 0.5962 - val_loss: 0.4251 - val_accuracy: 0.5441\n",
      "Epoch 755/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1700 - accuracy: 0.5988 - val_loss: 0.4418 - val_accuracy: 0.5324\n",
      "Epoch 756/1600\n",
      "680/680 [==============================] - 1s 968us/step - loss: 0.1892 - accuracy: 0.5948 - val_loss: 0.3940 - val_accuracy: 0.5441\n",
      "Epoch 757/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1824 - accuracy: 0.5951 - val_loss: 0.4161 - val_accuracy: 0.5412\n",
      "Epoch 758/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1717 - accuracy: 0.5962 - val_loss: 0.3915 - val_accuracy: 0.5441\n",
      "Epoch 759/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1684 - accuracy: 0.5981 - val_loss: 0.4306 - val_accuracy: 0.5294\n",
      "Epoch 760/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1827 - accuracy: 0.5929 - val_loss: 0.4200 - val_accuracy: 0.5529\n",
      "Epoch 761/1600\n",
      "680/680 [==============================] - 1s 903us/step - loss: 0.1848 - accuracy: 0.5951 - val_loss: 0.3958 - val_accuracy: 0.5500\n",
      "Epoch 762/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1855 - accuracy: 0.5940 - val_loss: 0.4340 - val_accuracy: 0.5324\n",
      "Epoch 763/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1846 - accuracy: 0.5948 - val_loss: 0.4224 - val_accuracy: 0.5353\n",
      "Epoch 764/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1736 - accuracy: 0.5981 - val_loss: 0.4033 - val_accuracy: 0.5471\n",
      "Epoch 765/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1716 - accuracy: 0.5970 - val_loss: 0.4227 - val_accuracy: 0.5324\n",
      "Epoch 766/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1859 - accuracy: 0.5955 - val_loss: 0.4709 - val_accuracy: 0.5265\n",
      "Epoch 767/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1852 - accuracy: 0.5944 - val_loss: 0.4122 - val_accuracy: 0.5441\n",
      "Epoch 768/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1756 - accuracy: 0.5962 - val_loss: 0.4110 - val_accuracy: 0.5471\n",
      "Epoch 769/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1902 - accuracy: 0.5911 - val_loss: 0.4312 - val_accuracy: 0.5529\n",
      "Epoch 770/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1783 - accuracy: 0.5959 - val_loss: 0.4091 - val_accuracy: 0.5412\n",
      "Epoch 771/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1794 - accuracy: 0.5951 - val_loss: 0.4190 - val_accuracy: 0.5441\n",
      "Epoch 772/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1684 - accuracy: 0.5977 - val_loss: 0.4248 - val_accuracy: 0.5353\n",
      "Epoch 773/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 924us/step - loss: 0.1893 - accuracy: 0.5933 - val_loss: 0.4036 - val_accuracy: 0.5529\n",
      "Epoch 774/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1668 - accuracy: 0.5974 - val_loss: 0.4447 - val_accuracy: 0.5441\n",
      "Epoch 775/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.2049 - accuracy: 0.5922 - val_loss: 0.4371 - val_accuracy: 0.5353\n",
      "Epoch 776/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1838 - accuracy: 0.5937 - val_loss: 0.4229 - val_accuracy: 0.5441\n",
      "Epoch 777/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1782 - accuracy: 0.5951 - val_loss: 0.4130 - val_accuracy: 0.5529\n",
      "Epoch 778/1600\n",
      "680/680 [==============================] - 1s 930us/step - loss: 0.1748 - accuracy: 0.5933 - val_loss: 0.4156 - val_accuracy: 0.5412\n",
      "Epoch 779/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1856 - accuracy: 0.5948 - val_loss: 0.4259 - val_accuracy: 0.5500\n",
      "Epoch 780/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1781 - accuracy: 0.5955 - val_loss: 0.4136 - val_accuracy: 0.5412\n",
      "Epoch 781/1600\n",
      "680/680 [==============================] - 1s 943us/step - loss: 0.1790 - accuracy: 0.5933 - val_loss: 0.4244 - val_accuracy: 0.5382\n",
      "Epoch 782/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1721 - accuracy: 0.5966 - val_loss: 0.4150 - val_accuracy: 0.5324\n",
      "Epoch 783/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1585 - accuracy: 0.6014 - val_loss: 0.4339 - val_accuracy: 0.5324\n",
      "Epoch 784/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1959 - accuracy: 0.5915 - val_loss: 0.4088 - val_accuracy: 0.5441\n",
      "Epoch 785/1600\n",
      "680/680 [==============================] - 1s 938us/step - loss: 0.1864 - accuracy: 0.5948 - val_loss: 0.4422 - val_accuracy: 0.5412\n",
      "Epoch 786/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1756 - accuracy: 0.5981 - val_loss: 0.4704 - val_accuracy: 0.5206\n",
      "Epoch 787/1600\n",
      "680/680 [==============================] - 1s 907us/step - loss: 0.1773 - accuracy: 0.5933 - val_loss: 0.4507 - val_accuracy: 0.5382\n",
      "Epoch 788/1600\n",
      "680/680 [==============================] - 1s 934us/step - loss: 0.1702 - accuracy: 0.5992 - val_loss: 0.4311 - val_accuracy: 0.5412\n",
      "Epoch 789/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1754 - accuracy: 0.5977 - val_loss: 0.4495 - val_accuracy: 0.5676\n",
      "Epoch 790/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1873 - accuracy: 0.5948 - val_loss: 0.3964 - val_accuracy: 0.5382\n",
      "Epoch 791/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1768 - accuracy: 0.5951 - val_loss: 0.4032 - val_accuracy: 0.5529\n",
      "Epoch 792/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1861 - accuracy: 0.5915 - val_loss: 0.4183 - val_accuracy: 0.5382\n",
      "Epoch 793/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1764 - accuracy: 0.5948 - val_loss: 0.4372 - val_accuracy: 0.5382\n",
      "Epoch 794/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1712 - accuracy: 0.5974 - val_loss: 0.3986 - val_accuracy: 0.5382\n",
      "Epoch 795/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1990 - accuracy: 0.5915 - val_loss: 0.4474 - val_accuracy: 0.5294\n",
      "Epoch 796/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1892 - accuracy: 0.5929 - val_loss: 0.4266 - val_accuracy: 0.5647\n",
      "Epoch 797/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1744 - accuracy: 0.5966 - val_loss: 0.4157 - val_accuracy: 0.5382\n",
      "Epoch 798/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1765 - accuracy: 0.5974 - val_loss: 0.4048 - val_accuracy: 0.5382\n",
      "Epoch 799/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1808 - accuracy: 0.5951 - val_loss: 0.4366 - val_accuracy: 0.5324\n",
      "Epoch 800/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1759 - accuracy: 0.5966 - val_loss: 0.3688 - val_accuracy: 0.5471\n",
      "Epoch 801/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1881 - accuracy: 0.5944 - val_loss: 0.4013 - val_accuracy: 0.5529\n",
      "Epoch 802/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1782 - accuracy: 0.5940 - val_loss: 0.4115 - val_accuracy: 0.5382\n",
      "Epoch 803/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.1649 - accuracy: 0.5966 - val_loss: 0.4400 - val_accuracy: 0.5441\n",
      "Epoch 804/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1796 - accuracy: 0.5959 - val_loss: 0.4256 - val_accuracy: 0.5471\n",
      "Epoch 805/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1756 - accuracy: 0.5981 - val_loss: 0.4572 - val_accuracy: 0.5441\n",
      "Epoch 806/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1729 - accuracy: 0.5955 - val_loss: 0.4562 - val_accuracy: 0.5412\n",
      "Epoch 807/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1781 - accuracy: 0.5959 - val_loss: 0.4157 - val_accuracy: 0.5441\n",
      "Epoch 808/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1843 - accuracy: 0.5944 - val_loss: 0.4181 - val_accuracy: 0.5441\n",
      "Epoch 809/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1747 - accuracy: 0.5970 - val_loss: 0.4523 - val_accuracy: 0.5324\n",
      "Epoch 810/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1776 - accuracy: 0.5974 - val_loss: 0.4246 - val_accuracy: 0.5412\n",
      "Epoch 811/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1816 - accuracy: 0.5948 - val_loss: 0.4266 - val_accuracy: 0.5412\n",
      "Epoch 812/1600\n",
      "680/680 [==============================] - 1s 862us/step - loss: 0.1786 - accuracy: 0.5948 - val_loss: 0.4560 - val_accuracy: 0.5265\n",
      "Epoch 813/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1701 - accuracy: 0.5974 - val_loss: 0.4328 - val_accuracy: 0.5412\n",
      "Epoch 814/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1727 - accuracy: 0.5959 - val_loss: 0.4252 - val_accuracy: 0.5618\n",
      "Epoch 815/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1693 - accuracy: 0.5985 - val_loss: 0.4152 - val_accuracy: 0.5500\n",
      "Epoch 816/1600\n",
      "680/680 [==============================] - 1s 907us/step - loss: 0.1809 - accuracy: 0.5959 - val_loss: 0.4566 - val_accuracy: 0.5412\n",
      "Epoch 817/1600\n",
      "680/680 [==============================] - 1s 935us/step - loss: 0.1891 - accuracy: 0.5926 - val_loss: 0.4336 - val_accuracy: 0.5412\n",
      "Epoch 818/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1786 - accuracy: 0.5929 - val_loss: 0.4084 - val_accuracy: 0.5382\n",
      "Epoch 819/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1874 - accuracy: 0.5955 - val_loss: 0.4076 - val_accuracy: 0.5529\n",
      "Epoch 820/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1844 - accuracy: 0.5974 - val_loss: 0.4259 - val_accuracy: 0.5412\n",
      "Epoch 821/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1925 - accuracy: 0.5922 - val_loss: 0.4754 - val_accuracy: 0.5471\n",
      "Epoch 822/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1968 - accuracy: 0.5918 - val_loss: 0.4539 - val_accuracy: 0.5441\n",
      "Epoch 823/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1717 - accuracy: 0.5977 - val_loss: 0.3948 - val_accuracy: 0.5588\n",
      "Epoch 824/1600\n",
      "680/680 [==============================] - 1s 906us/step - loss: 0.1767 - accuracy: 0.5974 - val_loss: 0.4399 - val_accuracy: 0.5324\n",
      "Epoch 825/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1807 - accuracy: 0.5962 - val_loss: 0.4417 - val_accuracy: 0.5412\n",
      "Epoch 826/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1749 - accuracy: 0.5962 - val_loss: 0.4501 - val_accuracy: 0.5412\n",
      "Epoch 827/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1788 - accuracy: 0.5962 - val_loss: 0.4444 - val_accuracy: 0.5324\n",
      "Epoch 828/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 874us/step - loss: 0.1740 - accuracy: 0.5955 - val_loss: 0.4335 - val_accuracy: 0.5382\n",
      "Epoch 829/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1938 - accuracy: 0.5933 - val_loss: 0.4264 - val_accuracy: 0.5441\n",
      "Epoch 830/1600\n",
      "680/680 [==============================] - 1s 864us/step - loss: 0.1808 - accuracy: 0.5955 - val_loss: 0.4130 - val_accuracy: 0.5412\n",
      "Epoch 831/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1639 - accuracy: 0.5996 - val_loss: 0.4530 - val_accuracy: 0.5324\n",
      "Epoch 832/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1834 - accuracy: 0.5933 - val_loss: 0.4580 - val_accuracy: 0.5412\n",
      "Epoch 833/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.2012 - accuracy: 0.5907 - val_loss: 0.4535 - val_accuracy: 0.5353\n",
      "Epoch 834/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1833 - accuracy: 0.5937 - val_loss: 0.4390 - val_accuracy: 0.5500\n",
      "Epoch 835/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1756 - accuracy: 0.5977 - val_loss: 0.4551 - val_accuracy: 0.5353\n",
      "Epoch 836/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1689 - accuracy: 0.5962 - val_loss: 0.4232 - val_accuracy: 0.5529\n",
      "Epoch 837/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1668 - accuracy: 0.5985 - val_loss: 0.4761 - val_accuracy: 0.5412\n",
      "Epoch 838/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1778 - accuracy: 0.5955 - val_loss: 0.4526 - val_accuracy: 0.5529\n",
      "Epoch 839/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1912 - accuracy: 0.5929 - val_loss: 0.4570 - val_accuracy: 0.5235\n",
      "Epoch 840/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.1751 - accuracy: 0.5951 - val_loss: 0.4347 - val_accuracy: 0.5500\n",
      "Epoch 841/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1806 - accuracy: 0.5951 - val_loss: 0.4598 - val_accuracy: 0.5382\n",
      "Epoch 842/1600\n",
      "680/680 [==============================] - 1s 902us/step - loss: 0.1824 - accuracy: 0.5962 - val_loss: 0.4518 - val_accuracy: 0.5471\n",
      "Epoch 843/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1749 - accuracy: 0.5977 - val_loss: 0.4316 - val_accuracy: 0.5471\n",
      "Epoch 844/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1842 - accuracy: 0.5926 - val_loss: 0.4242 - val_accuracy: 0.5471\n",
      "Epoch 845/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1684 - accuracy: 0.5970 - val_loss: 0.4329 - val_accuracy: 0.5559\n",
      "Epoch 846/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1733 - accuracy: 0.5985 - val_loss: 0.4060 - val_accuracy: 0.5500\n",
      "Epoch 847/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.1828 - accuracy: 0.5951 - val_loss: 0.4329 - val_accuracy: 0.5500\n",
      "Epoch 848/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1795 - accuracy: 0.5937 - val_loss: 0.4434 - val_accuracy: 0.5500\n",
      "Epoch 849/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1875 - accuracy: 0.5955 - val_loss: 0.4626 - val_accuracy: 0.5441\n",
      "Epoch 850/1600\n",
      "680/680 [==============================] - 1s 864us/step - loss: 0.1824 - accuracy: 0.5944 - val_loss: 0.4320 - val_accuracy: 0.5471\n",
      "Epoch 851/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1896 - accuracy: 0.5937 - val_loss: 0.4559 - val_accuracy: 0.5412\n",
      "Epoch 852/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1687 - accuracy: 0.5937 - val_loss: 0.4344 - val_accuracy: 0.5382\n",
      "Epoch 853/1600\n",
      "680/680 [==============================] - 1s 868us/step - loss: 0.1812 - accuracy: 0.5948 - val_loss: 0.3901 - val_accuracy: 0.5441\n",
      "Epoch 854/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1924 - accuracy: 0.5929 - val_loss: 0.4309 - val_accuracy: 0.5382\n",
      "Epoch 855/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1868 - accuracy: 0.5955 - val_loss: 0.4450 - val_accuracy: 0.5441\n",
      "Epoch 856/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1679 - accuracy: 0.5985 - val_loss: 0.4657 - val_accuracy: 0.5471\n",
      "Epoch 857/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1726 - accuracy: 0.5944 - val_loss: 0.4559 - val_accuracy: 0.5324\n",
      "Epoch 858/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1848 - accuracy: 0.5948 - val_loss: 0.4123 - val_accuracy: 0.5441\n",
      "Epoch 859/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1736 - accuracy: 0.5951 - val_loss: 0.4017 - val_accuracy: 0.5412\n",
      "Epoch 860/1600\n",
      "680/680 [==============================] - 1s 868us/step - loss: 0.1792 - accuracy: 0.5951 - val_loss: 0.4077 - val_accuracy: 0.5441\n",
      "Epoch 861/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1712 - accuracy: 0.5962 - val_loss: 0.4073 - val_accuracy: 0.5500\n",
      "Epoch 862/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.2208 - accuracy: 0.5878 - val_loss: 0.4311 - val_accuracy: 0.5441\n",
      "Epoch 863/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1718 - accuracy: 0.5977 - val_loss: 0.4400 - val_accuracy: 0.5412\n",
      "Epoch 864/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1705 - accuracy: 0.5992 - val_loss: 0.4505 - val_accuracy: 0.5206\n",
      "Epoch 865/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1821 - accuracy: 0.5966 - val_loss: 0.4632 - val_accuracy: 0.5471\n",
      "Epoch 866/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1766 - accuracy: 0.5966 - val_loss: 0.4728 - val_accuracy: 0.5294\n",
      "Epoch 867/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1850 - accuracy: 0.5948 - val_loss: 0.4448 - val_accuracy: 0.5324\n",
      "Epoch 868/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1751 - accuracy: 0.5966 - val_loss: 0.4258 - val_accuracy: 0.5618\n",
      "Epoch 869/1600\n",
      "680/680 [==============================] - 1s 936us/step - loss: 0.1751 - accuracy: 0.5996 - val_loss: 0.4521 - val_accuracy: 0.5412\n",
      "Epoch 870/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1670 - accuracy: 0.5977 - val_loss: 0.4690 - val_accuracy: 0.5382\n",
      "Epoch 871/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1825 - accuracy: 0.5940 - val_loss: 0.4610 - val_accuracy: 0.5382\n",
      "Epoch 872/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1778 - accuracy: 0.5970 - val_loss: 0.4393 - val_accuracy: 0.5412\n",
      "Epoch 873/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1797 - accuracy: 0.5974 - val_loss: 0.4356 - val_accuracy: 0.5412\n",
      "Epoch 874/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1754 - accuracy: 0.5996 - val_loss: 0.4295 - val_accuracy: 0.5471\n",
      "Epoch 875/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1738 - accuracy: 0.5937 - val_loss: 0.4293 - val_accuracy: 0.5529\n",
      "Epoch 876/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1746 - accuracy: 0.5988 - val_loss: 0.4675 - val_accuracy: 0.5176\n",
      "Epoch 877/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1832 - accuracy: 0.5962 - val_loss: 0.4491 - val_accuracy: 0.5412\n",
      "Epoch 878/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.1807 - accuracy: 0.5966 - val_loss: 0.4407 - val_accuracy: 0.5324\n",
      "Epoch 879/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1898 - accuracy: 0.5933 - val_loss: 0.4451 - val_accuracy: 0.5382\n",
      "Epoch 880/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1875 - accuracy: 0.5922 - val_loss: 0.4138 - val_accuracy: 0.5412\n",
      "Epoch 881/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1694 - accuracy: 0.5970 - val_loss: 0.4243 - val_accuracy: 0.5471\n",
      "Epoch 882/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1847 - accuracy: 0.5933 - val_loss: 0.4022 - val_accuracy: 0.5559\n",
      "Epoch 883/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 914us/step - loss: 0.1699 - accuracy: 0.5985 - val_loss: 0.4276 - val_accuracy: 0.5441\n",
      "Epoch 884/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1847 - accuracy: 0.5959 - val_loss: 0.4396 - val_accuracy: 0.5412\n",
      "Epoch 885/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1904 - accuracy: 0.5951 - val_loss: 0.4208 - val_accuracy: 0.5618\n",
      "Epoch 886/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1760 - accuracy: 0.5959 - val_loss: 0.4379 - val_accuracy: 0.5353\n",
      "Epoch 887/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1706 - accuracy: 0.5966 - val_loss: 0.4485 - val_accuracy: 0.5441\n",
      "Epoch 888/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1866 - accuracy: 0.5922 - val_loss: 0.4423 - val_accuracy: 0.5559\n",
      "Epoch 889/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1812 - accuracy: 0.5940 - val_loss: 0.4433 - val_accuracy: 0.5441\n",
      "Epoch 890/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1724 - accuracy: 0.6010 - val_loss: 0.4233 - val_accuracy: 0.5382\n",
      "Epoch 891/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1780 - accuracy: 0.5974 - val_loss: 0.4907 - val_accuracy: 0.5206\n",
      "Epoch 892/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1836 - accuracy: 0.5926 - val_loss: 0.4434 - val_accuracy: 0.5382\n",
      "Epoch 893/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1783 - accuracy: 0.5966 - val_loss: 0.4118 - val_accuracy: 0.5412\n",
      "Epoch 894/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1744 - accuracy: 0.5970 - val_loss: 0.4498 - val_accuracy: 0.5294\n",
      "Epoch 895/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1855 - accuracy: 0.5944 - val_loss: 0.4225 - val_accuracy: 0.5382\n",
      "Epoch 896/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1839 - accuracy: 0.5955 - val_loss: 0.4200 - val_accuracy: 0.5441\n",
      "Epoch 897/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1733 - accuracy: 0.5970 - val_loss: 0.4264 - val_accuracy: 0.5441\n",
      "Epoch 898/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1738 - accuracy: 0.5966 - val_loss: 0.4454 - val_accuracy: 0.5382\n",
      "Epoch 899/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1775 - accuracy: 0.5977 - val_loss: 0.4573 - val_accuracy: 0.5412\n",
      "Epoch 900/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1913 - accuracy: 0.5929 - val_loss: 0.4681 - val_accuracy: 0.5294\n",
      "Epoch 901/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1833 - accuracy: 0.5940 - val_loss: 0.3768 - val_accuracy: 0.5529\n",
      "Epoch 902/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1888 - accuracy: 0.5937 - val_loss: 0.4689 - val_accuracy: 0.5353\n",
      "Epoch 903/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1849 - accuracy: 0.5933 - val_loss: 0.4311 - val_accuracy: 0.5382\n",
      "Epoch 904/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1686 - accuracy: 0.5985 - val_loss: 0.4758 - val_accuracy: 0.5529\n",
      "Epoch 905/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1817 - accuracy: 0.5948 - val_loss: 0.4252 - val_accuracy: 0.5382\n",
      "Epoch 906/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1851 - accuracy: 0.5966 - val_loss: 0.4314 - val_accuracy: 0.5412\n",
      "Epoch 907/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1855 - accuracy: 0.5944 - val_loss: 0.4279 - val_accuracy: 0.5500\n",
      "Epoch 908/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1751 - accuracy: 0.5962 - val_loss: 0.4210 - val_accuracy: 0.5471\n",
      "Epoch 909/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1937 - accuracy: 0.5944 - val_loss: 0.4486 - val_accuracy: 0.5382\n",
      "Epoch 910/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1782 - accuracy: 0.5929 - val_loss: 0.4356 - val_accuracy: 0.5471\n",
      "Epoch 911/1600\n",
      "680/680 [==============================] - 1s 863us/step - loss: 0.1747 - accuracy: 0.5966 - val_loss: 0.4259 - val_accuracy: 0.5471\n",
      "Epoch 912/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1631 - accuracy: 0.6021 - val_loss: 0.4580 - val_accuracy: 0.5382\n",
      "Epoch 913/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1895 - accuracy: 0.5926 - val_loss: 0.4172 - val_accuracy: 0.5382\n",
      "Epoch 914/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1789 - accuracy: 0.5933 - val_loss: 0.4212 - val_accuracy: 0.5441\n",
      "Epoch 915/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1758 - accuracy: 0.5955 - val_loss: 0.4387 - val_accuracy: 0.5382\n",
      "Epoch 916/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1746 - accuracy: 0.5966 - val_loss: 0.4296 - val_accuracy: 0.5412\n",
      "Epoch 917/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1797 - accuracy: 0.5933 - val_loss: 0.4176 - val_accuracy: 0.5471\n",
      "Epoch 918/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1789 - accuracy: 0.5985 - val_loss: 0.4703 - val_accuracy: 0.5324\n",
      "Epoch 919/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1697 - accuracy: 0.5974 - val_loss: 0.3895 - val_accuracy: 0.5647\n",
      "Epoch 920/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1844 - accuracy: 0.5926 - val_loss: 0.4207 - val_accuracy: 0.5588\n",
      "Epoch 921/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1740 - accuracy: 0.5974 - val_loss: 0.4554 - val_accuracy: 0.5353\n",
      "Epoch 922/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1737 - accuracy: 0.5966 - val_loss: 0.4081 - val_accuracy: 0.5412\n",
      "Epoch 923/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1799 - accuracy: 0.5940 - val_loss: 0.4495 - val_accuracy: 0.5382\n",
      "Epoch 924/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1765 - accuracy: 0.5988 - val_loss: 0.4404 - val_accuracy: 0.5382\n",
      "Epoch 925/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1690 - accuracy: 0.5992 - val_loss: 0.4539 - val_accuracy: 0.5471\n",
      "Epoch 926/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1778 - accuracy: 0.5944 - val_loss: 0.4452 - val_accuracy: 0.5412\n",
      "Epoch 927/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1800 - accuracy: 0.5948 - val_loss: 0.4355 - val_accuracy: 0.5412\n",
      "Epoch 928/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1665 - accuracy: 0.5977 - val_loss: 0.4314 - val_accuracy: 0.5412\n",
      "Epoch 929/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1771 - accuracy: 0.5955 - val_loss: 0.3747 - val_accuracy: 0.5588\n",
      "Epoch 930/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1950 - accuracy: 0.5940 - val_loss: 0.4029 - val_accuracy: 0.5529\n",
      "Epoch 931/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1700 - accuracy: 0.5977 - val_loss: 0.4329 - val_accuracy: 0.5294\n",
      "Epoch 932/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1830 - accuracy: 0.5955 - val_loss: 0.4076 - val_accuracy: 0.5471\n",
      "Epoch 933/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1755 - accuracy: 0.5970 - val_loss: 0.4399 - val_accuracy: 0.5353\n",
      "Epoch 934/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1794 - accuracy: 0.5955 - val_loss: 0.4524 - val_accuracy: 0.5412\n",
      "Epoch 935/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1899 - accuracy: 0.5904 - val_loss: 0.4323 - val_accuracy: 0.5382\n",
      "Epoch 936/1600\n",
      "680/680 [==============================] - 1s 868us/step - loss: 0.1739 - accuracy: 0.5962 - val_loss: 0.4291 - val_accuracy: 0.5471\n",
      "Epoch 937/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1697 - accuracy: 0.5962 - val_loss: 0.4259 - val_accuracy: 0.5529\n",
      "Epoch 938/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 883us/step - loss: 0.1940 - accuracy: 0.5926 - val_loss: 0.4070 - val_accuracy: 0.5529\n",
      "Epoch 939/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1793 - accuracy: 0.5926 - val_loss: 0.4294 - val_accuracy: 0.5382\n",
      "Epoch 940/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1730 - accuracy: 0.5962 - val_loss: 0.4138 - val_accuracy: 0.5500\n",
      "Epoch 941/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1781 - accuracy: 0.5933 - val_loss: 0.4358 - val_accuracy: 0.5471\n",
      "Epoch 942/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1874 - accuracy: 0.5937 - val_loss: 0.4234 - val_accuracy: 0.5353\n",
      "Epoch 943/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1712 - accuracy: 0.5959 - val_loss: 0.3970 - val_accuracy: 0.5412\n",
      "Epoch 944/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1738 - accuracy: 0.5966 - val_loss: 0.4057 - val_accuracy: 0.5324\n",
      "Epoch 945/1600\n",
      "680/680 [==============================] - 1s 968us/step - loss: 0.1868 - accuracy: 0.5929 - val_loss: 0.4284 - val_accuracy: 0.5441\n",
      "Epoch 946/1600\n",
      "680/680 [==============================] - 1s 949us/step - loss: 0.1809 - accuracy: 0.5951 - val_loss: 0.4342 - val_accuracy: 0.5412\n",
      "Epoch 947/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1865 - accuracy: 0.5951 - val_loss: 0.4098 - val_accuracy: 0.5412\n",
      "Epoch 948/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1702 - accuracy: 0.5981 - val_loss: 0.4292 - val_accuracy: 0.5353\n",
      "Epoch 949/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1718 - accuracy: 0.5977 - val_loss: 0.4110 - val_accuracy: 0.5412\n",
      "Epoch 950/1600\n",
      "680/680 [==============================] - 1s 990us/step - loss: 0.1839 - accuracy: 0.5937 - val_loss: 0.4091 - val_accuracy: 0.5412\n",
      "Epoch 951/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1841 - accuracy: 0.5948 - val_loss: 0.4269 - val_accuracy: 0.5441\n",
      "Epoch 952/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1824 - accuracy: 0.5929 - val_loss: 0.4432 - val_accuracy: 0.5324\n",
      "Epoch 953/1600\n",
      "680/680 [==============================] - 1s 963us/step - loss: 0.1748 - accuracy: 0.5962 - val_loss: 0.4312 - val_accuracy: 0.5382\n",
      "Epoch 954/1600\n",
      "680/680 [==============================] - 1s 949us/step - loss: 0.1756 - accuracy: 0.5951 - val_loss: 0.4273 - val_accuracy: 0.5353\n",
      "Epoch 955/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1684 - accuracy: 0.5988 - val_loss: 0.4153 - val_accuracy: 0.5412\n",
      "Epoch 956/1600\n",
      "680/680 [==============================] - 1s 854us/step - loss: 0.1724 - accuracy: 0.5981 - val_loss: 0.4452 - val_accuracy: 0.5265\n",
      "Epoch 957/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1753 - accuracy: 0.5974 - val_loss: 0.4200 - val_accuracy: 0.5588\n",
      "Epoch 958/1600\n",
      "680/680 [==============================] - 1s 941us/step - loss: 0.1936 - accuracy: 0.5937 - val_loss: 0.4279 - val_accuracy: 0.5382\n",
      "Epoch 959/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1679 - accuracy: 0.5948 - val_loss: 0.4372 - val_accuracy: 0.5471\n",
      "Epoch 960/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1677 - accuracy: 0.5985 - val_loss: 0.4385 - val_accuracy: 0.5441\n",
      "Epoch 961/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1818 - accuracy: 0.5926 - val_loss: 0.4491 - val_accuracy: 0.5353\n",
      "Epoch 962/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1775 - accuracy: 0.5966 - val_loss: 0.4342 - val_accuracy: 0.5529\n",
      "Epoch 963/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1776 - accuracy: 0.5955 - val_loss: 0.4446 - val_accuracy: 0.5382\n",
      "Epoch 964/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1692 - accuracy: 0.5985 - val_loss: 0.4325 - val_accuracy: 0.5412\n",
      "Epoch 965/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1747 - accuracy: 0.5970 - val_loss: 0.4190 - val_accuracy: 0.5382\n",
      "Epoch 966/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1745 - accuracy: 0.5962 - val_loss: 0.4246 - val_accuracy: 0.5529\n",
      "Epoch 967/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1707 - accuracy: 0.5974 - val_loss: 0.4291 - val_accuracy: 0.5441\n",
      "Epoch 968/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1884 - accuracy: 0.5948 - val_loss: 0.4500 - val_accuracy: 0.5353\n",
      "Epoch 969/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1756 - accuracy: 0.5959 - val_loss: 0.4260 - val_accuracy: 0.5324\n",
      "Epoch 970/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1843 - accuracy: 0.5922 - val_loss: 0.4441 - val_accuracy: 0.5294\n",
      "Epoch 971/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1659 - accuracy: 0.5985 - val_loss: 0.4148 - val_accuracy: 0.5500\n",
      "Epoch 972/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1888 - accuracy: 0.5915 - val_loss: 0.4650 - val_accuracy: 0.5353\n",
      "Epoch 973/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1714 - accuracy: 0.5977 - val_loss: 0.4693 - val_accuracy: 0.5353\n",
      "Epoch 974/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1679 - accuracy: 0.5981 - val_loss: 0.4014 - val_accuracy: 0.5647\n",
      "Epoch 975/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2000 - accuracy: 0.5911 - val_loss: 0.4384 - val_accuracy: 0.5441\n",
      "Epoch 976/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.5974 - val_loss: 0.4458 - val_accuracy: 0.5294\n",
      "Epoch 977/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1726 - accuracy: 0.5955 - val_loss: 0.4231 - val_accuracy: 0.5559\n",
      "Epoch 978/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1815 - accuracy: 0.5944 - val_loss: 0.4438 - val_accuracy: 0.5324\n",
      "Epoch 979/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1676 - accuracy: 0.5970 - val_loss: 0.4197 - val_accuracy: 0.5500\n",
      "Epoch 980/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1654 - accuracy: 0.5988 - val_loss: 0.4161 - val_accuracy: 0.5235\n",
      "Epoch 981/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1791 - accuracy: 0.5944 - val_loss: 0.4051 - val_accuracy: 0.5471\n",
      "Epoch 982/1600\n",
      "680/680 [==============================] - 1s 943us/step - loss: 0.1738 - accuracy: 0.5970 - val_loss: 0.4183 - val_accuracy: 0.5471\n",
      "Epoch 983/1600\n",
      "680/680 [==============================] - 1s 930us/step - loss: 0.1882 - accuracy: 0.5918 - val_loss: 0.4003 - val_accuracy: 0.5412\n",
      "Epoch 984/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1701 - accuracy: 0.5985 - val_loss: 0.4282 - val_accuracy: 0.5353\n",
      "Epoch 985/1600\n",
      "680/680 [==============================] - 1s 965us/step - loss: 0.1847 - accuracy: 0.5948 - val_loss: 0.4531 - val_accuracy: 0.5294\n",
      "Epoch 986/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1741 - accuracy: 0.5966 - val_loss: 0.4804 - val_accuracy: 0.5265\n",
      "Epoch 987/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1776 - accuracy: 0.5970 - val_loss: 0.3986 - val_accuracy: 0.5441\n",
      "Epoch 988/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1744 - accuracy: 0.5959 - val_loss: 0.4389 - val_accuracy: 0.5471\n",
      "Epoch 989/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1718 - accuracy: 0.5977 - val_loss: 0.4457 - val_accuracy: 0.5353\n",
      "Epoch 990/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1774 - accuracy: 0.5977 - val_loss: 0.4179 - val_accuracy: 0.5500\n",
      "Epoch 991/1600\n",
      "680/680 [==============================] - 1s 866us/step - loss: 0.1762 - accuracy: 0.5988 - val_loss: 0.4035 - val_accuracy: 0.5529\n",
      "Epoch 992/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1670 - accuracy: 0.5999 - val_loss: 0.4308 - val_accuracy: 0.5353\n",
      "Epoch 993/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 879us/step - loss: 0.1816 - accuracy: 0.5948 - val_loss: 0.4065 - val_accuracy: 0.5559\n",
      "Epoch 994/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1984 - accuracy: 0.5904 - val_loss: 0.4340 - val_accuracy: 0.5441\n",
      "Epoch 995/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1765 - accuracy: 0.5951 - val_loss: 0.4116 - val_accuracy: 0.5441\n",
      "Epoch 996/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1885 - accuracy: 0.5933 - val_loss: 0.4698 - val_accuracy: 0.5206\n",
      "Epoch 997/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1729 - accuracy: 0.5977 - val_loss: 0.4226 - val_accuracy: 0.5471\n",
      "Epoch 998/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1679 - accuracy: 0.5977 - val_loss: 0.4201 - val_accuracy: 0.5441\n",
      "Epoch 999/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1815 - accuracy: 0.5948 - val_loss: 0.4013 - val_accuracy: 0.5500\n",
      "Epoch 1000/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1907 - accuracy: 0.5933 - val_loss: 0.4527 - val_accuracy: 0.5353\n",
      "Epoch 1001/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1743 - accuracy: 0.5974 - val_loss: 0.4523 - val_accuracy: 0.5353\n",
      "Epoch 1002/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1722 - accuracy: 0.5962 - val_loss: 0.3949 - val_accuracy: 0.5500\n",
      "Epoch 1003/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1805 - accuracy: 0.5974 - val_loss: 0.4187 - val_accuracy: 0.5500\n",
      "Epoch 1004/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1753 - accuracy: 0.5962 - val_loss: 0.4471 - val_accuracy: 0.5382\n",
      "Epoch 1005/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1599 - accuracy: 0.6007 - val_loss: 0.4314 - val_accuracy: 0.5441\n",
      "Epoch 1006/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1754 - accuracy: 0.5955 - val_loss: 0.5028 - val_accuracy: 0.5353\n",
      "Epoch 1007/1600\n",
      "680/680 [==============================] - 1s 862us/step - loss: 0.1905 - accuracy: 0.5933 - val_loss: 0.4630 - val_accuracy: 0.5176\n",
      "Epoch 1008/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1861 - accuracy: 0.5933 - val_loss: 0.4309 - val_accuracy: 0.5529\n",
      "Epoch 1009/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1736 - accuracy: 0.5962 - val_loss: 0.4405 - val_accuracy: 0.5647\n",
      "Epoch 1010/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1847 - accuracy: 0.5937 - val_loss: 0.4321 - val_accuracy: 0.5382\n",
      "Epoch 1011/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1638 - accuracy: 0.5985 - val_loss: 0.4265 - val_accuracy: 0.5471\n",
      "Epoch 1012/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1819 - accuracy: 0.5959 - val_loss: 0.4597 - val_accuracy: 0.5324\n",
      "Epoch 1013/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1766 - accuracy: 0.5937 - val_loss: 0.4315 - val_accuracy: 0.5324\n",
      "Epoch 1014/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1699 - accuracy: 0.5977 - val_loss: 0.4282 - val_accuracy: 0.5471\n",
      "Epoch 1015/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1657 - accuracy: 0.5985 - val_loss: 0.4116 - val_accuracy: 0.5529\n",
      "Epoch 1016/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1740 - accuracy: 0.5985 - val_loss: 0.4323 - val_accuracy: 0.5471\n",
      "Epoch 1017/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1855 - accuracy: 0.5959 - val_loss: 0.3811 - val_accuracy: 0.5588\n",
      "Epoch 1018/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1722 - accuracy: 0.5970 - val_loss: 0.4561 - val_accuracy: 0.5382\n",
      "Epoch 1019/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1729 - accuracy: 0.5974 - val_loss: 0.4487 - val_accuracy: 0.5324\n",
      "Epoch 1020/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.1856 - accuracy: 0.5933 - val_loss: 0.4395 - val_accuracy: 0.5471\n",
      "Epoch 1021/1600\n",
      "680/680 [==============================] - 1s 907us/step - loss: 0.1683 - accuracy: 0.5992 - val_loss: 0.4462 - val_accuracy: 0.5382\n",
      "Epoch 1022/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1742 - accuracy: 0.5962 - val_loss: 0.4095 - val_accuracy: 0.5500\n",
      "Epoch 1023/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.1675 - accuracy: 0.5999 - val_loss: 0.4408 - val_accuracy: 0.5353\n",
      "Epoch 1024/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1640 - accuracy: 0.5985 - val_loss: 0.4211 - val_accuracy: 0.5471\n",
      "Epoch 1025/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1707 - accuracy: 0.5970 - val_loss: 0.4111 - val_accuracy: 0.5618\n",
      "Epoch 1026/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.2000 - accuracy: 0.5911 - val_loss: 0.4291 - val_accuracy: 0.5500\n",
      "Epoch 1027/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1795 - accuracy: 0.5959 - val_loss: 0.4217 - val_accuracy: 0.5382\n",
      "Epoch 1028/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1827 - accuracy: 0.5970 - val_loss: 0.4289 - val_accuracy: 0.5382\n",
      "Epoch 1029/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1892 - accuracy: 0.5948 - val_loss: 0.4150 - val_accuracy: 0.5441\n",
      "Epoch 1030/1600\n",
      "680/680 [==============================] - 1s 863us/step - loss: 0.1700 - accuracy: 0.5977 - val_loss: 0.4723 - val_accuracy: 0.5235\n",
      "Epoch 1031/1600\n",
      "680/680 [==============================] - 1s 904us/step - loss: 0.1933 - accuracy: 0.5929 - val_loss: 0.4181 - val_accuracy: 0.5382\n",
      "Epoch 1032/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1888 - accuracy: 0.5940 - val_loss: 0.4215 - val_accuracy: 0.5235\n",
      "Epoch 1033/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1826 - accuracy: 0.5937 - val_loss: 0.4022 - val_accuracy: 0.5500\n",
      "Epoch 1034/1600\n",
      "680/680 [==============================] - 1s 866us/step - loss: 0.1695 - accuracy: 0.5974 - val_loss: 0.4042 - val_accuracy: 0.5471\n",
      "Epoch 1035/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1773 - accuracy: 0.5962 - val_loss: 0.4275 - val_accuracy: 0.5441\n",
      "Epoch 1036/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1658 - accuracy: 0.5977 - val_loss: 0.4464 - val_accuracy: 0.5412\n",
      "Epoch 1037/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1945 - accuracy: 0.5933 - val_loss: 0.4103 - val_accuracy: 0.5412\n",
      "Epoch 1038/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1648 - accuracy: 0.5988 - val_loss: 0.3692 - val_accuracy: 0.5559\n",
      "Epoch 1039/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1862 - accuracy: 0.5955 - val_loss: 0.4195 - val_accuracy: 0.5412\n",
      "Epoch 1040/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1808 - accuracy: 0.5955 - val_loss: 0.4359 - val_accuracy: 0.5353\n",
      "Epoch 1041/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1750 - accuracy: 0.5955 - val_loss: 0.4498 - val_accuracy: 0.5353\n",
      "Epoch 1042/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1823 - accuracy: 0.5944 - val_loss: 0.4117 - val_accuracy: 0.5441\n",
      "Epoch 1043/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1790 - accuracy: 0.5970 - val_loss: 0.4471 - val_accuracy: 0.5441\n",
      "Epoch 1044/1600\n",
      "680/680 [==============================] - 1s 868us/step - loss: 0.1864 - accuracy: 0.5944 - val_loss: 0.3881 - val_accuracy: 0.5500\n",
      "Epoch 1045/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1796 - accuracy: 0.5974 - val_loss: 0.4305 - val_accuracy: 0.5412\n",
      "Epoch 1046/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1726 - accuracy: 0.5970 - val_loss: 0.4342 - val_accuracy: 0.5324\n",
      "Epoch 1047/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1777 - accuracy: 0.5940 - val_loss: 0.4800 - val_accuracy: 0.5324\n",
      "Epoch 1048/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 922us/step - loss: 0.1800 - accuracy: 0.5959 - val_loss: 0.3930 - val_accuracy: 0.5441\n",
      "Epoch 1049/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1793 - accuracy: 0.5962 - val_loss: 0.4259 - val_accuracy: 0.5382\n",
      "Epoch 1050/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1796 - accuracy: 0.5951 - val_loss: 0.4211 - val_accuracy: 0.5353\n",
      "Epoch 1051/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1631 - accuracy: 0.5974 - val_loss: 0.4188 - val_accuracy: 0.5265\n",
      "Epoch 1052/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1738 - accuracy: 0.5966 - val_loss: 0.4199 - val_accuracy: 0.5529\n",
      "Epoch 1053/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1762 - accuracy: 0.5937 - val_loss: 0.4757 - val_accuracy: 0.5294\n",
      "Epoch 1054/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1753 - accuracy: 0.5966 - val_loss: 0.4136 - val_accuracy: 0.5500\n",
      "Epoch 1055/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1728 - accuracy: 0.5955 - val_loss: 0.4297 - val_accuracy: 0.5618\n",
      "Epoch 1056/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1967 - accuracy: 0.5922 - val_loss: 0.4047 - val_accuracy: 0.5471\n",
      "Epoch 1057/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1633 - accuracy: 0.5992 - val_loss: 0.4319 - val_accuracy: 0.5382\n",
      "Epoch 1058/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1905 - accuracy: 0.5959 - val_loss: 0.4126 - val_accuracy: 0.5529\n",
      "Epoch 1059/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1613 - accuracy: 0.5992 - val_loss: 0.4277 - val_accuracy: 0.5412\n",
      "Epoch 1060/1600\n",
      "680/680 [==============================] - 1s 906us/step - loss: 0.1741 - accuracy: 0.5966 - val_loss: 0.4409 - val_accuracy: 0.5382\n",
      "Epoch 1061/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1870 - accuracy: 0.5948 - val_loss: 0.4293 - val_accuracy: 0.5382\n",
      "Epoch 1062/1600\n",
      "680/680 [==============================] - 1s 868us/step - loss: 0.1611 - accuracy: 0.5985 - val_loss: 0.4395 - val_accuracy: 0.5294\n",
      "Epoch 1063/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1855 - accuracy: 0.5951 - val_loss: 0.4124 - val_accuracy: 0.5471\n",
      "Epoch 1064/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1784 - accuracy: 0.5974 - val_loss: 0.4021 - val_accuracy: 0.5500\n",
      "Epoch 1065/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1799 - accuracy: 0.5944 - val_loss: 0.4032 - val_accuracy: 0.5412\n",
      "Epoch 1066/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1902 - accuracy: 0.5955 - val_loss: 0.3964 - val_accuracy: 0.5559\n",
      "Epoch 1067/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1752 - accuracy: 0.5966 - val_loss: 0.4107 - val_accuracy: 0.5382\n",
      "Epoch 1068/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1850 - accuracy: 0.5959 - val_loss: 0.3968 - val_accuracy: 0.5382\n",
      "Epoch 1069/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1886 - accuracy: 0.5929 - val_loss: 0.4552 - val_accuracy: 0.5206\n",
      "Epoch 1070/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1906 - accuracy: 0.5922 - val_loss: 0.4663 - val_accuracy: 0.5265\n",
      "Epoch 1071/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1623 - accuracy: 0.5974 - val_loss: 0.3908 - val_accuracy: 0.5500\n",
      "Epoch 1072/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1796 - accuracy: 0.5951 - val_loss: 0.4362 - val_accuracy: 0.5353\n",
      "Epoch 1073/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1705 - accuracy: 0.5951 - val_loss: 0.4023 - val_accuracy: 0.5441\n",
      "Epoch 1074/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1700 - accuracy: 0.5992 - val_loss: 0.4209 - val_accuracy: 0.5441\n",
      "Epoch 1075/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1896 - accuracy: 0.5962 - val_loss: 0.3951 - val_accuracy: 0.5529\n",
      "Epoch 1076/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1784 - accuracy: 0.5970 - val_loss: 0.4151 - val_accuracy: 0.5529\n",
      "Epoch 1077/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1733 - accuracy: 0.5970 - val_loss: 0.4217 - val_accuracy: 0.5618\n",
      "Epoch 1078/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1830 - accuracy: 0.5951 - val_loss: 0.4486 - val_accuracy: 0.5412\n",
      "Epoch 1079/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1767 - accuracy: 0.5966 - val_loss: 0.4518 - val_accuracy: 0.5324\n",
      "Epoch 1080/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1719 - accuracy: 0.5985 - val_loss: 0.4417 - val_accuracy: 0.5500\n",
      "Epoch 1081/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1747 - accuracy: 0.5959 - val_loss: 0.4284 - val_accuracy: 0.5235\n",
      "Epoch 1082/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1828 - accuracy: 0.5962 - val_loss: 0.4478 - val_accuracy: 0.5412\n",
      "Epoch 1083/1600\n",
      "680/680 [==============================] - 1s 902us/step - loss: 0.1788 - accuracy: 0.5929 - val_loss: 0.4260 - val_accuracy: 0.5529\n",
      "Epoch 1084/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1731 - accuracy: 0.5966 - val_loss: 0.4164 - val_accuracy: 0.5412\n",
      "Epoch 1085/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1737 - accuracy: 0.5940 - val_loss: 0.4253 - val_accuracy: 0.5441\n",
      "Epoch 1086/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1764 - accuracy: 0.5962 - val_loss: 0.4445 - val_accuracy: 0.5353\n",
      "Epoch 1087/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1793 - accuracy: 0.5962 - val_loss: 0.4736 - val_accuracy: 0.5265\n",
      "Epoch 1088/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.1679 - accuracy: 0.5988 - val_loss: 0.4434 - val_accuracy: 0.5324\n",
      "Epoch 1089/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1607 - accuracy: 0.5988 - val_loss: 0.4208 - val_accuracy: 0.5294\n",
      "Epoch 1090/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1815 - accuracy: 0.5951 - val_loss: 0.4847 - val_accuracy: 0.5353\n",
      "Epoch 1091/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1852 - accuracy: 0.5937 - val_loss: 0.4033 - val_accuracy: 0.5471\n",
      "Epoch 1092/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1735 - accuracy: 0.5959 - val_loss: 0.4261 - val_accuracy: 0.5500\n",
      "Epoch 1093/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1799 - accuracy: 0.5959 - val_loss: 0.4501 - val_accuracy: 0.5412\n",
      "Epoch 1094/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1887 - accuracy: 0.5933 - val_loss: 0.4234 - val_accuracy: 0.5353\n",
      "Epoch 1095/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1666 - accuracy: 0.5996 - val_loss: 0.4310 - val_accuracy: 0.5441\n",
      "Epoch 1096/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1921 - accuracy: 0.5944 - val_loss: 0.4191 - val_accuracy: 0.5382\n",
      "Epoch 1097/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1801 - accuracy: 0.5944 - val_loss: 0.4354 - val_accuracy: 0.5382\n",
      "Epoch 1098/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1761 - accuracy: 0.5955 - val_loss: 0.4004 - val_accuracy: 0.5559\n",
      "Epoch 1099/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1730 - accuracy: 0.5977 - val_loss: 0.4511 - val_accuracy: 0.5382\n",
      "Epoch 1100/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1692 - accuracy: 0.5966 - val_loss: 0.4781 - val_accuracy: 0.5647\n",
      "Epoch 1101/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.1707 - accuracy: 0.5981 - val_loss: 0.4275 - val_accuracy: 0.5471\n",
      "Epoch 1102/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1681 - accuracy: 0.5992 - val_loss: 0.4506 - val_accuracy: 0.5441\n",
      "Epoch 1103/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 886us/step - loss: 0.1896 - accuracy: 0.5933 - val_loss: 0.4405 - val_accuracy: 0.5382\n",
      "Epoch 1104/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1705 - accuracy: 0.5955 - val_loss: 0.3942 - val_accuracy: 0.5559\n",
      "Epoch 1105/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1686 - accuracy: 0.5977 - val_loss: 0.4290 - val_accuracy: 0.5471\n",
      "Epoch 1106/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1731 - accuracy: 0.5988 - val_loss: 0.4368 - val_accuracy: 0.5441\n",
      "Epoch 1107/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1669 - accuracy: 0.5970 - val_loss: 0.4490 - val_accuracy: 0.5353\n",
      "Epoch 1108/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1791 - accuracy: 0.5977 - val_loss: 0.4146 - val_accuracy: 0.5529\n",
      "Epoch 1109/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.2028 - accuracy: 0.5922 - val_loss: 0.4133 - val_accuracy: 0.5559\n",
      "Epoch 1110/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1714 - accuracy: 0.5985 - val_loss: 0.4468 - val_accuracy: 0.5412\n",
      "Epoch 1111/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1700 - accuracy: 0.5985 - val_loss: 0.4418 - val_accuracy: 0.5294\n",
      "Epoch 1112/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1721 - accuracy: 0.5974 - val_loss: 0.4454 - val_accuracy: 0.5353\n",
      "Epoch 1113/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1769 - accuracy: 0.5996 - val_loss: 0.4439 - val_accuracy: 0.5353\n",
      "Epoch 1114/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1732 - accuracy: 0.5988 - val_loss: 0.4532 - val_accuracy: 0.5294\n",
      "Epoch 1115/1600\n",
      "680/680 [==============================] - 1s 864us/step - loss: 0.1801 - accuracy: 0.5955 - val_loss: 0.4607 - val_accuracy: 0.5353\n",
      "Epoch 1116/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.1753 - accuracy: 0.5992 - val_loss: 0.4549 - val_accuracy: 0.5353\n",
      "Epoch 1117/1600\n",
      "680/680 [==============================] - 1s 857us/step - loss: 0.1877 - accuracy: 0.5922 - val_loss: 0.4391 - val_accuracy: 0.5382\n",
      "Epoch 1118/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1723 - accuracy: 0.5981 - val_loss: 0.4046 - val_accuracy: 0.5382\n",
      "Epoch 1119/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.2008 - accuracy: 0.5900 - val_loss: 0.4414 - val_accuracy: 0.5412\n",
      "Epoch 1120/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1745 - accuracy: 0.5962 - val_loss: 0.4226 - val_accuracy: 0.5471\n",
      "Epoch 1121/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1734 - accuracy: 0.5977 - val_loss: 0.4631 - val_accuracy: 0.5294\n",
      "Epoch 1122/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1681 - accuracy: 0.5962 - val_loss: 0.4204 - val_accuracy: 0.5500\n",
      "Epoch 1123/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1789 - accuracy: 0.5937 - val_loss: 0.4434 - val_accuracy: 0.5206\n",
      "Epoch 1124/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1780 - accuracy: 0.5974 - val_loss: 0.4422 - val_accuracy: 0.5294\n",
      "Epoch 1125/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1758 - accuracy: 0.5962 - val_loss: 0.4266 - val_accuracy: 0.5441\n",
      "Epoch 1126/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1762 - accuracy: 0.5959 - val_loss: 0.4329 - val_accuracy: 0.5324\n",
      "Epoch 1127/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1818 - accuracy: 0.5974 - val_loss: 0.4520 - val_accuracy: 0.5588\n",
      "Epoch 1128/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1837 - accuracy: 0.5962 - val_loss: 0.4239 - val_accuracy: 0.5471\n",
      "Epoch 1129/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1796 - accuracy: 0.5970 - val_loss: 0.4478 - val_accuracy: 0.5324\n",
      "Epoch 1130/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1863 - accuracy: 0.5951 - val_loss: 0.4002 - val_accuracy: 0.5441\n",
      "Epoch 1131/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1802 - accuracy: 0.5955 - val_loss: 0.4164 - val_accuracy: 0.5382\n",
      "Epoch 1132/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1785 - accuracy: 0.5959 - val_loss: 0.4127 - val_accuracy: 0.5353\n",
      "Epoch 1133/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1798 - accuracy: 0.5999 - val_loss: 0.4715 - val_accuracy: 0.5382\n",
      "Epoch 1134/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1779 - accuracy: 0.5955 - val_loss: 0.4458 - val_accuracy: 0.5353\n",
      "Epoch 1135/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1714 - accuracy: 0.5977 - val_loss: 0.4546 - val_accuracy: 0.5441\n",
      "Epoch 1136/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1741 - accuracy: 0.5970 - val_loss: 0.4406 - val_accuracy: 0.5441\n",
      "Epoch 1137/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1943 - accuracy: 0.5915 - val_loss: 0.4410 - val_accuracy: 0.5382\n",
      "Epoch 1138/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1793 - accuracy: 0.5944 - val_loss: 0.4122 - val_accuracy: 0.5500\n",
      "Epoch 1139/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1735 - accuracy: 0.5977 - val_loss: 0.4438 - val_accuracy: 0.5588\n",
      "Epoch 1140/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1718 - accuracy: 0.5974 - val_loss: 0.4423 - val_accuracy: 0.5441\n",
      "Epoch 1141/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.1684 - accuracy: 0.5985 - val_loss: 0.4268 - val_accuracy: 0.5500\n",
      "Epoch 1142/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1651 - accuracy: 0.5996 - val_loss: 0.4711 - val_accuracy: 0.5382\n",
      "Epoch 1143/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1790 - accuracy: 0.5962 - val_loss: 0.4042 - val_accuracy: 0.5471\n",
      "Epoch 1144/1600\n",
      "680/680 [==============================] - 1s 941us/step - loss: 0.1884 - accuracy: 0.5959 - val_loss: 0.4455 - val_accuracy: 0.5441\n",
      "Epoch 1145/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.1696 - accuracy: 0.5988 - val_loss: 0.4616 - val_accuracy: 0.5382\n",
      "Epoch 1146/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1823 - accuracy: 0.5940 - val_loss: 0.4443 - val_accuracy: 0.5441\n",
      "Epoch 1147/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1803 - accuracy: 0.5959 - val_loss: 0.4480 - val_accuracy: 0.5441\n",
      "Epoch 1148/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1853 - accuracy: 0.5951 - val_loss: 0.4319 - val_accuracy: 0.5441\n",
      "Epoch 1149/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.1804 - accuracy: 0.5951 - val_loss: 0.4331 - val_accuracy: 0.5382\n",
      "Epoch 1150/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1862 - accuracy: 0.5929 - val_loss: 0.4414 - val_accuracy: 0.5353\n",
      "Epoch 1151/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1687 - accuracy: 0.5981 - val_loss: 0.4581 - val_accuracy: 0.5382\n",
      "Epoch 1152/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1713 - accuracy: 0.5959 - val_loss: 0.4136 - val_accuracy: 0.5500\n",
      "Epoch 1153/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1845 - accuracy: 0.5962 - val_loss: 0.4145 - val_accuracy: 0.5471\n",
      "Epoch 1154/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1699 - accuracy: 0.5959 - val_loss: 0.4379 - val_accuracy: 0.5353\n",
      "Epoch 1155/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1765 - accuracy: 0.5962 - val_loss: 0.4338 - val_accuracy: 0.5294\n",
      "Epoch 1156/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1820 - accuracy: 0.5937 - val_loss: 0.4096 - val_accuracy: 0.5382\n",
      "Epoch 1157/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1692 - accuracy: 0.5970 - val_loss: 0.4421 - val_accuracy: 0.5412\n",
      "Epoch 1158/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 877us/step - loss: 0.1824 - accuracy: 0.5962 - val_loss: 0.4718 - val_accuracy: 0.5353\n",
      "Epoch 1159/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1725 - accuracy: 0.5940 - val_loss: 0.4224 - val_accuracy: 0.5500\n",
      "Epoch 1160/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1815 - accuracy: 0.5944 - val_loss: 0.4624 - val_accuracy: 0.5265\n",
      "Epoch 1161/1600\n",
      "680/680 [==============================] - 1s 943us/step - loss: 0.1875 - accuracy: 0.5966 - val_loss: 0.4449 - val_accuracy: 0.5382\n",
      "Epoch 1162/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1667 - accuracy: 0.5988 - val_loss: 0.4340 - val_accuracy: 0.5353\n",
      "Epoch 1163/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1626 - accuracy: 0.5988 - val_loss: 0.4429 - val_accuracy: 0.5441\n",
      "Epoch 1164/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1750 - accuracy: 0.5974 - val_loss: 0.4348 - val_accuracy: 0.5471\n",
      "Epoch 1165/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1874 - accuracy: 0.5926 - val_loss: 0.4373 - val_accuracy: 0.5412\n",
      "Epoch 1166/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1732 - accuracy: 0.5970 - val_loss: 0.4221 - val_accuracy: 0.5382\n",
      "Epoch 1167/1600\n",
      "680/680 [==============================] - 1s 907us/step - loss: 0.1677 - accuracy: 0.5981 - val_loss: 0.4281 - val_accuracy: 0.5412\n",
      "Epoch 1168/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1910 - accuracy: 0.5937 - val_loss: 0.4530 - val_accuracy: 0.5294\n",
      "Epoch 1169/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.2021 - accuracy: 0.5896 - val_loss: 0.4082 - val_accuracy: 0.5265\n",
      "Epoch 1170/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1885 - accuracy: 0.5911 - val_loss: 0.4514 - val_accuracy: 0.5382\n",
      "Epoch 1171/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1759 - accuracy: 0.5955 - val_loss: 0.4567 - val_accuracy: 0.5500\n",
      "Epoch 1172/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1634 - accuracy: 0.5988 - val_loss: 0.4394 - val_accuracy: 0.5382\n",
      "Epoch 1173/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.1661 - accuracy: 0.6014 - val_loss: 0.4506 - val_accuracy: 0.5441\n",
      "Epoch 1174/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1670 - accuracy: 0.5999 - val_loss: 0.4445 - val_accuracy: 0.5500\n",
      "Epoch 1175/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1834 - accuracy: 0.5929 - val_loss: 0.4520 - val_accuracy: 0.5441\n",
      "Epoch 1176/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1845 - accuracy: 0.5933 - val_loss: 0.4479 - val_accuracy: 0.5412\n",
      "Epoch 1177/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1739 - accuracy: 0.5951 - val_loss: 0.4216 - val_accuracy: 0.5500\n",
      "Epoch 1178/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1670 - accuracy: 0.5996 - val_loss: 0.4530 - val_accuracy: 0.5412\n",
      "Epoch 1179/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1821 - accuracy: 0.5966 - val_loss: 0.4337 - val_accuracy: 0.5471\n",
      "Epoch 1180/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1650 - accuracy: 0.5966 - val_loss: 0.4705 - val_accuracy: 0.5382\n",
      "Epoch 1181/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1743 - accuracy: 0.5959 - val_loss: 0.4624 - val_accuracy: 0.5353\n",
      "Epoch 1182/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1695 - accuracy: 0.5977 - val_loss: 0.4643 - val_accuracy: 0.5353\n",
      "Epoch 1183/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1762 - accuracy: 0.5962 - val_loss: 0.4359 - val_accuracy: 0.5294\n",
      "Epoch 1184/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1623 - accuracy: 0.5985 - val_loss: 0.4348 - val_accuracy: 0.5529\n",
      "Epoch 1185/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1652 - accuracy: 0.5985 - val_loss: 0.4136 - val_accuracy: 0.5500\n",
      "Epoch 1186/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1773 - accuracy: 0.5966 - val_loss: 0.4784 - val_accuracy: 0.5265\n",
      "Epoch 1187/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1766 - accuracy: 0.5962 - val_loss: 0.4358 - val_accuracy: 0.5382\n",
      "Epoch 1188/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1697 - accuracy: 0.5966 - val_loss: 0.4247 - val_accuracy: 0.5471\n",
      "Epoch 1189/1600\n",
      "680/680 [==============================] - 1s 1000us/step - loss: 0.1784 - accuracy: 0.5966 - val_loss: 0.4467 - val_accuracy: 0.5588\n",
      "Epoch 1190/1600\n",
      "680/680 [==============================] - 1s 967us/step - loss: 0.1719 - accuracy: 0.5977 - val_loss: 0.4237 - val_accuracy: 0.5471\n",
      "Epoch 1191/1600\n",
      "680/680 [==============================] - 1s 998us/step - loss: 0.1632 - accuracy: 0.5992 - val_loss: 0.4473 - val_accuracy: 0.5471\n",
      "Epoch 1192/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.1820 - accuracy: 0.5951 - val_loss: 0.4413 - val_accuracy: 0.5441\n",
      "Epoch 1193/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1745 - accuracy: 0.5948 - val_loss: 0.4239 - val_accuracy: 0.5441\n",
      "Epoch 1194/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1834 - accuracy: 0.5948 - val_loss: 0.4319 - val_accuracy: 0.5500\n",
      "Epoch 1195/1600\n",
      "680/680 [==============================] - 1s 938us/step - loss: 0.1675 - accuracy: 0.5996 - val_loss: 0.4427 - val_accuracy: 0.5382\n",
      "Epoch 1196/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1648 - accuracy: 0.5992 - val_loss: 0.4297 - val_accuracy: 0.5382\n",
      "Epoch 1197/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1745 - accuracy: 0.5974 - val_loss: 0.4817 - val_accuracy: 0.5265\n",
      "Epoch 1198/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1823 - accuracy: 0.5929 - val_loss: 0.4587 - val_accuracy: 0.5382\n",
      "Epoch 1199/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1659 - accuracy: 0.6003 - val_loss: 0.4330 - val_accuracy: 0.5294\n",
      "Epoch 1200/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1776 - accuracy: 0.5940 - val_loss: 0.4450 - val_accuracy: 0.5382\n",
      "Epoch 1201/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1732 - accuracy: 0.5985 - val_loss: 0.4121 - val_accuracy: 0.5412\n",
      "Epoch 1202/1600\n",
      "680/680 [==============================] - 1s 862us/step - loss: 0.1791 - accuracy: 0.5951 - val_loss: 0.4033 - val_accuracy: 0.5500\n",
      "Epoch 1203/1600\n",
      "680/680 [==============================] - 1s 901us/step - loss: 0.1739 - accuracy: 0.5948 - val_loss: 0.4233 - val_accuracy: 0.5324\n",
      "Epoch 1204/1600\n",
      "680/680 [==============================] - 1s 854us/step - loss: 0.1753 - accuracy: 0.5966 - val_loss: 0.4960 - val_accuracy: 0.5324\n",
      "Epoch 1205/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1777 - accuracy: 0.5959 - val_loss: 0.4457 - val_accuracy: 0.5382\n",
      "Epoch 1206/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1714 - accuracy: 0.5985 - val_loss: 0.4336 - val_accuracy: 0.5588\n",
      "Epoch 1207/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1735 - accuracy: 0.5974 - val_loss: 0.4870 - val_accuracy: 0.5382\n",
      "Epoch 1208/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1960 - accuracy: 0.5951 - val_loss: 0.4530 - val_accuracy: 0.5294\n",
      "Epoch 1209/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.1726 - accuracy: 0.5966 - val_loss: 0.4338 - val_accuracy: 0.5412\n",
      "Epoch 1210/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1697 - accuracy: 0.5970 - val_loss: 0.4648 - val_accuracy: 0.5176\n",
      "Epoch 1211/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1841 - accuracy: 0.5926 - val_loss: 0.4649 - val_accuracy: 0.5412\n",
      "Epoch 1212/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1752 - accuracy: 0.5955 - val_loss: 0.4454 - val_accuracy: 0.5353\n",
      "Epoch 1213/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 921us/step - loss: 0.1730 - accuracy: 0.5974 - val_loss: 0.4547 - val_accuracy: 0.5324\n",
      "Epoch 1214/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1762 - accuracy: 0.5948 - val_loss: 0.4301 - val_accuracy: 0.5471\n",
      "Epoch 1215/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1711 - accuracy: 0.5988 - val_loss: 0.4514 - val_accuracy: 0.5412\n",
      "Epoch 1216/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1821 - accuracy: 0.5937 - val_loss: 0.4296 - val_accuracy: 0.5324\n",
      "Epoch 1217/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1666 - accuracy: 0.5962 - val_loss: 0.4323 - val_accuracy: 0.5412\n",
      "Epoch 1218/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1794 - accuracy: 0.5959 - val_loss: 0.4317 - val_accuracy: 0.5471\n",
      "Epoch 1219/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1761 - accuracy: 0.5977 - val_loss: 0.4679 - val_accuracy: 0.5441\n",
      "Epoch 1220/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1620 - accuracy: 0.6003 - val_loss: 0.4643 - val_accuracy: 0.5588\n",
      "Epoch 1221/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1690 - accuracy: 0.5977 - val_loss: 0.4410 - val_accuracy: 0.5382\n",
      "Epoch 1222/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1968 - accuracy: 0.5933 - val_loss: 0.4090 - val_accuracy: 0.5529\n",
      "Epoch 1223/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1702 - accuracy: 0.5962 - val_loss: 0.4206 - val_accuracy: 0.5412\n",
      "Epoch 1224/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1731 - accuracy: 0.5959 - val_loss: 0.4319 - val_accuracy: 0.5588\n",
      "Epoch 1225/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1810 - accuracy: 0.5951 - val_loss: 0.4451 - val_accuracy: 0.5471\n",
      "Epoch 1226/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1864 - accuracy: 0.5944 - val_loss: 0.4364 - val_accuracy: 0.5382\n",
      "Epoch 1227/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1823 - accuracy: 0.5926 - val_loss: 0.4330 - val_accuracy: 0.5441\n",
      "Epoch 1228/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1626 - accuracy: 0.5981 - val_loss: 0.4243 - val_accuracy: 0.5353\n",
      "Epoch 1229/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1753 - accuracy: 0.5951 - val_loss: 0.4527 - val_accuracy: 0.5441\n",
      "Epoch 1230/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1652 - accuracy: 0.5996 - val_loss: 0.4130 - val_accuracy: 0.5441\n",
      "Epoch 1231/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1893 - accuracy: 0.5915 - val_loss: 0.4306 - val_accuracy: 0.5441\n",
      "Epoch 1232/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1779 - accuracy: 0.5962 - val_loss: 0.4313 - val_accuracy: 0.5294\n",
      "Epoch 1233/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1743 - accuracy: 0.5955 - val_loss: 0.4739 - val_accuracy: 0.5324\n",
      "Epoch 1234/1600\n",
      "680/680 [==============================] - 1s 904us/step - loss: 0.1902 - accuracy: 0.5907 - val_loss: 0.4312 - val_accuracy: 0.5412\n",
      "Epoch 1235/1600\n",
      "680/680 [==============================] - 1s 930us/step - loss: 0.1730 - accuracy: 0.5977 - val_loss: 0.4736 - val_accuracy: 0.5324\n",
      "Epoch 1236/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1666 - accuracy: 0.5962 - val_loss: 0.4951 - val_accuracy: 0.5294\n",
      "Epoch 1237/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1763 - accuracy: 0.5974 - val_loss: 0.4097 - val_accuracy: 0.5412\n",
      "Epoch 1238/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1764 - accuracy: 0.5955 - val_loss: 0.4445 - val_accuracy: 0.5412\n",
      "Epoch 1239/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1783 - accuracy: 0.5951 - val_loss: 0.4575 - val_accuracy: 0.5500\n",
      "Epoch 1240/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1708 - accuracy: 0.5970 - val_loss: 0.4551 - val_accuracy: 0.5441\n",
      "Epoch 1241/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.1691 - accuracy: 0.5981 - val_loss: 0.4190 - val_accuracy: 0.5353\n",
      "Epoch 1242/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1738 - accuracy: 0.5970 - val_loss: 0.4614 - val_accuracy: 0.5324\n",
      "Epoch 1243/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1867 - accuracy: 0.5922 - val_loss: 0.4591 - val_accuracy: 0.5353\n",
      "Epoch 1244/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1672 - accuracy: 0.5999 - val_loss: 0.4399 - val_accuracy: 0.5324\n",
      "Epoch 1245/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1676 - accuracy: 0.5974 - val_loss: 0.4460 - val_accuracy: 0.5353\n",
      "Epoch 1246/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1821 - accuracy: 0.5937 - val_loss: 0.4721 - val_accuracy: 0.5412\n",
      "Epoch 1247/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1809 - accuracy: 0.5955 - val_loss: 0.4707 - val_accuracy: 0.5235\n",
      "Epoch 1248/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1762 - accuracy: 0.5940 - val_loss: 0.4423 - val_accuracy: 0.5353\n",
      "Epoch 1249/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1768 - accuracy: 0.5951 - val_loss: 0.4532 - val_accuracy: 0.5382\n",
      "Epoch 1250/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1799 - accuracy: 0.5948 - val_loss: 0.4633 - val_accuracy: 0.5441\n",
      "Epoch 1251/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1670 - accuracy: 0.5974 - val_loss: 0.4443 - val_accuracy: 0.5382\n",
      "Epoch 1252/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1737 - accuracy: 0.5966 - val_loss: 0.4209 - val_accuracy: 0.5559\n",
      "Epoch 1253/1600\n",
      "680/680 [==============================] - 1s 864us/step - loss: 0.1641 - accuracy: 0.5988 - val_loss: 0.4568 - val_accuracy: 0.5412\n",
      "Epoch 1254/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1641 - accuracy: 0.5981 - val_loss: 0.4503 - val_accuracy: 0.5382\n",
      "Epoch 1255/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1877 - accuracy: 0.5951 - val_loss: 0.4567 - val_accuracy: 0.5412\n",
      "Epoch 1256/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1762 - accuracy: 0.5955 - val_loss: 0.4184 - val_accuracy: 0.5441\n",
      "Epoch 1257/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1630 - accuracy: 0.6003 - val_loss: 0.4562 - val_accuracy: 0.5324\n",
      "Epoch 1258/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1884 - accuracy: 0.5944 - val_loss: 0.4448 - val_accuracy: 0.5500\n",
      "Epoch 1259/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1696 - accuracy: 0.5959 - val_loss: 0.4697 - val_accuracy: 0.5441\n",
      "Epoch 1260/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1769 - accuracy: 0.5974 - val_loss: 0.4315 - val_accuracy: 0.5441\n",
      "Epoch 1261/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1808 - accuracy: 0.5955 - val_loss: 0.4350 - val_accuracy: 0.5382\n",
      "Epoch 1262/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1722 - accuracy: 0.5951 - val_loss: 0.4227 - val_accuracy: 0.5559\n",
      "Epoch 1263/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1720 - accuracy: 0.5970 - val_loss: 0.4238 - val_accuracy: 0.5471\n",
      "Epoch 1264/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1718 - accuracy: 0.5966 - val_loss: 0.4432 - val_accuracy: 0.5500\n",
      "Epoch 1265/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1754 - accuracy: 0.5959 - val_loss: 0.4503 - val_accuracy: 0.5500\n",
      "Epoch 1266/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1594 - accuracy: 0.5996 - val_loss: 0.4407 - val_accuracy: 0.5471\n",
      "Epoch 1267/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1937 - accuracy: 0.5918 - val_loss: 0.4423 - val_accuracy: 0.5471\n",
      "Epoch 1268/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 932us/step - loss: 0.1878 - accuracy: 0.5966 - val_loss: 0.4282 - val_accuracy: 0.5382\n",
      "Epoch 1269/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1566 - accuracy: 0.6010 - val_loss: 0.4352 - val_accuracy: 0.5382\n",
      "Epoch 1270/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1813 - accuracy: 0.5933 - val_loss: 0.4754 - val_accuracy: 0.5294\n",
      "Epoch 1271/1600\n",
      "680/680 [==============================] - 1s 907us/step - loss: 0.1764 - accuracy: 0.5962 - val_loss: 0.4575 - val_accuracy: 0.5265\n",
      "Epoch 1272/1600\n",
      "680/680 [==============================] - 1s 973us/step - loss: 0.1797 - accuracy: 0.5929 - val_loss: 0.4489 - val_accuracy: 0.5412\n",
      "Epoch 1273/1600\n",
      "680/680 [==============================] - 1s 980us/step - loss: 0.1652 - accuracy: 0.5988 - val_loss: 0.4232 - val_accuracy: 0.5412\n",
      "Epoch 1274/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1907 - accuracy: 0.5904 - val_loss: 0.5026 - val_accuracy: 0.5176\n",
      "Epoch 1275/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1732 - accuracy: 0.5966 - val_loss: 0.4440 - val_accuracy: 0.5412\n",
      "Epoch 1276/1600\n",
      "680/680 [==============================] - 1s 930us/step - loss: 0.1692 - accuracy: 0.5951 - val_loss: 0.4413 - val_accuracy: 0.5441\n",
      "Epoch 1277/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1791 - accuracy: 0.5959 - val_loss: 0.4627 - val_accuracy: 0.5471\n",
      "Epoch 1278/1600\n",
      "680/680 [==============================] - 1s 952us/step - loss: 0.1787 - accuracy: 0.5940 - val_loss: 0.4382 - val_accuracy: 0.5500\n",
      "Epoch 1279/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1676 - accuracy: 0.5985 - val_loss: 0.4344 - val_accuracy: 0.5471\n",
      "Epoch 1280/1600\n",
      "680/680 [==============================] - 1s 935us/step - loss: 0.1662 - accuracy: 0.5966 - val_loss: 0.4466 - val_accuracy: 0.5412\n",
      "Epoch 1281/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1799 - accuracy: 0.5929 - val_loss: 0.4302 - val_accuracy: 0.5471\n",
      "Epoch 1282/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1803 - accuracy: 0.5948 - val_loss: 0.4607 - val_accuracy: 0.5412\n",
      "Epoch 1283/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1724 - accuracy: 0.5970 - val_loss: 0.4621 - val_accuracy: 0.5353\n",
      "Epoch 1284/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1830 - accuracy: 0.5926 - val_loss: 0.4604 - val_accuracy: 0.5235\n",
      "Epoch 1285/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1786 - accuracy: 0.5937 - val_loss: 0.4329 - val_accuracy: 0.5441\n",
      "Epoch 1286/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1703 - accuracy: 0.5985 - val_loss: 0.4538 - val_accuracy: 0.5471\n",
      "Epoch 1287/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1773 - accuracy: 0.5970 - val_loss: 0.4186 - val_accuracy: 0.5471\n",
      "Epoch 1288/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1662 - accuracy: 0.5988 - val_loss: 0.4499 - val_accuracy: 0.5441\n",
      "Epoch 1289/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1904 - accuracy: 0.5926 - val_loss: 0.4318 - val_accuracy: 0.5500\n",
      "Epoch 1290/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1783 - accuracy: 0.5951 - val_loss: 0.4551 - val_accuracy: 0.5441\n",
      "Epoch 1291/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1736 - accuracy: 0.5966 - val_loss: 0.4363 - val_accuracy: 0.5500\n",
      "Epoch 1292/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1831 - accuracy: 0.5937 - val_loss: 0.4435 - val_accuracy: 0.5441\n",
      "Epoch 1293/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1726 - accuracy: 0.5948 - val_loss: 0.4501 - val_accuracy: 0.5382\n",
      "Epoch 1294/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1699 - accuracy: 0.5985 - val_loss: 0.4419 - val_accuracy: 0.5441\n",
      "Epoch 1295/1600\n",
      "680/680 [==============================] - 1s 902us/step - loss: 0.1799 - accuracy: 0.5962 - val_loss: 0.4542 - val_accuracy: 0.5324\n",
      "Epoch 1296/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1713 - accuracy: 0.5962 - val_loss: 0.4560 - val_accuracy: 0.5382\n",
      "Epoch 1297/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1768 - accuracy: 0.5955 - val_loss: 0.4670 - val_accuracy: 0.5294\n",
      "Epoch 1298/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1681 - accuracy: 0.5977 - val_loss: 0.4491 - val_accuracy: 0.5500\n",
      "Epoch 1299/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1726 - accuracy: 0.5944 - val_loss: 0.4233 - val_accuracy: 0.5500\n",
      "Epoch 1300/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1707 - accuracy: 0.5977 - val_loss: 0.4818 - val_accuracy: 0.5412\n",
      "Epoch 1301/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1852 - accuracy: 0.5959 - val_loss: 0.4413 - val_accuracy: 0.5441\n",
      "Epoch 1302/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1672 - accuracy: 0.5966 - val_loss: 0.4546 - val_accuracy: 0.5471\n",
      "Epoch 1303/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.2025 - accuracy: 0.5889 - val_loss: 0.4340 - val_accuracy: 0.5529\n",
      "Epoch 1304/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1743 - accuracy: 0.5951 - val_loss: 0.4304 - val_accuracy: 0.5324\n",
      "Epoch 1305/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1653 - accuracy: 0.5977 - val_loss: 0.4635 - val_accuracy: 0.5382\n",
      "Epoch 1306/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1734 - accuracy: 0.5966 - val_loss: 0.4376 - val_accuracy: 0.5265\n",
      "Epoch 1307/1600\n",
      "680/680 [==============================] - 1s 898us/step - loss: 0.1814 - accuracy: 0.5940 - val_loss: 0.4541 - val_accuracy: 0.5441\n",
      "Epoch 1308/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1575 - accuracy: 0.5999 - val_loss: 0.4456 - val_accuracy: 0.5412\n",
      "Epoch 1309/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1839 - accuracy: 0.5937 - val_loss: 0.4427 - val_accuracy: 0.5441\n",
      "Epoch 1310/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1744 - accuracy: 0.5955 - val_loss: 0.4502 - val_accuracy: 0.5529\n",
      "Epoch 1311/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1653 - accuracy: 0.6003 - val_loss: 0.4387 - val_accuracy: 0.5353\n",
      "Epoch 1312/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1676 - accuracy: 0.5992 - val_loss: 0.4588 - val_accuracy: 0.5471\n",
      "Epoch 1313/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1747 - accuracy: 0.5985 - val_loss: 0.4551 - val_accuracy: 0.5412\n",
      "Epoch 1314/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1834 - accuracy: 0.5955 - val_loss: 0.4836 - val_accuracy: 0.5412\n",
      "Epoch 1315/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1868 - accuracy: 0.5922 - val_loss: 0.4608 - val_accuracy: 0.5382\n",
      "Epoch 1316/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1674 - accuracy: 0.5962 - val_loss: 0.4345 - val_accuracy: 0.5412\n",
      "Epoch 1317/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.1714 - accuracy: 0.5948 - val_loss: 0.3995 - val_accuracy: 0.5382\n",
      "Epoch 1318/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1713 - accuracy: 0.5962 - val_loss: 0.4562 - val_accuracy: 0.5412\n",
      "Epoch 1319/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.1700 - accuracy: 0.5959 - val_loss: 0.4480 - val_accuracy: 0.5441\n",
      "Epoch 1320/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1862 - accuracy: 0.5926 - val_loss: 0.4350 - val_accuracy: 0.5471\n",
      "Epoch 1321/1600\n",
      "680/680 [==============================] - 1s 907us/step - loss: 0.1689 - accuracy: 0.5959 - val_loss: 0.4086 - val_accuracy: 0.5353\n",
      "Epoch 1322/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1729 - accuracy: 0.5966 - val_loss: 0.4082 - val_accuracy: 0.5412\n",
      "Epoch 1323/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 899us/step - loss: 0.1897 - accuracy: 0.5940 - val_loss: 0.4457 - val_accuracy: 0.5441\n",
      "Epoch 1324/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1686 - accuracy: 0.5962 - val_loss: 0.4435 - val_accuracy: 0.5353\n",
      "Epoch 1325/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1800 - accuracy: 0.5970 - val_loss: 0.4867 - val_accuracy: 0.5235\n",
      "Epoch 1326/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1758 - accuracy: 0.5940 - val_loss: 0.4294 - val_accuracy: 0.5412\n",
      "Epoch 1327/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1702 - accuracy: 0.5985 - val_loss: 0.4677 - val_accuracy: 0.5412\n",
      "Epoch 1328/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1746 - accuracy: 0.5944 - val_loss: 0.4345 - val_accuracy: 0.5471\n",
      "Epoch 1329/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1773 - accuracy: 0.5951 - val_loss: 0.4575 - val_accuracy: 0.5441\n",
      "Epoch 1330/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1608 - accuracy: 0.5992 - val_loss: 0.4662 - val_accuracy: 0.5529\n",
      "Epoch 1331/1600\n",
      "680/680 [==============================] - 1s 901us/step - loss: 0.1882 - accuracy: 0.5915 - val_loss: 0.4356 - val_accuracy: 0.5529\n",
      "Epoch 1332/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.1768 - accuracy: 0.5948 - val_loss: 0.4469 - val_accuracy: 0.5382\n",
      "Epoch 1333/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1703 - accuracy: 0.5959 - val_loss: 0.4528 - val_accuracy: 0.5382\n",
      "Epoch 1334/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1735 - accuracy: 0.5970 - val_loss: 0.4258 - val_accuracy: 0.5353\n",
      "Epoch 1335/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1716 - accuracy: 0.5966 - val_loss: 0.4252 - val_accuracy: 0.5500\n",
      "Epoch 1336/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1695 - accuracy: 0.5955 - val_loss: 0.4347 - val_accuracy: 0.5412\n",
      "Epoch 1337/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1766 - accuracy: 0.5951 - val_loss: 0.4440 - val_accuracy: 0.5382\n",
      "Epoch 1338/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1667 - accuracy: 0.5974 - val_loss: 0.4457 - val_accuracy: 0.5382\n",
      "Epoch 1339/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1675 - accuracy: 0.5977 - val_loss: 0.4877 - val_accuracy: 0.5235\n",
      "Epoch 1340/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1800 - accuracy: 0.5929 - val_loss: 0.4351 - val_accuracy: 0.5529\n",
      "Epoch 1341/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.1735 - accuracy: 0.5959 - val_loss: 0.4570 - val_accuracy: 0.5441\n",
      "Epoch 1342/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1798 - accuracy: 0.5981 - val_loss: 0.4393 - val_accuracy: 0.5324\n",
      "Epoch 1343/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1782 - accuracy: 0.5951 - val_loss: 0.4391 - val_accuracy: 0.5471\n",
      "Epoch 1344/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1637 - accuracy: 0.5988 - val_loss: 0.4470 - val_accuracy: 0.5441\n",
      "Epoch 1345/1600\n",
      "680/680 [==============================] - 1s 903us/step - loss: 0.1752 - accuracy: 0.5933 - val_loss: 0.4587 - val_accuracy: 0.5500\n",
      "Epoch 1346/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1829 - accuracy: 0.5926 - val_loss: 0.4659 - val_accuracy: 0.5382\n",
      "Epoch 1347/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1642 - accuracy: 0.5977 - val_loss: 0.4561 - val_accuracy: 0.5412\n",
      "Epoch 1348/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1829 - accuracy: 0.5937 - val_loss: 0.4851 - val_accuracy: 0.5412\n",
      "Epoch 1349/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1756 - accuracy: 0.5951 - val_loss: 0.4353 - val_accuracy: 0.5382\n",
      "Epoch 1350/1600\n",
      "680/680 [==============================] - 1s 903us/step - loss: 0.1787 - accuracy: 0.5922 - val_loss: 0.4550 - val_accuracy: 0.5618\n",
      "Epoch 1351/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1679 - accuracy: 0.5985 - val_loss: 0.4556 - val_accuracy: 0.5412\n",
      "Epoch 1352/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1858 - accuracy: 0.5951 - val_loss: 0.4581 - val_accuracy: 0.5471\n",
      "Epoch 1353/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1837 - accuracy: 0.5922 - val_loss: 0.4545 - val_accuracy: 0.5294\n",
      "Epoch 1354/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1678 - accuracy: 0.5974 - val_loss: 0.4541 - val_accuracy: 0.5412\n",
      "Epoch 1355/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1866 - accuracy: 0.5911 - val_loss: 0.4739 - val_accuracy: 0.5412\n",
      "Epoch 1356/1600\n",
      "680/680 [==============================] - 1s 936us/step - loss: 0.1663 - accuracy: 0.5966 - val_loss: 0.4071 - val_accuracy: 0.5471\n",
      "Epoch 1357/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1657 - accuracy: 0.5966 - val_loss: 0.4781 - val_accuracy: 0.5441\n",
      "Epoch 1358/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1734 - accuracy: 0.5940 - val_loss: 0.4453 - val_accuracy: 0.5441\n",
      "Epoch 1359/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1731 - accuracy: 0.5959 - val_loss: 0.4544 - val_accuracy: 0.5441\n",
      "Epoch 1360/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1689 - accuracy: 0.5962 - val_loss: 0.4689 - val_accuracy: 0.5500\n",
      "Epoch 1361/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1550 - accuracy: 0.6003 - val_loss: 0.4490 - val_accuracy: 0.5471\n",
      "Epoch 1362/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.1670 - accuracy: 0.5962 - val_loss: 0.4483 - val_accuracy: 0.5500\n",
      "Epoch 1363/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1927 - accuracy: 0.5918 - val_loss: 0.4717 - val_accuracy: 0.5412\n",
      "Epoch 1364/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1778 - accuracy: 0.5959 - val_loss: 0.4382 - val_accuracy: 0.5471\n",
      "Epoch 1365/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1728 - accuracy: 0.5955 - val_loss: 0.4617 - val_accuracy: 0.5353\n",
      "Epoch 1366/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1703 - accuracy: 0.5970 - val_loss: 0.4744 - val_accuracy: 0.5353\n",
      "Epoch 1367/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1698 - accuracy: 0.5970 - val_loss: 0.4554 - val_accuracy: 0.5500\n",
      "Epoch 1368/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1680 - accuracy: 0.5985 - val_loss: 0.4497 - val_accuracy: 0.5441\n",
      "Epoch 1369/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1716 - accuracy: 0.5955 - val_loss: 0.4752 - val_accuracy: 0.5294\n",
      "Epoch 1370/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1964 - accuracy: 0.5926 - val_loss: 0.4444 - val_accuracy: 0.5559\n",
      "Epoch 1371/1600\n",
      "680/680 [==============================] - 1s 902us/step - loss: 0.1659 - accuracy: 0.5959 - val_loss: 0.4567 - val_accuracy: 0.5500\n",
      "Epoch 1372/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1701 - accuracy: 0.5959 - val_loss: 0.4622 - val_accuracy: 0.5412\n",
      "Epoch 1373/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1726 - accuracy: 0.5955 - val_loss: 0.4484 - val_accuracy: 0.5471\n",
      "Epoch 1374/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1766 - accuracy: 0.5962 - val_loss: 0.4752 - val_accuracy: 0.5412\n",
      "Epoch 1375/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1605 - accuracy: 0.5988 - val_loss: 0.4751 - val_accuracy: 0.5412\n",
      "Epoch 1376/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1638 - accuracy: 0.5962 - val_loss: 0.4439 - val_accuracy: 0.5500\n",
      "Epoch 1377/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1801 - accuracy: 0.5944 - val_loss: 0.4610 - val_accuracy: 0.5471\n",
      "Epoch 1378/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 927us/step - loss: 0.1713 - accuracy: 0.5962 - val_loss: 0.4684 - val_accuracy: 0.5471\n",
      "Epoch 1379/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1793 - accuracy: 0.5951 - val_loss: 0.4878 - val_accuracy: 0.5294\n",
      "Epoch 1380/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1764 - accuracy: 0.5948 - val_loss: 0.4511 - val_accuracy: 0.5529\n",
      "Epoch 1381/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1718 - accuracy: 0.5977 - val_loss: 0.4760 - val_accuracy: 0.5294\n",
      "Epoch 1382/1600\n",
      "680/680 [==============================] - 1s 973us/step - loss: 0.1669 - accuracy: 0.5959 - val_loss: 0.4865 - val_accuracy: 0.5588\n",
      "Epoch 1383/1600\n",
      "680/680 [==============================] - 1s 949us/step - loss: 0.1810 - accuracy: 0.5926 - val_loss: 0.4529 - val_accuracy: 0.5324\n",
      "Epoch 1384/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1753 - accuracy: 0.5951 - val_loss: 0.4353 - val_accuracy: 0.5500\n",
      "Epoch 1385/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1746 - accuracy: 0.5940 - val_loss: 0.4597 - val_accuracy: 0.5412\n",
      "Epoch 1386/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1810 - accuracy: 0.5951 - val_loss: 0.4569 - val_accuracy: 0.5265\n",
      "Epoch 1387/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1690 - accuracy: 0.5962 - val_loss: 0.4409 - val_accuracy: 0.5441\n",
      "Epoch 1388/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1778 - accuracy: 0.5948 - val_loss: 0.4687 - val_accuracy: 0.5176\n",
      "Epoch 1389/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1675 - accuracy: 0.5962 - val_loss: 0.4608 - val_accuracy: 0.5382\n",
      "Epoch 1390/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1638 - accuracy: 0.5996 - val_loss: 0.4611 - val_accuracy: 0.5471\n",
      "Epoch 1391/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1721 - accuracy: 0.5951 - val_loss: 0.4760 - val_accuracy: 0.5353\n",
      "Epoch 1392/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1628 - accuracy: 0.5981 - val_loss: 0.4530 - val_accuracy: 0.5500\n",
      "Epoch 1393/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1892 - accuracy: 0.5937 - val_loss: 0.4627 - val_accuracy: 0.5559\n",
      "Epoch 1394/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1678 - accuracy: 0.5951 - val_loss: 0.4624 - val_accuracy: 0.5500\n",
      "Epoch 1395/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1695 - accuracy: 0.5977 - val_loss: 0.4562 - val_accuracy: 0.5412\n",
      "Epoch 1396/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1604 - accuracy: 0.5996 - val_loss: 0.4525 - val_accuracy: 0.5382\n",
      "Epoch 1397/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1831 - accuracy: 0.5940 - val_loss: 0.4738 - val_accuracy: 0.5441\n",
      "Epoch 1398/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1768 - accuracy: 0.5922 - val_loss: 0.4604 - val_accuracy: 0.5441\n",
      "Epoch 1399/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1744 - accuracy: 0.5948 - val_loss: 0.4698 - val_accuracy: 0.5441\n",
      "Epoch 1400/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1662 - accuracy: 0.5970 - val_loss: 0.4753 - val_accuracy: 0.5294\n",
      "Epoch 1401/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.1961 - accuracy: 0.5904 - val_loss: 0.4630 - val_accuracy: 0.5441\n",
      "Epoch 1402/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1710 - accuracy: 0.5948 - val_loss: 0.4558 - val_accuracy: 0.5353\n",
      "Epoch 1403/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1669 - accuracy: 0.5962 - val_loss: 0.4387 - val_accuracy: 0.5441\n",
      "Epoch 1404/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1749 - accuracy: 0.5948 - val_loss: 0.4626 - val_accuracy: 0.5294\n",
      "Epoch 1405/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1714 - accuracy: 0.5951 - val_loss: 0.4466 - val_accuracy: 0.5500\n",
      "Epoch 1406/1600\n",
      "680/680 [==============================] - 1s 907us/step - loss: 0.1558 - accuracy: 0.6010 - val_loss: 0.4683 - val_accuracy: 0.5353\n",
      "Epoch 1407/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1899 - accuracy: 0.5889 - val_loss: 0.4434 - val_accuracy: 0.5441\n",
      "Epoch 1408/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1831 - accuracy: 0.5937 - val_loss: 0.4585 - val_accuracy: 0.5412\n",
      "Epoch 1409/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1617 - accuracy: 0.5985 - val_loss: 0.4555 - val_accuracy: 0.5500\n",
      "Epoch 1410/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1824 - accuracy: 0.5933 - val_loss: 0.4532 - val_accuracy: 0.5324\n",
      "Epoch 1411/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1744 - accuracy: 0.5962 - val_loss: 0.4747 - val_accuracy: 0.5324\n",
      "Epoch 1412/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1657 - accuracy: 0.5951 - val_loss: 0.4507 - val_accuracy: 0.5412\n",
      "Epoch 1413/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1673 - accuracy: 0.5955 - val_loss: 0.4350 - val_accuracy: 0.5500\n",
      "Epoch 1414/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1848 - accuracy: 0.5911 - val_loss: 0.4512 - val_accuracy: 0.5412\n",
      "Epoch 1415/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1631 - accuracy: 0.5970 - val_loss: 0.4540 - val_accuracy: 0.5471\n",
      "Epoch 1416/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1659 - accuracy: 0.5962 - val_loss: 0.4622 - val_accuracy: 0.5382\n",
      "Epoch 1417/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1679 - accuracy: 0.5966 - val_loss: 0.4464 - val_accuracy: 0.5382\n",
      "Epoch 1418/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.1709 - accuracy: 0.5974 - val_loss: 0.5027 - val_accuracy: 0.5118\n",
      "Epoch 1419/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1683 - accuracy: 0.5966 - val_loss: 0.4732 - val_accuracy: 0.5324\n",
      "Epoch 1420/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1583 - accuracy: 0.5999 - val_loss: 0.4923 - val_accuracy: 0.5265\n",
      "Epoch 1421/1600\n",
      "680/680 [==============================] - 1s 904us/step - loss: 0.1837 - accuracy: 0.5944 - val_loss: 0.4386 - val_accuracy: 0.5382\n",
      "Epoch 1422/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1834 - accuracy: 0.5918 - val_loss: 0.4516 - val_accuracy: 0.5471\n",
      "Epoch 1423/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1662 - accuracy: 0.5992 - val_loss: 0.4524 - val_accuracy: 0.5176\n",
      "Epoch 1424/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.2009 - accuracy: 0.5918 - val_loss: 0.4616 - val_accuracy: 0.5441\n",
      "Epoch 1425/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1821 - accuracy: 0.5933 - val_loss: 0.4482 - val_accuracy: 0.5382\n",
      "Epoch 1426/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1684 - accuracy: 0.5974 - val_loss: 0.4496 - val_accuracy: 0.5441\n",
      "Epoch 1427/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1658 - accuracy: 0.5985 - val_loss: 0.4577 - val_accuracy: 0.5294\n",
      "Epoch 1428/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1834 - accuracy: 0.5944 - val_loss: 0.4536 - val_accuracy: 0.5294\n",
      "Epoch 1429/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1787 - accuracy: 0.5926 - val_loss: 0.4535 - val_accuracy: 0.5441\n",
      "Epoch 1430/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1700 - accuracy: 0.5966 - val_loss: 0.4565 - val_accuracy: 0.5412\n",
      "Epoch 1431/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1656 - accuracy: 0.5988 - val_loss: 0.4860 - val_accuracy: 0.5353\n",
      "Epoch 1432/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1812 - accuracy: 0.5937 - val_loss: 0.4898 - val_accuracy: 0.5382\n",
      "Epoch 1433/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 887us/step - loss: 0.1725 - accuracy: 0.5962 - val_loss: 0.4909 - val_accuracy: 0.5353\n",
      "Epoch 1434/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1642 - accuracy: 0.5985 - val_loss: 0.4842 - val_accuracy: 0.5235\n",
      "Epoch 1435/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1716 - accuracy: 0.5951 - val_loss: 0.4911 - val_accuracy: 0.5206\n",
      "Epoch 1436/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1699 - accuracy: 0.5955 - val_loss: 0.4655 - val_accuracy: 0.5324\n",
      "Epoch 1437/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1766 - accuracy: 0.5959 - val_loss: 0.4634 - val_accuracy: 0.5441\n",
      "Epoch 1438/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1680 - accuracy: 0.5959 - val_loss: 0.4473 - val_accuracy: 0.5441\n",
      "Epoch 1439/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1654 - accuracy: 0.5985 - val_loss: 0.4653 - val_accuracy: 0.5441\n",
      "Epoch 1440/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1703 - accuracy: 0.5970 - val_loss: 0.4256 - val_accuracy: 0.5441\n",
      "Epoch 1441/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1790 - accuracy: 0.5937 - val_loss: 0.4505 - val_accuracy: 0.5471\n",
      "Epoch 1442/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1678 - accuracy: 0.5981 - val_loss: 0.4936 - val_accuracy: 0.5353\n",
      "Epoch 1443/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1773 - accuracy: 0.5970 - val_loss: 0.4513 - val_accuracy: 0.5353\n",
      "Epoch 1444/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1806 - accuracy: 0.5951 - val_loss: 0.4806 - val_accuracy: 0.5441\n",
      "Epoch 1445/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1902 - accuracy: 0.5933 - val_loss: 0.4672 - val_accuracy: 0.5294\n",
      "Epoch 1446/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1841 - accuracy: 0.5937 - val_loss: 0.4465 - val_accuracy: 0.5382\n",
      "Epoch 1447/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1662 - accuracy: 0.5977 - val_loss: 0.4575 - val_accuracy: 0.5324\n",
      "Epoch 1448/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.1660 - accuracy: 0.5951 - val_loss: 0.4400 - val_accuracy: 0.5441\n",
      "Epoch 1449/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1664 - accuracy: 0.5974 - val_loss: 0.4362 - val_accuracy: 0.5412\n",
      "Epoch 1450/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1673 - accuracy: 0.5966 - val_loss: 0.4347 - val_accuracy: 0.5441\n",
      "Epoch 1451/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1695 - accuracy: 0.5974 - val_loss: 0.4433 - val_accuracy: 0.5500\n",
      "Epoch 1452/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1671 - accuracy: 0.5977 - val_loss: 0.4535 - val_accuracy: 0.5441\n",
      "Epoch 1453/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1809 - accuracy: 0.5937 - val_loss: 0.4624 - val_accuracy: 0.5412\n",
      "Epoch 1454/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1787 - accuracy: 0.5948 - val_loss: 0.4870 - val_accuracy: 0.5500\n",
      "Epoch 1455/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1685 - accuracy: 0.5962 - val_loss: 0.4620 - val_accuracy: 0.5441\n",
      "Epoch 1456/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1643 - accuracy: 0.5985 - val_loss: 0.4722 - val_accuracy: 0.5353\n",
      "Epoch 1457/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1650 - accuracy: 0.5974 - val_loss: 0.4741 - val_accuracy: 0.5441\n",
      "Epoch 1458/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1916 - accuracy: 0.5922 - val_loss: 0.4786 - val_accuracy: 0.5324\n",
      "Epoch 1459/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1760 - accuracy: 0.5944 - val_loss: 0.4616 - val_accuracy: 0.5353\n",
      "Epoch 1460/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1664 - accuracy: 0.5974 - val_loss: 0.4468 - val_accuracy: 0.5382\n",
      "Epoch 1461/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1639 - accuracy: 0.5977 - val_loss: 0.4608 - val_accuracy: 0.5382\n",
      "Epoch 1462/1600\n",
      "680/680 [==============================] - 1s 860us/step - loss: 0.1634 - accuracy: 0.5996 - val_loss: 0.4720 - val_accuracy: 0.5412\n",
      "Epoch 1463/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1765 - accuracy: 0.5962 - val_loss: 0.4605 - val_accuracy: 0.5382\n",
      "Epoch 1464/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1666 - accuracy: 0.5985 - val_loss: 0.4428 - val_accuracy: 0.5441\n",
      "Epoch 1465/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1817 - accuracy: 0.5951 - val_loss: 0.4755 - val_accuracy: 0.5412\n",
      "Epoch 1466/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1624 - accuracy: 0.5970 - val_loss: 0.4581 - val_accuracy: 0.5382\n",
      "Epoch 1467/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1691 - accuracy: 0.5977 - val_loss: 0.4517 - val_accuracy: 0.5471\n",
      "Epoch 1468/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.1726 - accuracy: 0.5981 - val_loss: 0.4450 - val_accuracy: 0.5471\n",
      "Epoch 1469/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1580 - accuracy: 0.5996 - val_loss: 0.4211 - val_accuracy: 0.5559\n",
      "Epoch 1470/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1764 - accuracy: 0.5955 - val_loss: 0.4513 - val_accuracy: 0.5324\n",
      "Epoch 1471/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1774 - accuracy: 0.5962 - val_loss: 0.4925 - val_accuracy: 0.5294\n",
      "Epoch 1472/1600\n",
      "680/680 [==============================] - 1s 863us/step - loss: 0.1738 - accuracy: 0.5962 - val_loss: 0.4522 - val_accuracy: 0.5441\n",
      "Epoch 1473/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1719 - accuracy: 0.5974 - val_loss: 0.4519 - val_accuracy: 0.5353\n",
      "Epoch 1474/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1675 - accuracy: 0.5988 - val_loss: 0.4158 - val_accuracy: 0.5500\n",
      "Epoch 1475/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1776 - accuracy: 0.5937 - val_loss: 0.4533 - val_accuracy: 0.5382\n",
      "Epoch 1476/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1872 - accuracy: 0.5933 - val_loss: 0.4445 - val_accuracy: 0.5500\n",
      "Epoch 1477/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1610 - accuracy: 0.5985 - val_loss: 0.4535 - val_accuracy: 0.5500\n",
      "Epoch 1478/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1710 - accuracy: 0.5951 - val_loss: 0.4807 - val_accuracy: 0.5353\n",
      "Epoch 1479/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1783 - accuracy: 0.5940 - val_loss: 0.4435 - val_accuracy: 0.5353\n",
      "Epoch 1480/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1540 - accuracy: 0.6003 - val_loss: 0.4706 - val_accuracy: 0.5412\n",
      "Epoch 1481/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1684 - accuracy: 0.5970 - val_loss: 0.4897 - val_accuracy: 0.5500\n",
      "Epoch 1482/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1809 - accuracy: 0.5955 - val_loss: 0.4687 - val_accuracy: 0.5412\n",
      "Epoch 1483/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1834 - accuracy: 0.5915 - val_loss: 0.4344 - val_accuracy: 0.5382\n",
      "Epoch 1484/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1631 - accuracy: 0.5970 - val_loss: 0.4809 - val_accuracy: 0.5353\n",
      "Epoch 1485/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1861 - accuracy: 0.5937 - val_loss: 0.4614 - val_accuracy: 0.5529\n",
      "Epoch 1486/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1729 - accuracy: 0.5966 - val_loss: 0.4956 - val_accuracy: 0.5324\n",
      "Epoch 1487/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1668 - accuracy: 0.5977 - val_loss: 0.4704 - val_accuracy: 0.5353\n",
      "Epoch 1488/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 921us/step - loss: 0.1609 - accuracy: 0.5974 - val_loss: 0.4727 - val_accuracy: 0.5471\n",
      "Epoch 1489/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1734 - accuracy: 0.5962 - val_loss: 0.4273 - val_accuracy: 0.5353\n",
      "Epoch 1490/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1733 - accuracy: 0.5974 - val_loss: 0.4254 - val_accuracy: 0.5382\n",
      "Epoch 1491/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.1725 - accuracy: 0.5948 - val_loss: 0.5047 - val_accuracy: 0.5441\n",
      "Epoch 1492/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.1877 - accuracy: 0.5915 - val_loss: 0.4504 - val_accuracy: 0.5412\n",
      "Epoch 1493/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1713 - accuracy: 0.5955 - val_loss: 0.4856 - val_accuracy: 0.5441\n",
      "Epoch 1494/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1814 - accuracy: 0.5940 - val_loss: 0.4499 - val_accuracy: 0.5529\n",
      "Epoch 1495/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1632 - accuracy: 0.6007 - val_loss: 0.4808 - val_accuracy: 0.5324\n",
      "Epoch 1496/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1671 - accuracy: 0.5940 - val_loss: 0.4541 - val_accuracy: 0.5471\n",
      "Epoch 1497/1600\n",
      "680/680 [==============================] - 1s 898us/step - loss: 0.1842 - accuracy: 0.5940 - val_loss: 0.4316 - val_accuracy: 0.5500\n",
      "Epoch 1498/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1783 - accuracy: 0.5933 - val_loss: 0.4496 - val_accuracy: 0.5294\n",
      "Epoch 1499/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1769 - accuracy: 0.5959 - val_loss: 0.4954 - val_accuracy: 0.5412\n",
      "Epoch 1500/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1724 - accuracy: 0.5944 - val_loss: 0.4348 - val_accuracy: 0.5412\n",
      "Epoch 1501/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1679 - accuracy: 0.5966 - val_loss: 0.4620 - val_accuracy: 0.5294\n",
      "Epoch 1502/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1787 - accuracy: 0.5944 - val_loss: 0.4436 - val_accuracy: 0.5324\n",
      "Epoch 1503/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1600 - accuracy: 0.5996 - val_loss: 0.4511 - val_accuracy: 0.5353\n",
      "Epoch 1504/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1571 - accuracy: 0.5999 - val_loss: 0.4757 - val_accuracy: 0.5441\n",
      "Epoch 1505/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1906 - accuracy: 0.5926 - val_loss: 0.4299 - val_accuracy: 0.5412\n",
      "Epoch 1506/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1643 - accuracy: 0.5970 - val_loss: 0.4484 - val_accuracy: 0.5382\n",
      "Epoch 1507/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1817 - accuracy: 0.5915 - val_loss: 0.4880 - val_accuracy: 0.5441\n",
      "Epoch 1508/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1603 - accuracy: 0.5977 - val_loss: 0.4499 - val_accuracy: 0.5353\n",
      "Epoch 1509/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1850 - accuracy: 0.5926 - val_loss: 0.4570 - val_accuracy: 0.5382\n",
      "Epoch 1510/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1800 - accuracy: 0.5929 - val_loss: 0.4591 - val_accuracy: 0.5471\n",
      "Epoch 1511/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1575 - accuracy: 0.5988 - val_loss: 0.4869 - val_accuracy: 0.5441\n",
      "Epoch 1512/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1770 - accuracy: 0.5951 - val_loss: 0.4639 - val_accuracy: 0.5382\n",
      "Epoch 1513/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1649 - accuracy: 0.5977 - val_loss: 0.4573 - val_accuracy: 0.5412\n",
      "Epoch 1514/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1651 - accuracy: 0.5955 - val_loss: 0.4492 - val_accuracy: 0.5471\n",
      "Epoch 1515/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1698 - accuracy: 0.5970 - val_loss: 0.4644 - val_accuracy: 0.5471\n",
      "Epoch 1516/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1788 - accuracy: 0.5937 - val_loss: 0.4309 - val_accuracy: 0.5441\n",
      "Epoch 1517/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1565 - accuracy: 0.5985 - val_loss: 0.4297 - val_accuracy: 0.5529\n",
      "Epoch 1518/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1702 - accuracy: 0.5962 - val_loss: 0.4580 - val_accuracy: 0.5471\n",
      "Epoch 1519/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1767 - accuracy: 0.5959 - val_loss: 0.4608 - val_accuracy: 0.5294\n",
      "Epoch 1520/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1633 - accuracy: 0.5962 - val_loss: 0.4913 - val_accuracy: 0.5441\n",
      "Epoch 1521/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1866 - accuracy: 0.5929 - val_loss: 0.4638 - val_accuracy: 0.5353\n",
      "Epoch 1522/1600\n",
      "680/680 [==============================] - 1s 907us/step - loss: 0.1579 - accuracy: 0.5992 - val_loss: 0.4874 - val_accuracy: 0.5412\n",
      "Epoch 1523/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1780 - accuracy: 0.5926 - val_loss: 0.4722 - val_accuracy: 0.5471\n",
      "Epoch 1524/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1722 - accuracy: 0.5985 - val_loss: 0.4607 - val_accuracy: 0.5353\n",
      "Epoch 1525/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1843 - accuracy: 0.5951 - val_loss: 0.4654 - val_accuracy: 0.5324\n",
      "Epoch 1526/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1723 - accuracy: 0.5977 - val_loss: 0.4458 - val_accuracy: 0.5353\n",
      "Epoch 1527/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1711 - accuracy: 0.5977 - val_loss: 0.4699 - val_accuracy: 0.5353\n",
      "Epoch 1528/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1590 - accuracy: 0.5988 - val_loss: 0.4788 - val_accuracy: 0.5176\n",
      "Epoch 1529/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1788 - accuracy: 0.5929 - val_loss: 0.4523 - val_accuracy: 0.5471\n",
      "Epoch 1530/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1788 - accuracy: 0.5937 - val_loss: 0.4639 - val_accuracy: 0.5412\n",
      "Epoch 1531/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1547 - accuracy: 0.6003 - val_loss: 0.4775 - val_accuracy: 0.5353\n",
      "Epoch 1532/1600\n",
      "680/680 [==============================] - 1s 898us/step - loss: 0.1685 - accuracy: 0.5962 - val_loss: 0.4416 - val_accuracy: 0.5441\n",
      "Epoch 1533/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1805 - accuracy: 0.5933 - val_loss: 0.4554 - val_accuracy: 0.5324\n",
      "Epoch 1534/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1672 - accuracy: 0.5955 - val_loss: 0.4459 - val_accuracy: 0.5441\n",
      "Epoch 1535/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1818 - accuracy: 0.5970 - val_loss: 0.4450 - val_accuracy: 0.5265\n",
      "Epoch 1536/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1564 - accuracy: 0.5999 - val_loss: 0.4911 - val_accuracy: 0.5441\n",
      "Epoch 1537/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1718 - accuracy: 0.5974 - val_loss: 0.4873 - val_accuracy: 0.5412\n",
      "Epoch 1538/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1615 - accuracy: 0.5974 - val_loss: 0.4892 - val_accuracy: 0.5412\n",
      "Epoch 1539/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1789 - accuracy: 0.5940 - val_loss: 0.4471 - val_accuracy: 0.5529\n",
      "Epoch 1540/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1752 - accuracy: 0.5966 - val_loss: 0.4460 - val_accuracy: 0.5382\n",
      "Epoch 1541/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1659 - accuracy: 0.5962 - val_loss: 0.4665 - val_accuracy: 0.5559\n",
      "Epoch 1542/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1624 - accuracy: 0.6003 - val_loss: 0.4724 - val_accuracy: 0.5324\n",
      "Epoch 1543/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 883us/step - loss: 0.1702 - accuracy: 0.5974 - val_loss: 0.4817 - val_accuracy: 0.5324\n",
      "Epoch 1544/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1787 - accuracy: 0.5948 - val_loss: 0.5056 - val_accuracy: 0.5206\n",
      "Epoch 1545/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1720 - accuracy: 0.5974 - val_loss: 0.4501 - val_accuracy: 0.5294\n",
      "Epoch 1546/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1775 - accuracy: 0.5948 - val_loss: 0.4601 - val_accuracy: 0.5500\n",
      "Epoch 1547/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1661 - accuracy: 0.5970 - val_loss: 0.4732 - val_accuracy: 0.5441\n",
      "Epoch 1548/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1720 - accuracy: 0.5944 - val_loss: 0.4667 - val_accuracy: 0.5382\n",
      "Epoch 1549/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1755 - accuracy: 0.5948 - val_loss: 0.4523 - val_accuracy: 0.5382\n",
      "Epoch 1550/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1703 - accuracy: 0.5974 - val_loss: 0.4698 - val_accuracy: 0.5471\n",
      "Epoch 1551/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1684 - accuracy: 0.5974 - val_loss: 0.4726 - val_accuracy: 0.5471\n",
      "Epoch 1552/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1645 - accuracy: 0.6018 - val_loss: 0.4733 - val_accuracy: 0.5382\n",
      "Epoch 1553/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.1872 - accuracy: 0.5933 - val_loss: 0.4555 - val_accuracy: 0.5500\n",
      "Epoch 1554/1600\n",
      "680/680 [==============================] - 1s 974us/step - loss: 0.1839 - accuracy: 0.5926 - val_loss: 0.4798 - val_accuracy: 0.5265\n",
      "Epoch 1555/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1678 - accuracy: 0.5977 - val_loss: 0.4580 - val_accuracy: 0.5441\n",
      "Epoch 1556/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1607 - accuracy: 0.5981 - val_loss: 0.4769 - val_accuracy: 0.5294\n",
      "Epoch 1557/1600\n",
      "680/680 [==============================] - 1s 866us/step - loss: 0.1621 - accuracy: 0.5981 - val_loss: 0.4687 - val_accuracy: 0.5412\n",
      "Epoch 1558/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1684 - accuracy: 0.5966 - val_loss: 0.4688 - val_accuracy: 0.5294\n",
      "Epoch 1559/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1640 - accuracy: 0.5988 - val_loss: 0.4954 - val_accuracy: 0.5382\n",
      "Epoch 1560/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1919 - accuracy: 0.5940 - val_loss: 0.4540 - val_accuracy: 0.5265\n",
      "Epoch 1561/1600\n",
      "680/680 [==============================] - 1s 949us/step - loss: 0.1752 - accuracy: 0.5940 - val_loss: 0.4594 - val_accuracy: 0.5265\n",
      "Epoch 1562/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1571 - accuracy: 0.5988 - val_loss: 0.4671 - val_accuracy: 0.5353\n",
      "Epoch 1563/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1771 - accuracy: 0.5944 - val_loss: 0.4667 - val_accuracy: 0.5500\n",
      "Epoch 1564/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1649 - accuracy: 0.5970 - val_loss: 0.4672 - val_accuracy: 0.5500\n",
      "Epoch 1565/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1698 - accuracy: 0.5981 - val_loss: 0.4667 - val_accuracy: 0.5324\n",
      "Epoch 1566/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1671 - accuracy: 0.5970 - val_loss: 0.4481 - val_accuracy: 0.5471\n",
      "Epoch 1567/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1612 - accuracy: 0.5977 - val_loss: 0.4888 - val_accuracy: 0.5382\n",
      "Epoch 1568/1600\n",
      "680/680 [==============================] - 1s 859us/step - loss: 0.1724 - accuracy: 0.5977 - val_loss: 0.4618 - val_accuracy: 0.5353\n",
      "Epoch 1569/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1652 - accuracy: 0.5981 - val_loss: 0.4512 - val_accuracy: 0.5382\n",
      "Epoch 1570/1600\n",
      "680/680 [==============================] - 1s 856us/step - loss: 0.1713 - accuracy: 0.5951 - val_loss: 0.4457 - val_accuracy: 0.5441\n",
      "Epoch 1571/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1715 - accuracy: 0.5996 - val_loss: 0.4510 - val_accuracy: 0.5265\n",
      "Epoch 1572/1600\n",
      "680/680 [==============================] - 1s 840us/step - loss: 0.1736 - accuracy: 0.5948 - val_loss: 0.4848 - val_accuracy: 0.5353\n",
      "Epoch 1573/1600\n",
      "680/680 [==============================] - 1s 945us/step - loss: 0.1769 - accuracy: 0.5944 - val_loss: 0.4657 - val_accuracy: 0.5412\n",
      "Epoch 1574/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1739 - accuracy: 0.5959 - val_loss: 0.4411 - val_accuracy: 0.5441\n",
      "Epoch 1575/1600\n",
      "680/680 [==============================] - 1s 864us/step - loss: 0.1671 - accuracy: 0.5974 - val_loss: 0.4650 - val_accuracy: 0.5500\n",
      "Epoch 1576/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1592 - accuracy: 0.6010 - val_loss: 0.4829 - val_accuracy: 0.5441\n",
      "Epoch 1577/1600\n",
      "680/680 [==============================] - 1s 852us/step - loss: 0.1595 - accuracy: 0.5977 - val_loss: 0.5068 - val_accuracy: 0.5029\n",
      "Epoch 1578/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.1817 - accuracy: 0.5955 - val_loss: 0.4562 - val_accuracy: 0.5353\n",
      "Epoch 1579/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1681 - accuracy: 0.5977 - val_loss: 0.4644 - val_accuracy: 0.5353\n",
      "Epoch 1580/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1565 - accuracy: 0.5981 - val_loss: 0.4374 - val_accuracy: 0.5471\n",
      "Epoch 1581/1600\n",
      "680/680 [==============================] - 1s 859us/step - loss: 0.1924 - accuracy: 0.5933 - val_loss: 0.4646 - val_accuracy: 0.5265\n",
      "Epoch 1582/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1674 - accuracy: 0.5966 - val_loss: 0.4580 - val_accuracy: 0.5441\n",
      "Epoch 1583/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.1753 - accuracy: 0.5977 - val_loss: 0.4345 - val_accuracy: 0.5471\n",
      "Epoch 1584/1600\n",
      "680/680 [==============================] - 1s 862us/step - loss: 0.1757 - accuracy: 0.5955 - val_loss: 0.4677 - val_accuracy: 0.5412\n",
      "Epoch 1585/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1769 - accuracy: 0.5970 - val_loss: 0.5058 - val_accuracy: 0.5412\n",
      "Epoch 1586/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1638 - accuracy: 0.5977 - val_loss: 0.4889 - val_accuracy: 0.5441\n",
      "Epoch 1587/1600\n",
      "680/680 [==============================] - 1s 851us/step - loss: 0.1661 - accuracy: 0.5966 - val_loss: 0.4575 - val_accuracy: 0.5324\n",
      "Epoch 1588/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1788 - accuracy: 0.5951 - val_loss: 0.4509 - val_accuracy: 0.5471\n",
      "Epoch 1589/1600\n",
      "680/680 [==============================] - 1s 937us/step - loss: 0.1744 - accuracy: 0.5988 - val_loss: 0.4687 - val_accuracy: 0.5382\n",
      "Epoch 1590/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.1644 - accuracy: 0.5970 - val_loss: 0.5131 - val_accuracy: 0.5353\n",
      "Epoch 1591/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1853 - accuracy: 0.5955 - val_loss: 0.4241 - val_accuracy: 0.5559\n",
      "Epoch 1592/1600\n",
      "680/680 [==============================] - 1s 842us/step - loss: 0.1776 - accuracy: 0.5951 - val_loss: 0.4494 - val_accuracy: 0.5471\n",
      "Epoch 1593/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1619 - accuracy: 0.5981 - val_loss: 0.5020 - val_accuracy: 0.5353\n",
      "Epoch 1594/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1700 - accuracy: 0.5981 - val_loss: 0.4455 - val_accuracy: 0.5471\n",
      "Epoch 1595/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.1782 - accuracy: 0.5959 - val_loss: 0.4811 - val_accuracy: 0.5441\n",
      "Epoch 1596/1600\n",
      "680/680 [==============================] - 1s 975us/step - loss: 0.1767 - accuracy: 0.5951 - val_loss: 0.4972 - val_accuracy: 0.5441\n",
      "Epoch 1597/1600\n",
      "680/680 [==============================] - 1s 844us/step - loss: 0.1828 - accuracy: 0.5933 - val_loss: 0.4886 - val_accuracy: 0.5353\n",
      "Epoch 1598/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 935us/step - loss: 0.1736 - accuracy: 0.5966 - val_loss: 0.4545 - val_accuracy: 0.5441\n",
      "Epoch 1599/1600\n",
      "680/680 [==============================] - 1s 840us/step - loss: 0.1876 - accuracy: 0.5937 - val_loss: 0.4849 - val_accuracy: 0.5324\n",
      "Epoch 1600/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1627 - accuracy: 0.5988 - val_loss: 0.4455 - val_accuracy: 0.5471\n"
     ]
    }
   ],
   "source": [
    "classTrain = Transformer.toClassification(activitiesTrain)\n",
    "classVal = Transformer.toClassification(activitiesValidate)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.01,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False\n",
    ")\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "l1Reg = 0.001\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=np.shape(trainData)[1], activation='softmax', kernel_regularizer = keras.regularizers.L2(l1Reg)))\n",
    "model.add(Dense(150, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model.add(Dense(75, activation='relu', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model.add(Dense(100, activation='relu', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model.add(Dense(100, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model.add(Dense(1, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "\n",
    "model.compile(loss='MeanSquaredError', optimizer=\"adam\", metrics=['accuracy'])\n",
    "history = model.fit(trainData, Transformer.toClassification(activitiesTrain), \n",
    "                    validation_data = (valData, classVal), epochs=1600, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3caf2ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x1fa2e247910>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "classify = Transformer.toClassification(activitiesTrain)\n",
    "constantGuess = (len(classify[classify == 1]))/len(classify)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,8))\n",
    "\n",
    "ax.plot(history.history[\"accuracy\"], color=\"green\")\n",
    "ax.plot(history.history[\"val_accuracy\"], color=\"purple\")\n",
    "ax.axhline(constantGuess, color=\"blue\", linestyle=\"dashed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b656c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
