{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf02e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import Loader\n",
    "import Transformer\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.losses import MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "402c42c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "compoundsTrain, smilesTrain, labelsTrain, compoundDataTrain, activitiesTrain = Loader.getTrain(defaultValue=0)\n",
    "compoundsTest, smilesTest, labelsTest, compoundDataTest, activitiesTest = Loader.getTest(defaultValue=0)\n",
    "compoundsValidate, smilesValidate, labelsValidate, compoundDataValidate, activitiesValidate = Loader.getValidate(defaultValue=0)\n",
    "\n",
    "#print(labelsTrain)\n",
    "#print(compoundsTrain)\n",
    "#print(smilesTrain)\n",
    "#print(activitiesTrain)\n",
    "\n",
    "#for i in range(len(labelsTrain)):\n",
    "#    print(labelsTrain[i] + \": \", compoundDataTrain[0,i])\n",
    "\n",
    "# def toClassification(y): # The resulting array will contain values of -1 if it is below 4.5 and 1 if it is above\n",
    "#     y = np.array(y)\n",
    "#     classification = (y.astype(float)>4).astype(int)\n",
    "#     return classification * 2 - 1\n",
    "\n",
    "# def normalizeData(train,test,validate):\n",
    "#     for i in range(np.shape(train)[1]):\n",
    "#         std = np.std(train[:,i])\n",
    "#         mean = np.mean(train[:,i])\n",
    "#         if(std == 0):\n",
    "#             std = 1\n",
    "#         train[:,i] = (train[:,i] - mean) / std\n",
    "#         test[:,i] = (test[:,i] - mean) / std\n",
    "#         validate[:,i] = (validate[:,i] - mean) / std\n",
    "#     return train, test, validate\n",
    "\n",
    "# def normalize_2d(matrix):\n",
    "#     norm = np.linalg.norm(matrix,np.inf)\n",
    "#     matrix = matrix/norm  # normalized matrix\n",
    "#     return matrix\n",
    "\n",
    "# normalizeData(compoundDataTrain, compoundDataTest, compoundDataValidate)\n",
    "\n",
    "# print(len(compoundsTrain), len(compoundsTest), len(compoundsValidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b9dda4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcut2d retention: [0.99364773]\n",
      "\ttotal: 99.3647727341231%\n",
      "chi retention: [0.9541968]\n",
      "\ttotal: 95.41967968597893%\n",
      "paoe retention: [0.31495127 0.19509321 0.1390042  0.08838179]\n",
      "\ttotal: 73.743047918213%\n",
      "smr retention: [0.50634726 0.24568468 0.08792319 0.07435827]\n",
      "\ttotal: 91.43133907840063%\n",
      "slogp retention: [0.44620276 0.22134465 0.15753302]\n",
      "\ttotal: 82.50804204396218%\n",
      "estate_vsa retention: [0.29224011 0.18964809 0.14361318]\n",
      "\ttotal: 62.55013868306429%\n",
      "vsa_estate retention: [0.49255304 0.32866098 0.09719245]\n",
      "\ttotal: 91.84064669216791%\n",
      "fr retention: [0.32153498 0.12578177 0.09944384]\n",
      "\ttotal: 54.67605936051172%\n"
     ]
    }
   ],
   "source": [
    "labelsPCA, trainPCA, testPCA, valPCA = Transformer.applyPCA(labelsTrain,  compoundDataTrain, \n",
    "                                                            compoundDataTest, compoundDataValidate,\n",
    "                                                            endDims=[1,1,4,4,3,3,3,3])\n",
    "\n",
    "labelsMeanPCA, trainMeanPCA = Transformer.useAverageFD(labelsPCA, trainPCA)\n",
    "_, testMeanPCA = Transformer.useAverageFD(labelsPCA, testPCA)\n",
    "_, valMeanPCA = Transformer.useAverageFD(labelsPCA, valPCA)\n",
    "\n",
    "labelsMaxPCA, trainMaxPCA = Transformer.useMaxFD(labelsPCA, trainPCA)\n",
    "_, testMaxPCA = Transformer.useMaxFD(labelsPCA, testPCA)\n",
    "_, valMaxPCA = Transformer.useMaxFD(labelsPCA, valPCA)\n",
    "\n",
    "#after transformations are done assign data\n",
    "dataLabels = labelsMaxPCA\n",
    "trainData = trainMaxPCA\n",
    "testData = testMaxPCA\n",
    "valData = valMaxPCA\n",
    "\n",
    "trainData, testData, valData = Transformer.normalizeData(trainData, testData, valData, newMean=0, newStd=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941488f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = Transformer.toClassification(activitiesTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4c3b173",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 1.2877 - accuracy: 0.2205 - val_loss: 0.6534 - val_accuracy: 0.4029\n",
      "Epoch 2/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5906 - accuracy: 0.4520 - val_loss: 0.5538 - val_accuracy: 0.4500\n",
      "Epoch 3/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.5214 - accuracy: 0.4833 - val_loss: 0.5281 - val_accuracy: 0.4324\n",
      "Epoch 4/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.4808 - accuracy: 0.4914 - val_loss: 0.5160 - val_accuracy: 0.4647\n",
      "Epoch 5/1600\n",
      "680/680 [==============================] - 1s 938us/step - loss: 0.4544 - accuracy: 0.5009 - val_loss: 0.4712 - val_accuracy: 0.4765\n",
      "Epoch 6/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.4377 - accuracy: 0.5024 - val_loss: 0.4759 - val_accuracy: 0.4676\n",
      "Epoch 7/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.5086 - val_loss: 0.4479 - val_accuracy: 0.5088\n",
      "Epoch 8/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.4027 - accuracy: 0.5164 - val_loss: 0.4597 - val_accuracy: 0.4647\n",
      "Epoch 9/1600\n",
      "680/680 [==============================] - 1s 939us/step - loss: 0.3892 - accuracy: 0.5193 - val_loss: 0.4497 - val_accuracy: 0.5000\n",
      "Epoch 10/1600\n",
      "680/680 [==============================] - 1s 960us/step - loss: 0.3839 - accuracy: 0.5256 - val_loss: 0.4549 - val_accuracy: 0.5088\n",
      "Epoch 11/1600\n",
      "680/680 [==============================] - 1s 941us/step - loss: 0.3666 - accuracy: 0.5278 - val_loss: 0.4327 - val_accuracy: 0.4941\n",
      "Epoch 12/1600\n",
      "680/680 [==============================] - 1s 956us/step - loss: 0.3633 - accuracy: 0.5318 - val_loss: 0.4209 - val_accuracy: 0.5000\n",
      "Epoch 13/1600\n",
      "680/680 [==============================] - 1s 977us/step - loss: 0.3556 - accuracy: 0.5293 - val_loss: 0.4155 - val_accuracy: 0.5118\n",
      "Epoch 14/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.3481 - accuracy: 0.5337 - val_loss: 0.4250 - val_accuracy: 0.5000\n",
      "Epoch 15/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.3420 - accuracy: 0.5374 - val_loss: 0.4258 - val_accuracy: 0.5088\n",
      "Epoch 16/1600\n",
      "680/680 [==============================] - 1s 935us/step - loss: 0.3403 - accuracy: 0.5388 - val_loss: 0.4094 - val_accuracy: 0.5235\n",
      "Epoch 17/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.3300 - accuracy: 0.5396 - val_loss: 0.4106 - val_accuracy: 0.5059\n",
      "Epoch 18/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.3337 - accuracy: 0.5392 - val_loss: 0.4250 - val_accuracy: 0.5118\n",
      "Epoch 19/1600\n",
      "680/680 [==============================] - 1s 903us/step - loss: 0.3211 - accuracy: 0.5425 - val_loss: 0.4109 - val_accuracy: 0.5147\n",
      "Epoch 20/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.3155 - accuracy: 0.5447 - val_loss: 0.4107 - val_accuracy: 0.5412\n",
      "Epoch 21/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.3103 - accuracy: 0.5462 - val_loss: 0.4269 - val_accuracy: 0.5353\n",
      "Epoch 22/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.3049 - accuracy: 0.5499 - val_loss: 0.4171 - val_accuracy: 0.5412\n",
      "Epoch 23/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.3028 - accuracy: 0.5477 - val_loss: 0.4072 - val_accuracy: 0.5353\n",
      "Epoch 24/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.3004 - accuracy: 0.5510 - val_loss: 0.4211 - val_accuracy: 0.5265\n",
      "Epoch 25/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.3012 - accuracy: 0.5513 - val_loss: 0.4071 - val_accuracy: 0.5176\n",
      "Epoch 26/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.2901 - accuracy: 0.5543 - val_loss: 0.3973 - val_accuracy: 0.5206\n",
      "Epoch 27/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.2951 - accuracy: 0.5499 - val_loss: 0.4022 - val_accuracy: 0.5176\n",
      "Epoch 28/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.2840 - accuracy: 0.5513 - val_loss: 0.4082 - val_accuracy: 0.5382\n",
      "Epoch 29/1600\n",
      "680/680 [==============================] - 1s 935us/step - loss: 0.2844 - accuracy: 0.5521 - val_loss: 0.4166 - val_accuracy: 0.5235\n",
      "Epoch 30/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.2783 - accuracy: 0.5609 - val_loss: 0.4138 - val_accuracy: 0.5235\n",
      "Epoch 31/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.2819 - accuracy: 0.5554 - val_loss: 0.4324 - val_accuracy: 0.5088\n",
      "Epoch 32/1600\n",
      "680/680 [==============================] - 1s 903us/step - loss: 0.2818 - accuracy: 0.5528 - val_loss: 0.4054 - val_accuracy: 0.5294\n",
      "Epoch 33/1600\n",
      "680/680 [==============================] - 1s 907us/step - loss: 0.2835 - accuracy: 0.5561 - val_loss: 0.3888 - val_accuracy: 0.5206\n",
      "Epoch 34/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.2880 - accuracy: 0.5532 - val_loss: 0.3821 - val_accuracy: 0.5176\n",
      "Epoch 35/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.2673 - accuracy: 0.5569 - val_loss: 0.4024 - val_accuracy: 0.5059\n",
      "Epoch 36/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.2712 - accuracy: 0.5565 - val_loss: 0.3947 - val_accuracy: 0.5441\n",
      "Epoch 37/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.2688 - accuracy: 0.5624 - val_loss: 0.3790 - val_accuracy: 0.5353\n",
      "Epoch 38/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.2662 - accuracy: 0.5613 - val_loss: 0.3725 - val_accuracy: 0.5294\n",
      "Epoch 39/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.2684 - accuracy: 0.5646 - val_loss: 0.3945 - val_accuracy: 0.5176\n",
      "Epoch 40/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.2700 - accuracy: 0.5583 - val_loss: 0.3825 - val_accuracy: 0.5441\n",
      "Epoch 41/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.2606 - accuracy: 0.5635 - val_loss: 0.3713 - val_accuracy: 0.5324\n",
      "Epoch 42/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.2538 - accuracy: 0.5628 - val_loss: 0.3791 - val_accuracy: 0.5088\n",
      "Epoch 43/1600\n",
      "680/680 [==============================] - 1s 904us/step - loss: 0.2639 - accuracy: 0.5639 - val_loss: 0.3735 - val_accuracy: 0.5088\n",
      "Epoch 44/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2545 - accuracy: 0.5620 - val_loss: 0.3874 - val_accuracy: 0.5441\n",
      "Epoch 45/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2542 - accuracy: 0.5661 - val_loss: 0.3686 - val_accuracy: 0.5235\n",
      "Epoch 46/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2596 - accuracy: 0.5620 - val_loss: 0.3997 - val_accuracy: 0.5176\n",
      "Epoch 47/1600\n",
      "680/680 [==============================] - 1s 970us/step - loss: 0.2539 - accuracy: 0.5646 - val_loss: 0.3759 - val_accuracy: 0.5324\n",
      "Epoch 48/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2460 - accuracy: 0.5716 - val_loss: 0.3830 - val_accuracy: 0.5147\n",
      "Epoch 49/1600\n",
      "680/680 [==============================] - 1s 968us/step - loss: 0.2494 - accuracy: 0.5657 - val_loss: 0.3666 - val_accuracy: 0.5382\n",
      "Epoch 50/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.2496 - accuracy: 0.5657 - val_loss: 0.3812 - val_accuracy: 0.5382\n",
      "Epoch 51/1600\n",
      "680/680 [==============================] - 1s 946us/step - loss: 0.2492 - accuracy: 0.5672 - val_loss: 0.4327 - val_accuracy: 0.4941\n",
      "Epoch 52/1600\n",
      "680/680 [==============================] - 1s 983us/step - loss: 0.2489 - accuracy: 0.5683 - val_loss: 0.4119 - val_accuracy: 0.5088\n",
      "Epoch 53/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2514 - accuracy: 0.5650 - val_loss: 0.3758 - val_accuracy: 0.5324\n",
      "Epoch 54/1600\n",
      "680/680 [==============================] - 1s 989us/step - loss: 0.2402 - accuracy: 0.5690 - val_loss: 0.4086 - val_accuracy: 0.5500\n",
      "Epoch 55/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.2421 - accuracy: 0.5712 - val_loss: 0.3851 - val_accuracy: 0.5382\n",
      "Epoch 56/1600\n",
      "680/680 [==============================] - 1s 993us/step - loss: 0.2438 - accuracy: 0.5694 - val_loss: 0.3904 - val_accuracy: 0.5529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.2450 - accuracy: 0.5731 - val_loss: 0.3980 - val_accuracy: 0.5559\n",
      "Epoch 58/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.2405 - accuracy: 0.5672 - val_loss: 0.3835 - val_accuracy: 0.5559\n",
      "Epoch 59/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.2523 - accuracy: 0.5697 - val_loss: 0.3894 - val_accuracy: 0.5353\n",
      "Epoch 60/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2396 - accuracy: 0.5716 - val_loss: 0.3919 - val_accuracy: 0.5353\n",
      "Epoch 61/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2439 - accuracy: 0.5709 - val_loss: 0.3919 - val_accuracy: 0.5324\n",
      "Epoch 62/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2408 - accuracy: 0.5701 - val_loss: 0.3771 - val_accuracy: 0.5441\n",
      "Epoch 63/1600\n",
      "680/680 [==============================] - 1s 971us/step - loss: 0.2315 - accuracy: 0.5731 - val_loss: 0.3942 - val_accuracy: 0.5324\n",
      "Epoch 64/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2440 - accuracy: 0.5723 - val_loss: 0.3813 - val_accuracy: 0.5353\n",
      "Epoch 65/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2279 - accuracy: 0.5745 - val_loss: 0.3933 - val_accuracy: 0.5382\n",
      "Epoch 66/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2352 - accuracy: 0.5712 - val_loss: 0.3706 - val_accuracy: 0.5412\n",
      "Epoch 67/1600\n",
      "680/680 [==============================] - 1s 981us/step - loss: 0.2288 - accuracy: 0.5731 - val_loss: 0.3968 - val_accuracy: 0.5324\n",
      "Epoch 68/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2288 - accuracy: 0.5767 - val_loss: 0.3934 - val_accuracy: 0.5324\n",
      "Epoch 69/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2341 - accuracy: 0.5705 - val_loss: 0.3987 - val_accuracy: 0.5147\n",
      "Epoch 70/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2277 - accuracy: 0.5778 - val_loss: 0.4196 - val_accuracy: 0.5235\n",
      "Epoch 71/1600\n",
      "680/680 [==============================] - 1s 968us/step - loss: 0.2240 - accuracy: 0.5727 - val_loss: 0.4148 - val_accuracy: 0.5412\n",
      "Epoch 72/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2411 - accuracy: 0.5709 - val_loss: 0.4175 - val_accuracy: 0.5412\n",
      "Epoch 73/1600\n",
      "680/680 [==============================] - 1s 947us/step - loss: 0.2295 - accuracy: 0.5749 - val_loss: 0.3920 - val_accuracy: 0.5471\n",
      "Epoch 74/1600\n",
      "680/680 [==============================] - 1s 866us/step - loss: 0.2319 - accuracy: 0.5760 - val_loss: 0.3982 - val_accuracy: 0.5500\n",
      "Epoch 75/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.2223 - accuracy: 0.5760 - val_loss: 0.4255 - val_accuracy: 0.5265\n",
      "Epoch 76/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.2210 - accuracy: 0.5778 - val_loss: 0.3999 - val_accuracy: 0.5353\n",
      "Epoch 77/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.2277 - accuracy: 0.5793 - val_loss: 0.3890 - val_accuracy: 0.5265\n",
      "Epoch 78/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.2303 - accuracy: 0.5756 - val_loss: 0.3811 - val_accuracy: 0.5412\n",
      "Epoch 79/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.2243 - accuracy: 0.5767 - val_loss: 0.4064 - val_accuracy: 0.5588\n",
      "Epoch 80/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2309 - accuracy: 0.5801 - val_loss: 0.4360 - val_accuracy: 0.5206\n",
      "Epoch 81/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2280 - accuracy: 0.5745 - val_loss: 0.3803 - val_accuracy: 0.5294\n",
      "Epoch 82/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.2138 - accuracy: 0.5789 - val_loss: 0.4330 - val_accuracy: 0.5206\n",
      "Epoch 83/1600\n",
      "680/680 [==============================] - 1s 988us/step - loss: 0.2222 - accuracy: 0.5786 - val_loss: 0.4036 - val_accuracy: 0.5559\n",
      "Epoch 84/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2247 - accuracy: 0.5801 - val_loss: 0.4080 - val_accuracy: 0.5353\n",
      "Epoch 85/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.2195 - accuracy: 0.5804 - val_loss: 0.4153 - val_accuracy: 0.5382\n",
      "Epoch 86/1600\n",
      "680/680 [==============================] - 1s 960us/step - loss: 0.2177 - accuracy: 0.5804 - val_loss: 0.3840 - val_accuracy: 0.5559\n",
      "Epoch 87/1600\n",
      "680/680 [==============================] - 1s 972us/step - loss: 0.2190 - accuracy: 0.5801 - val_loss: 0.3825 - val_accuracy: 0.5412\n",
      "Epoch 88/1600\n",
      "680/680 [==============================] - 1s 944us/step - loss: 0.2271 - accuracy: 0.5819 - val_loss: 0.4033 - val_accuracy: 0.5353\n",
      "Epoch 89/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2214 - accuracy: 0.5812 - val_loss: 0.3837 - val_accuracy: 0.5529\n",
      "Epoch 90/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.2239 - accuracy: 0.5808 - val_loss: 0.4272 - val_accuracy: 0.5500\n",
      "Epoch 91/1600\n",
      "680/680 [==============================] - 1s 937us/step - loss: 0.2136 - accuracy: 0.5793 - val_loss: 0.4014 - val_accuracy: 0.5471\n",
      "Epoch 92/1600\n",
      "680/680 [==============================] - 1s 956us/step - loss: 0.2184 - accuracy: 0.5815 - val_loss: 0.3698 - val_accuracy: 0.5412\n",
      "Epoch 93/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.2112 - accuracy: 0.5812 - val_loss: 0.4056 - val_accuracy: 0.5588\n",
      "Epoch 94/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.2226 - accuracy: 0.5801 - val_loss: 0.4066 - val_accuracy: 0.5353\n",
      "Epoch 95/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.2193 - accuracy: 0.5841 - val_loss: 0.4043 - val_accuracy: 0.5294\n",
      "Epoch 96/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.2155 - accuracy: 0.5808 - val_loss: 0.3946 - val_accuracy: 0.5441\n",
      "Epoch 97/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.2222 - accuracy: 0.5826 - val_loss: 0.4214 - val_accuracy: 0.5353\n",
      "Epoch 98/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.2149 - accuracy: 0.5819 - val_loss: 0.4042 - val_accuracy: 0.5324\n",
      "Epoch 99/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.2045 - accuracy: 0.5845 - val_loss: 0.4109 - val_accuracy: 0.5529\n",
      "Epoch 100/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.2122 - accuracy: 0.5859 - val_loss: 0.3907 - val_accuracy: 0.5412\n",
      "Epoch 101/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.2276 - accuracy: 0.5819 - val_loss: 0.3877 - val_accuracy: 0.5618\n",
      "Epoch 102/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.2059 - accuracy: 0.5834 - val_loss: 0.3941 - val_accuracy: 0.5618\n",
      "Epoch 103/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.2043 - accuracy: 0.5885 - val_loss: 0.4215 - val_accuracy: 0.5324\n",
      "Epoch 104/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.2141 - accuracy: 0.5834 - val_loss: 0.3926 - val_accuracy: 0.5500\n",
      "Epoch 105/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.2183 - accuracy: 0.5830 - val_loss: 0.4122 - val_accuracy: 0.5412\n",
      "Epoch 106/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.2093 - accuracy: 0.5834 - val_loss: 0.3987 - val_accuracy: 0.5588\n",
      "Epoch 107/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.2099 - accuracy: 0.5823 - val_loss: 0.4078 - val_accuracy: 0.5471\n",
      "Epoch 108/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.2029 - accuracy: 0.5841 - val_loss: 0.3944 - val_accuracy: 0.5588\n",
      "Epoch 109/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.2032 - accuracy: 0.5870 - val_loss: 0.4047 - val_accuracy: 0.5353\n",
      "Epoch 110/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.2129 - accuracy: 0.5863 - val_loss: 0.4032 - val_accuracy: 0.5471\n",
      "Epoch 111/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.2046 - accuracy: 0.5874 - val_loss: 0.4193 - val_accuracy: 0.5441\n",
      "Epoch 112/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.2114 - accuracy: 0.5819 - val_loss: 0.4086 - val_accuracy: 0.5441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/1600\n",
      "680/680 [==============================] - 1s 868us/step - loss: 0.2045 - accuracy: 0.5863 - val_loss: 0.3872 - val_accuracy: 0.5471\n",
      "Epoch 114/1600\n",
      "680/680 [==============================] - 1s 930us/step - loss: 0.2097 - accuracy: 0.5863 - val_loss: 0.3985 - val_accuracy: 0.5382\n",
      "Epoch 115/1600\n",
      "680/680 [==============================] - 1s 898us/step - loss: 0.2003 - accuracy: 0.5867 - val_loss: 0.3902 - val_accuracy: 0.5412\n",
      "Epoch 116/1600\n",
      "680/680 [==============================] - 1s 930us/step - loss: 0.1988 - accuracy: 0.5874 - val_loss: 0.3910 - val_accuracy: 0.5647\n",
      "Epoch 117/1600\n",
      "680/680 [==============================] - 1s 944us/step - loss: 0.1955 - accuracy: 0.5904 - val_loss: 0.4120 - val_accuracy: 0.5382\n",
      "Epoch 118/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.2250 - accuracy: 0.5830 - val_loss: 0.4140 - val_accuracy: 0.5529\n",
      "Epoch 119/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.2115 - accuracy: 0.5834 - val_loss: 0.4037 - val_accuracy: 0.5382\n",
      "Epoch 120/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.2042 - accuracy: 0.5867 - val_loss: 0.4339 - val_accuracy: 0.5324\n",
      "Epoch 121/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.2031 - accuracy: 0.5845 - val_loss: 0.3895 - val_accuracy: 0.5588\n",
      "Epoch 122/1600\n",
      "680/680 [==============================] - 1s 906us/step - loss: 0.2105 - accuracy: 0.5863 - val_loss: 0.4214 - val_accuracy: 0.5353\n",
      "Epoch 123/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.2090 - accuracy: 0.5859 - val_loss: 0.3894 - val_accuracy: 0.5353\n",
      "Epoch 124/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.2160 - accuracy: 0.5808 - val_loss: 0.3942 - val_accuracy: 0.5647\n",
      "Epoch 125/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.2058 - accuracy: 0.5874 - val_loss: 0.3905 - val_accuracy: 0.5412\n",
      "Epoch 126/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.2015 - accuracy: 0.5856 - val_loss: 0.3975 - val_accuracy: 0.5441\n",
      "Epoch 127/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.2222 - accuracy: 0.5823 - val_loss: 0.3354 - val_accuracy: 0.5500\n",
      "Epoch 128/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.2022 - accuracy: 0.5863 - val_loss: 0.3624 - val_accuracy: 0.5559\n",
      "Epoch 129/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1950 - accuracy: 0.5881 - val_loss: 0.3808 - val_accuracy: 0.5647\n",
      "Epoch 130/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.2038 - accuracy: 0.5863 - val_loss: 0.3874 - val_accuracy: 0.5324\n",
      "Epoch 131/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1936 - accuracy: 0.5881 - val_loss: 0.3657 - val_accuracy: 0.5441\n",
      "Epoch 132/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.2162 - accuracy: 0.5848 - val_loss: 0.3897 - val_accuracy: 0.5618\n",
      "Epoch 133/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1995 - accuracy: 0.5885 - val_loss: 0.3645 - val_accuracy: 0.5588\n",
      "Epoch 134/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1922 - accuracy: 0.5922 - val_loss: 0.3732 - val_accuracy: 0.5618\n",
      "Epoch 135/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.2032 - accuracy: 0.5874 - val_loss: 0.3922 - val_accuracy: 0.5471\n",
      "Epoch 136/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1978 - accuracy: 0.5870 - val_loss: 0.3879 - val_accuracy: 0.5441\n",
      "Epoch 137/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.2031 - accuracy: 0.5881 - val_loss: 0.4776 - val_accuracy: 0.5559\n",
      "Epoch 138/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.2122 - accuracy: 0.5874 - val_loss: 0.3801 - val_accuracy: 0.5353\n",
      "Epoch 139/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1976 - accuracy: 0.5896 - val_loss: 0.3680 - val_accuracy: 0.5559\n",
      "Epoch 140/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.2087 - accuracy: 0.5881 - val_loss: 0.3864 - val_accuracy: 0.5353\n",
      "Epoch 141/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.2061 - accuracy: 0.5870 - val_loss: 0.3692 - val_accuracy: 0.5471\n",
      "Epoch 142/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1980 - accuracy: 0.5878 - val_loss: 0.3753 - val_accuracy: 0.5588\n",
      "Epoch 143/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1995 - accuracy: 0.5878 - val_loss: 0.3745 - val_accuracy: 0.5529\n",
      "Epoch 144/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.2062 - accuracy: 0.5900 - val_loss: 0.3782 - val_accuracy: 0.5588\n",
      "Epoch 145/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1980 - accuracy: 0.5904 - val_loss: 0.3789 - val_accuracy: 0.5441\n",
      "Epoch 146/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.2078 - accuracy: 0.5878 - val_loss: 0.4264 - val_accuracy: 0.5353\n",
      "Epoch 147/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1980 - accuracy: 0.5881 - val_loss: 0.3960 - val_accuracy: 0.5559\n",
      "Epoch 148/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1996 - accuracy: 0.5874 - val_loss: 0.3872 - val_accuracy: 0.5441\n",
      "Epoch 149/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.2074 - accuracy: 0.5863 - val_loss: 0.3916 - val_accuracy: 0.5559\n",
      "Epoch 150/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.2135 - accuracy: 0.5863 - val_loss: 0.4011 - val_accuracy: 0.5441\n",
      "Epoch 151/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.2043 - accuracy: 0.5885 - val_loss: 0.3854 - val_accuracy: 0.5559\n",
      "Epoch 152/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1891 - accuracy: 0.5926 - val_loss: 0.4497 - val_accuracy: 0.5382\n",
      "Epoch 153/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1964 - accuracy: 0.5870 - val_loss: 0.4163 - val_accuracy: 0.5412\n",
      "Epoch 154/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1961 - accuracy: 0.5915 - val_loss: 0.3860 - val_accuracy: 0.5559\n",
      "Epoch 155/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.2259 - accuracy: 0.5837 - val_loss: 0.3902 - val_accuracy: 0.5500\n",
      "Epoch 156/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1960 - accuracy: 0.5881 - val_loss: 0.3838 - val_accuracy: 0.5735\n",
      "Epoch 157/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1856 - accuracy: 0.5940 - val_loss: 0.3835 - val_accuracy: 0.5500\n",
      "Epoch 158/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.2003 - accuracy: 0.5881 - val_loss: 0.4017 - val_accuracy: 0.5529\n",
      "Epoch 159/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1961 - accuracy: 0.5915 - val_loss: 0.3950 - val_accuracy: 0.5500\n",
      "Epoch 160/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.2058 - accuracy: 0.5863 - val_loss: 0.3833 - val_accuracy: 0.5529\n",
      "Epoch 161/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1999 - accuracy: 0.5915 - val_loss: 0.3982 - val_accuracy: 0.5412\n",
      "Epoch 162/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.2004 - accuracy: 0.5896 - val_loss: 0.3710 - val_accuracy: 0.5559\n",
      "Epoch 163/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1916 - accuracy: 0.5915 - val_loss: 0.4035 - val_accuracy: 0.5412\n",
      "Epoch 164/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.2098 - accuracy: 0.5874 - val_loss: 0.3712 - val_accuracy: 0.5382\n",
      "Epoch 165/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.2039 - accuracy: 0.5889 - val_loss: 0.3845 - val_accuracy: 0.5618\n",
      "Epoch 166/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.2048 - accuracy: 0.5896 - val_loss: 0.3991 - val_accuracy: 0.5647\n",
      "Epoch 167/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1985 - accuracy: 0.5881 - val_loss: 0.4466 - val_accuracy: 0.5294\n",
      "Epoch 168/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.2016 - accuracy: 0.5885 - val_loss: 0.3979 - val_accuracy: 0.5441\n",
      "Epoch 169/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1957 - accuracy: 0.5900 - val_loss: 0.3822 - val_accuracy: 0.5588\n",
      "Epoch 170/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1969 - accuracy: 0.5885 - val_loss: 0.3816 - val_accuracy: 0.5500\n",
      "Epoch 171/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1992 - accuracy: 0.5904 - val_loss: 0.3883 - val_accuracy: 0.5353\n",
      "Epoch 172/1600\n",
      "680/680 [==============================] - 1s 936us/step - loss: 0.1966 - accuracy: 0.5900 - val_loss: 0.4127 - val_accuracy: 0.5441\n",
      "Epoch 173/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1911 - accuracy: 0.5911 - val_loss: 0.3939 - val_accuracy: 0.5559\n",
      "Epoch 174/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.2050 - accuracy: 0.5874 - val_loss: 0.4280 - val_accuracy: 0.5382\n",
      "Epoch 175/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1936 - accuracy: 0.5907 - val_loss: 0.3839 - val_accuracy: 0.5382\n",
      "Epoch 176/1600\n",
      "680/680 [==============================] - 1s 868us/step - loss: 0.1906 - accuracy: 0.5904 - val_loss: 0.3910 - val_accuracy: 0.5559\n",
      "Epoch 177/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.2076 - accuracy: 0.5893 - val_loss: 0.3966 - val_accuracy: 0.5618\n",
      "Epoch 178/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1938 - accuracy: 0.5896 - val_loss: 0.3793 - val_accuracy: 0.5353\n",
      "Epoch 179/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.1896 - accuracy: 0.5915 - val_loss: 0.4130 - val_accuracy: 0.5412\n",
      "Epoch 180/1600\n",
      "680/680 [==============================] - 1s 937us/step - loss: 0.1893 - accuracy: 0.5933 - val_loss: 0.3753 - val_accuracy: 0.5500\n",
      "Epoch 181/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1968 - accuracy: 0.5896 - val_loss: 0.3578 - val_accuracy: 0.5588\n",
      "Epoch 182/1600\n",
      "680/680 [==============================] - 1s 934us/step - loss: 0.1970 - accuracy: 0.5904 - val_loss: 0.4275 - val_accuracy: 0.5706\n",
      "Epoch 183/1600\n",
      "680/680 [==============================] - 1s 936us/step - loss: 0.1985 - accuracy: 0.5904 - val_loss: 0.3834 - val_accuracy: 0.5500\n",
      "Epoch 184/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1900 - accuracy: 0.5911 - val_loss: 0.3757 - val_accuracy: 0.5559\n",
      "Epoch 185/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1954 - accuracy: 0.5900 - val_loss: 0.3953 - val_accuracy: 0.5529\n",
      "Epoch 186/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1840 - accuracy: 0.5944 - val_loss: 0.3900 - val_accuracy: 0.5471\n",
      "Epoch 187/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.1929 - accuracy: 0.5915 - val_loss: 0.4100 - val_accuracy: 0.5324\n",
      "Epoch 188/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1962 - accuracy: 0.5900 - val_loss: 0.3981 - val_accuracy: 0.5676\n",
      "Epoch 189/1600\n",
      "680/680 [==============================] - 1s 930us/step - loss: 0.1994 - accuracy: 0.5904 - val_loss: 0.4235 - val_accuracy: 0.5412\n",
      "Epoch 190/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1945 - accuracy: 0.5915 - val_loss: 0.4167 - val_accuracy: 0.5353\n",
      "Epoch 191/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.2030 - accuracy: 0.5878 - val_loss: 0.4345 - val_accuracy: 0.5471\n",
      "Epoch 192/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.2032 - accuracy: 0.5893 - val_loss: 0.4059 - val_accuracy: 0.5353\n",
      "Epoch 193/1600\n",
      "680/680 [==============================] - 1s 901us/step - loss: 0.1891 - accuracy: 0.5926 - val_loss: 0.4146 - val_accuracy: 0.5382\n",
      "Epoch 194/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1943 - accuracy: 0.5907 - val_loss: 0.4119 - val_accuracy: 0.5441\n",
      "Epoch 195/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.2009 - accuracy: 0.5885 - val_loss: 0.3838 - val_accuracy: 0.5529\n",
      "Epoch 196/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1911 - accuracy: 0.5926 - val_loss: 0.4254 - val_accuracy: 0.5559\n",
      "Epoch 197/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.2072 - accuracy: 0.5878 - val_loss: 0.3762 - val_accuracy: 0.5618\n",
      "Epoch 198/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1826 - accuracy: 0.5944 - val_loss: 0.4219 - val_accuracy: 0.5412\n",
      "Epoch 199/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1911 - accuracy: 0.5904 - val_loss: 0.4229 - val_accuracy: 0.5412\n",
      "Epoch 200/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.2023 - accuracy: 0.5900 - val_loss: 0.4404 - val_accuracy: 0.5382\n",
      "Epoch 201/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1990 - accuracy: 0.5893 - val_loss: 0.3863 - val_accuracy: 0.5500\n",
      "Epoch 202/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1923 - accuracy: 0.5907 - val_loss: 0.3625 - val_accuracy: 0.5618\n",
      "Epoch 203/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1815 - accuracy: 0.5929 - val_loss: 0.3833 - val_accuracy: 0.5559\n",
      "Epoch 204/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1927 - accuracy: 0.5900 - val_loss: 0.4124 - val_accuracy: 0.5441\n",
      "Epoch 205/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.2014 - accuracy: 0.5922 - val_loss: 0.3603 - val_accuracy: 0.5559\n",
      "Epoch 206/1600\n",
      "680/680 [==============================] - 1s 868us/step - loss: 0.1897 - accuracy: 0.5904 - val_loss: 0.4021 - val_accuracy: 0.5471\n",
      "Epoch 207/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1934 - accuracy: 0.5904 - val_loss: 0.3756 - val_accuracy: 0.5559\n",
      "Epoch 208/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1917 - accuracy: 0.5915 - val_loss: 0.3947 - val_accuracy: 0.5559\n",
      "Epoch 209/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.2082 - accuracy: 0.5874 - val_loss: 0.3886 - val_accuracy: 0.5353\n",
      "Epoch 210/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1948 - accuracy: 0.5889 - val_loss: 0.4285 - val_accuracy: 0.5324\n",
      "Epoch 211/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1940 - accuracy: 0.5896 - val_loss: 0.4531 - val_accuracy: 0.5412\n",
      "Epoch 212/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1955 - accuracy: 0.5926 - val_loss: 0.3800 - val_accuracy: 0.5559\n",
      "Epoch 213/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1964 - accuracy: 0.5915 - val_loss: 0.4021 - val_accuracy: 0.5441\n",
      "Epoch 214/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1855 - accuracy: 0.5926 - val_loss: 0.3654 - val_accuracy: 0.5500\n",
      "Epoch 215/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.2094 - accuracy: 0.5881 - val_loss: 0.4247 - val_accuracy: 0.5471\n",
      "Epoch 216/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1963 - accuracy: 0.5918 - val_loss: 0.3962 - val_accuracy: 0.5412\n",
      "Epoch 217/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1968 - accuracy: 0.5918 - val_loss: 0.3861 - val_accuracy: 0.5618\n",
      "Epoch 218/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1761 - accuracy: 0.5929 - val_loss: 0.3851 - val_accuracy: 0.5618\n",
      "Epoch 219/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1949 - accuracy: 0.5918 - val_loss: 0.4183 - val_accuracy: 0.5382\n",
      "Epoch 220/1600\n",
      "680/680 [==============================] - 1s 945us/step - loss: 0.1941 - accuracy: 0.5922 - val_loss: 0.4122 - val_accuracy: 0.5353\n",
      "Epoch 221/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1906 - accuracy: 0.5915 - val_loss: 0.3943 - val_accuracy: 0.5382\n",
      "Epoch 222/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1978 - accuracy: 0.5878 - val_loss: 0.4033 - val_accuracy: 0.5412\n",
      "Epoch 223/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 893us/step - loss: 0.1918 - accuracy: 0.5955 - val_loss: 0.3575 - val_accuracy: 0.5647\n",
      "Epoch 224/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1986 - accuracy: 0.5889 - val_loss: 0.4055 - val_accuracy: 0.5382\n",
      "Epoch 225/1600\n",
      "680/680 [==============================] - 1s 936us/step - loss: 0.1862 - accuracy: 0.5918 - val_loss: 0.3966 - val_accuracy: 0.5412\n",
      "Epoch 226/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.2007 - accuracy: 0.5889 - val_loss: 0.4059 - val_accuracy: 0.5441\n",
      "Epoch 227/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1900 - accuracy: 0.5922 - val_loss: 0.3566 - val_accuracy: 0.5588\n",
      "Epoch 228/1600\n",
      "680/680 [==============================] - 1s 901us/step - loss: 0.1778 - accuracy: 0.5966 - val_loss: 0.3464 - val_accuracy: 0.5588\n",
      "Epoch 229/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1970 - accuracy: 0.5918 - val_loss: 0.4444 - val_accuracy: 0.5206\n",
      "Epoch 230/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1863 - accuracy: 0.5926 - val_loss: 0.4104 - val_accuracy: 0.5441\n",
      "Epoch 231/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1875 - accuracy: 0.5940 - val_loss: 0.4229 - val_accuracy: 0.5471\n",
      "Epoch 232/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1925 - accuracy: 0.5900 - val_loss: 0.4041 - val_accuracy: 0.5471\n",
      "Epoch 233/1600\n",
      "680/680 [==============================] - 1s 904us/step - loss: 0.1966 - accuracy: 0.5937 - val_loss: 0.4135 - val_accuracy: 0.5500\n",
      "Epoch 234/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1884 - accuracy: 0.5937 - val_loss: 0.3991 - val_accuracy: 0.5441\n",
      "Epoch 235/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1821 - accuracy: 0.5922 - val_loss: 0.3904 - val_accuracy: 0.5559\n",
      "Epoch 236/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1978 - accuracy: 0.5929 - val_loss: 0.3734 - val_accuracy: 0.5588\n",
      "Epoch 237/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1897 - accuracy: 0.5918 - val_loss: 0.4235 - val_accuracy: 0.5412\n",
      "Epoch 238/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1931 - accuracy: 0.5929 - val_loss: 0.4222 - val_accuracy: 0.5412\n",
      "Epoch 239/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1886 - accuracy: 0.5926 - val_loss: 0.4650 - val_accuracy: 0.5147\n",
      "Epoch 240/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1836 - accuracy: 0.5933 - val_loss: 0.4173 - val_accuracy: 0.5382\n",
      "Epoch 241/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1888 - accuracy: 0.5922 - val_loss: 0.4000 - val_accuracy: 0.5559\n",
      "Epoch 242/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1954 - accuracy: 0.5929 - val_loss: 0.3609 - val_accuracy: 0.5588\n",
      "Epoch 243/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.1928 - accuracy: 0.5896 - val_loss: 0.3852 - val_accuracy: 0.5529\n",
      "Epoch 244/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1772 - accuracy: 0.5966 - val_loss: 0.4154 - val_accuracy: 0.5353\n",
      "Epoch 245/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1925 - accuracy: 0.5929 - val_loss: 0.4211 - val_accuracy: 0.5441\n",
      "Epoch 246/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1911 - accuracy: 0.5922 - val_loss: 0.4092 - val_accuracy: 0.5735\n",
      "Epoch 247/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1914 - accuracy: 0.5951 - val_loss: 0.3937 - val_accuracy: 0.5529\n",
      "Epoch 248/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1888 - accuracy: 0.5948 - val_loss: 0.4001 - val_accuracy: 0.5647\n",
      "Epoch 249/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1879 - accuracy: 0.5900 - val_loss: 0.4054 - val_accuracy: 0.5382\n",
      "Epoch 250/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.2009 - accuracy: 0.5896 - val_loss: 0.4114 - val_accuracy: 0.5441\n",
      "Epoch 251/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1873 - accuracy: 0.5911 - val_loss: 0.3837 - val_accuracy: 0.5441\n",
      "Epoch 252/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1790 - accuracy: 0.5940 - val_loss: 0.4081 - val_accuracy: 0.5559\n",
      "Epoch 253/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1993 - accuracy: 0.5918 - val_loss: 0.3984 - val_accuracy: 0.5529\n",
      "Epoch 254/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1943 - accuracy: 0.5915 - val_loss: 0.4138 - val_accuracy: 0.5471\n",
      "Epoch 255/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1927 - accuracy: 0.5915 - val_loss: 0.4004 - val_accuracy: 0.5588\n",
      "Epoch 256/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1969 - accuracy: 0.5893 - val_loss: 0.4085 - val_accuracy: 0.5441\n",
      "Epoch 257/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1799 - accuracy: 0.5929 - val_loss: 0.4127 - val_accuracy: 0.5735\n",
      "Epoch 258/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1846 - accuracy: 0.5959 - val_loss: 0.4018 - val_accuracy: 0.5500\n",
      "Epoch 259/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1866 - accuracy: 0.5970 - val_loss: 0.4116 - val_accuracy: 0.5382\n",
      "Epoch 260/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1961 - accuracy: 0.5915 - val_loss: 0.3725 - val_accuracy: 0.5441\n",
      "Epoch 261/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1840 - accuracy: 0.5944 - val_loss: 0.3933 - val_accuracy: 0.5559\n",
      "Epoch 262/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1904 - accuracy: 0.5951 - val_loss: 0.3737 - val_accuracy: 0.5588\n",
      "Epoch 263/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1868 - accuracy: 0.5933 - val_loss: 0.3845 - val_accuracy: 0.5500\n",
      "Epoch 264/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1737 - accuracy: 0.5974 - val_loss: 0.3805 - val_accuracy: 0.5588\n",
      "Epoch 265/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1994 - accuracy: 0.5929 - val_loss: 0.4052 - val_accuracy: 0.5412\n",
      "Epoch 266/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.1951 - accuracy: 0.5900 - val_loss: 0.3894 - val_accuracy: 0.5676\n",
      "Epoch 267/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1895 - accuracy: 0.5911 - val_loss: 0.3722 - val_accuracy: 0.5441\n",
      "Epoch 268/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1994 - accuracy: 0.5863 - val_loss: 0.3893 - val_accuracy: 0.5441\n",
      "Epoch 269/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1885 - accuracy: 0.5915 - val_loss: 0.3903 - val_accuracy: 0.5529\n",
      "Epoch 270/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1792 - accuracy: 0.5970 - val_loss: 0.3960 - val_accuracy: 0.5471\n",
      "Epoch 271/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1976 - accuracy: 0.5940 - val_loss: 0.3798 - val_accuracy: 0.5441\n",
      "Epoch 272/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1884 - accuracy: 0.5926 - val_loss: 0.4006 - val_accuracy: 0.5412\n",
      "Epoch 273/1600\n",
      "680/680 [==============================] - 1s 944us/step - loss: 0.1814 - accuracy: 0.5937 - val_loss: 0.3642 - val_accuracy: 0.5471\n",
      "Epoch 274/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.2124 - accuracy: 0.5848 - val_loss: 0.3969 - val_accuracy: 0.5412\n",
      "Epoch 275/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1959 - accuracy: 0.5904 - val_loss: 0.3689 - val_accuracy: 0.5471\n",
      "Epoch 276/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1942 - accuracy: 0.5911 - val_loss: 0.3881 - val_accuracy: 0.5559\n",
      "Epoch 277/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.1860 - accuracy: 0.5959 - val_loss: 0.4036 - val_accuracy: 0.5559\n",
      "Epoch 278/1600\n",
      "680/680 [==============================] - 1s 937us/step - loss: 0.1998 - accuracy: 0.5911 - val_loss: 0.3852 - val_accuracy: 0.5265\n",
      "Epoch 279/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1921 - accuracy: 0.5915 - val_loss: 0.4065 - val_accuracy: 0.5412\n",
      "Epoch 280/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1962 - accuracy: 0.5937 - val_loss: 0.3910 - val_accuracy: 0.5471\n",
      "Epoch 281/1600\n",
      "680/680 [==============================] - 1s 942us/step - loss: 0.1867 - accuracy: 0.5937 - val_loss: 0.4078 - val_accuracy: 0.5500\n",
      "Epoch 282/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1858 - accuracy: 0.5929 - val_loss: 0.3702 - val_accuracy: 0.5529\n",
      "Epoch 283/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1878 - accuracy: 0.5900 - val_loss: 0.4068 - val_accuracy: 0.5441\n",
      "Epoch 284/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1824 - accuracy: 0.5926 - val_loss: 0.3938 - val_accuracy: 0.5559\n",
      "Epoch 285/1600\n",
      "680/680 [==============================] - 1s 907us/step - loss: 0.1949 - accuracy: 0.5911 - val_loss: 0.3760 - val_accuracy: 0.5618\n",
      "Epoch 286/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1880 - accuracy: 0.5926 - val_loss: 0.3998 - val_accuracy: 0.5441\n",
      "Epoch 287/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.2003 - accuracy: 0.5907 - val_loss: 0.4234 - val_accuracy: 0.5441\n",
      "Epoch 288/1600\n",
      "680/680 [==============================] - 1s 935us/step - loss: 0.1831 - accuracy: 0.5944 - val_loss: 0.3721 - val_accuracy: 0.5588\n",
      "Epoch 289/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.1781 - accuracy: 0.5959 - val_loss: 0.3982 - val_accuracy: 0.5441\n",
      "Epoch 290/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1826 - accuracy: 0.5922 - val_loss: 0.3545 - val_accuracy: 0.5529\n",
      "Epoch 291/1600\n",
      "680/680 [==============================] - 1s 930us/step - loss: 0.2043 - accuracy: 0.5900 - val_loss: 0.3715 - val_accuracy: 0.5529\n",
      "Epoch 292/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1880 - accuracy: 0.5933 - val_loss: 0.3814 - val_accuracy: 0.5471\n",
      "Epoch 293/1600\n",
      "680/680 [==============================] - 1s 937us/step - loss: 0.1803 - accuracy: 0.5926 - val_loss: 0.3901 - val_accuracy: 0.5529\n",
      "Epoch 294/1600\n",
      "680/680 [==============================] - 1s 902us/step - loss: 0.1913 - accuracy: 0.5933 - val_loss: 0.3901 - val_accuracy: 0.5441\n",
      "Epoch 295/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1837 - accuracy: 0.5948 - val_loss: 0.4000 - val_accuracy: 0.5382\n",
      "Epoch 296/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1899 - accuracy: 0.5926 - val_loss: 0.3737 - val_accuracy: 0.5529\n",
      "Epoch 297/1600\n",
      "680/680 [==============================] - 1s 938us/step - loss: 0.1874 - accuracy: 0.5904 - val_loss: 0.3792 - val_accuracy: 0.5559\n",
      "Epoch 298/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.2016 - accuracy: 0.5896 - val_loss: 0.3923 - val_accuracy: 0.5353\n",
      "Epoch 299/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1846 - accuracy: 0.5922 - val_loss: 0.4066 - val_accuracy: 0.5353\n",
      "Epoch 300/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1811 - accuracy: 0.5911 - val_loss: 0.3899 - val_accuracy: 0.5382\n",
      "Epoch 301/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1850 - accuracy: 0.5911 - val_loss: 0.4124 - val_accuracy: 0.5441\n",
      "Epoch 302/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1936 - accuracy: 0.5915 - val_loss: 0.3852 - val_accuracy: 0.5529\n",
      "Epoch 303/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1871 - accuracy: 0.5911 - val_loss: 0.4034 - val_accuracy: 0.5471\n",
      "Epoch 304/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1811 - accuracy: 0.5951 - val_loss: 0.4038 - val_accuracy: 0.5588\n",
      "Epoch 305/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1790 - accuracy: 0.5948 - val_loss: 0.4258 - val_accuracy: 0.5382\n",
      "Epoch 306/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1937 - accuracy: 0.5926 - val_loss: 0.3780 - val_accuracy: 0.5529\n",
      "Epoch 307/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1841 - accuracy: 0.5926 - val_loss: 0.3741 - val_accuracy: 0.5529\n",
      "Epoch 308/1600\n",
      "680/680 [==============================] - 1s 901us/step - loss: 0.1857 - accuracy: 0.5937 - val_loss: 0.3835 - val_accuracy: 0.5588\n",
      "Epoch 309/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1808 - accuracy: 0.5944 - val_loss: 0.3795 - val_accuracy: 0.5559\n",
      "Epoch 310/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1950 - accuracy: 0.5929 - val_loss: 0.3588 - val_accuracy: 0.5529\n",
      "Epoch 311/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1873 - accuracy: 0.5922 - val_loss: 0.4257 - val_accuracy: 0.5412\n",
      "Epoch 312/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1808 - accuracy: 0.5937 - val_loss: 0.3926 - val_accuracy: 0.5529\n",
      "Epoch 313/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1882 - accuracy: 0.5933 - val_loss: 0.3937 - val_accuracy: 0.5441\n",
      "Epoch 314/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1822 - accuracy: 0.5933 - val_loss: 0.3908 - val_accuracy: 0.5529\n",
      "Epoch 315/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1827 - accuracy: 0.5940 - val_loss: 0.3461 - val_accuracy: 0.5588\n",
      "Epoch 316/1600\n",
      "680/680 [==============================] - 1s 902us/step - loss: 0.1866 - accuracy: 0.5948 - val_loss: 0.3723 - val_accuracy: 0.5706\n",
      "Epoch 317/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1874 - accuracy: 0.5951 - val_loss: 0.3852 - val_accuracy: 0.5441\n",
      "Epoch 318/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1956 - accuracy: 0.5918 - val_loss: 0.3721 - val_accuracy: 0.5647\n",
      "Epoch 319/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1791 - accuracy: 0.5940 - val_loss: 0.3581 - val_accuracy: 0.5588\n",
      "Epoch 320/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1875 - accuracy: 0.5937 - val_loss: 0.3951 - val_accuracy: 0.5412\n",
      "Epoch 321/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.2086 - accuracy: 0.5900 - val_loss: 0.3685 - val_accuracy: 0.5412\n",
      "Epoch 322/1600\n",
      "680/680 [==============================] - 1s 904us/step - loss: 0.1893 - accuracy: 0.5929 - val_loss: 0.4105 - val_accuracy: 0.5265\n",
      "Epoch 323/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1843 - accuracy: 0.5937 - val_loss: 0.3839 - val_accuracy: 0.5588\n",
      "Epoch 324/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1872 - accuracy: 0.5937 - val_loss: 0.3734 - val_accuracy: 0.5529\n",
      "Epoch 325/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1895 - accuracy: 0.5944 - val_loss: 0.3655 - val_accuracy: 0.5559\n",
      "Epoch 326/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1908 - accuracy: 0.5926 - val_loss: 0.3692 - val_accuracy: 0.5529\n",
      "Epoch 327/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1835 - accuracy: 0.5929 - val_loss: 0.3657 - val_accuracy: 0.5559\n",
      "Epoch 328/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1815 - accuracy: 0.5929 - val_loss: 0.3957 - val_accuracy: 0.5529\n",
      "Epoch 329/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.1800 - accuracy: 0.5940 - val_loss: 0.3877 - val_accuracy: 0.5500\n",
      "Epoch 330/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.2048 - accuracy: 0.5893 - val_loss: 0.3868 - val_accuracy: 0.5382\n",
      "Epoch 331/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1753 - accuracy: 0.5955 - val_loss: 0.4092 - val_accuracy: 0.5324\n",
      "Epoch 332/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1965 - accuracy: 0.5907 - val_loss: 0.4008 - val_accuracy: 0.5471\n",
      "Epoch 333/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 878us/step - loss: 0.1706 - accuracy: 0.5962 - val_loss: 0.3606 - val_accuracy: 0.5471\n",
      "Epoch 334/1600\n",
      "680/680 [==============================] - 1s 898us/step - loss: 0.1797 - accuracy: 0.5959 - val_loss: 0.4249 - val_accuracy: 0.5324\n",
      "Epoch 335/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.2065 - accuracy: 0.5904 - val_loss: 0.3632 - val_accuracy: 0.5529\n",
      "Epoch 336/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1858 - accuracy: 0.5918 - val_loss: 0.3783 - val_accuracy: 0.5441\n",
      "Epoch 337/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1811 - accuracy: 0.5948 - val_loss: 0.4058 - val_accuracy: 0.5588\n",
      "Epoch 338/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1921 - accuracy: 0.5915 - val_loss: 0.3495 - val_accuracy: 0.5588\n",
      "Epoch 339/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1772 - accuracy: 0.5937 - val_loss: 0.4115 - val_accuracy: 0.5353\n",
      "Epoch 340/1600\n",
      "680/680 [==============================] - 1s 938us/step - loss: 0.1839 - accuracy: 0.5955 - val_loss: 0.3691 - val_accuracy: 0.5618\n",
      "Epoch 341/1600\n",
      "680/680 [==============================] - 1s 934us/step - loss: 0.1904 - accuracy: 0.5926 - val_loss: 0.3659 - val_accuracy: 0.5471\n",
      "Epoch 342/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1928 - accuracy: 0.5907 - val_loss: 0.4001 - val_accuracy: 0.5412\n",
      "Epoch 343/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1818 - accuracy: 0.5926 - val_loss: 0.4176 - val_accuracy: 0.5382\n",
      "Epoch 344/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.1859 - accuracy: 0.5926 - val_loss: 0.3840 - val_accuracy: 0.5471\n",
      "Epoch 345/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.1821 - accuracy: 0.5937 - val_loss: 0.4363 - val_accuracy: 0.5588\n",
      "Epoch 346/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.2020 - accuracy: 0.5922 - val_loss: 0.3601 - val_accuracy: 0.5529\n",
      "Epoch 347/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1749 - accuracy: 0.5962 - val_loss: 0.3986 - val_accuracy: 0.5324\n",
      "Epoch 348/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1745 - accuracy: 0.5970 - val_loss: 0.3881 - val_accuracy: 0.5500\n",
      "Epoch 349/1600\n",
      "680/680 [==============================] - 1s 934us/step - loss: 0.2002 - accuracy: 0.5915 - val_loss: 0.4014 - val_accuracy: 0.5441\n",
      "Epoch 350/1600\n",
      "680/680 [==============================] - 1s 938us/step - loss: 0.1807 - accuracy: 0.5926 - val_loss: 0.3888 - val_accuracy: 0.5471\n",
      "Epoch 351/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1882 - accuracy: 0.5929 - val_loss: 0.3659 - val_accuracy: 0.5500\n",
      "Epoch 352/1600\n",
      "680/680 [==============================] - 1s 934us/step - loss: 0.1788 - accuracy: 0.5966 - val_loss: 0.3830 - val_accuracy: 0.5559\n",
      "Epoch 353/1600\n",
      "680/680 [==============================] - 1s 937us/step - loss: 0.1868 - accuracy: 0.5940 - val_loss: 0.3670 - val_accuracy: 0.5529\n",
      "Epoch 354/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1927 - accuracy: 0.5907 - val_loss: 0.3862 - val_accuracy: 0.5353\n",
      "Epoch 355/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1992 - accuracy: 0.5896 - val_loss: 0.4490 - val_accuracy: 0.5324\n",
      "Epoch 356/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1920 - accuracy: 0.5944 - val_loss: 0.3731 - val_accuracy: 0.5441\n",
      "Epoch 357/1600\n",
      "680/680 [==============================] - 1s 938us/step - loss: 0.1862 - accuracy: 0.5940 - val_loss: 0.3327 - val_accuracy: 0.5559\n",
      "Epoch 358/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1807 - accuracy: 0.5937 - val_loss: 0.3821 - val_accuracy: 0.5559\n",
      "Epoch 359/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1841 - accuracy: 0.5937 - val_loss: 0.4111 - val_accuracy: 0.5559\n",
      "Epoch 360/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.2009 - accuracy: 0.5881 - val_loss: 0.3902 - val_accuracy: 0.5471\n",
      "Epoch 361/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.1797 - accuracy: 0.5955 - val_loss: 0.4074 - val_accuracy: 0.5559\n",
      "Epoch 362/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1940 - accuracy: 0.5915 - val_loss: 0.3616 - val_accuracy: 0.5559\n",
      "Epoch 363/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1795 - accuracy: 0.5955 - val_loss: 0.3569 - val_accuracy: 0.5441\n",
      "Epoch 364/1600\n",
      "680/680 [==============================] - 1s 864us/step - loss: 0.1879 - accuracy: 0.5948 - val_loss: 0.3768 - val_accuracy: 0.5529\n",
      "Epoch 365/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1837 - accuracy: 0.5959 - val_loss: 0.3696 - val_accuracy: 0.5588\n",
      "Epoch 366/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1989 - accuracy: 0.5915 - val_loss: 0.4547 - val_accuracy: 0.5382\n",
      "Epoch 367/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1832 - accuracy: 0.5922 - val_loss: 0.3806 - val_accuracy: 0.5588\n",
      "Epoch 368/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1815 - accuracy: 0.5951 - val_loss: 0.3988 - val_accuracy: 0.5382\n",
      "Epoch 369/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1839 - accuracy: 0.5907 - val_loss: 0.4112 - val_accuracy: 0.5412\n",
      "Epoch 370/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1988 - accuracy: 0.5911 - val_loss: 0.3967 - val_accuracy: 0.5412\n",
      "Epoch 371/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1790 - accuracy: 0.5948 - val_loss: 0.3605 - val_accuracy: 0.5559\n",
      "Epoch 372/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1814 - accuracy: 0.5951 - val_loss: 0.4045 - val_accuracy: 0.5382\n",
      "Epoch 373/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1842 - accuracy: 0.5918 - val_loss: 0.3926 - val_accuracy: 0.5471\n",
      "Epoch 374/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1922 - accuracy: 0.5918 - val_loss: 0.3699 - val_accuracy: 0.5559\n",
      "Epoch 375/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.2021 - accuracy: 0.5893 - val_loss: 0.4441 - val_accuracy: 0.5294\n",
      "Epoch 376/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1769 - accuracy: 0.5955 - val_loss: 0.4224 - val_accuracy: 0.5382\n",
      "Epoch 377/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1858 - accuracy: 0.5915 - val_loss: 0.4358 - val_accuracy: 0.5500\n",
      "Epoch 378/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1855 - accuracy: 0.5944 - val_loss: 0.3743 - val_accuracy: 0.5588\n",
      "Epoch 379/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1924 - accuracy: 0.5907 - val_loss: 0.3883 - val_accuracy: 0.5559\n",
      "Epoch 380/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1897 - accuracy: 0.5926 - val_loss: 0.4037 - val_accuracy: 0.5500\n",
      "Epoch 381/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1955 - accuracy: 0.5911 - val_loss: 0.4079 - val_accuracy: 0.5500\n",
      "Epoch 382/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1807 - accuracy: 0.5948 - val_loss: 0.3831 - val_accuracy: 0.5588\n",
      "Epoch 383/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1995 - accuracy: 0.5893 - val_loss: 0.3839 - val_accuracy: 0.5471\n",
      "Epoch 384/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1978 - accuracy: 0.5915 - val_loss: 0.3795 - val_accuracy: 0.5559\n",
      "Epoch 385/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1730 - accuracy: 0.5951 - val_loss: 0.4017 - val_accuracy: 0.5500\n",
      "Epoch 386/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1853 - accuracy: 0.5944 - val_loss: 0.3851 - val_accuracy: 0.5588\n",
      "Epoch 387/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1816 - accuracy: 0.5951 - val_loss: 0.3798 - val_accuracy: 0.5500\n",
      "Epoch 388/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1863 - accuracy: 0.5944 - val_loss: 0.3772 - val_accuracy: 0.5529\n",
      "Epoch 389/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1932 - accuracy: 0.5893 - val_loss: 0.3905 - val_accuracy: 0.5529\n",
      "Epoch 390/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1768 - accuracy: 0.5974 - val_loss: 0.3864 - val_accuracy: 0.5529\n",
      "Epoch 391/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1830 - accuracy: 0.5959 - val_loss: 0.4090 - val_accuracy: 0.5500\n",
      "Epoch 392/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1838 - accuracy: 0.5937 - val_loss: 0.3959 - val_accuracy: 0.5529\n",
      "Epoch 393/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1858 - accuracy: 0.5937 - val_loss: 0.4040 - val_accuracy: 0.5441\n",
      "Epoch 394/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.2082 - accuracy: 0.5881 - val_loss: 0.3743 - val_accuracy: 0.5500\n",
      "Epoch 395/1600\n",
      "680/680 [==============================] - 1s 906us/step - loss: 0.1784 - accuracy: 0.5937 - val_loss: 0.3959 - val_accuracy: 0.5588\n",
      "Epoch 396/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.2001 - accuracy: 0.5918 - val_loss: 0.3849 - val_accuracy: 0.5500\n",
      "Epoch 397/1600\n",
      "680/680 [==============================] - 1s 937us/step - loss: 0.1793 - accuracy: 0.5951 - val_loss: 0.3540 - val_accuracy: 0.5618\n",
      "Epoch 398/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1850 - accuracy: 0.5933 - val_loss: 0.4182 - val_accuracy: 0.5353\n",
      "Epoch 399/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1803 - accuracy: 0.5951 - val_loss: 0.3681 - val_accuracy: 0.5412\n",
      "Epoch 400/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.2002 - accuracy: 0.5889 - val_loss: 0.4097 - val_accuracy: 0.5500\n",
      "Epoch 401/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1761 - accuracy: 0.5951 - val_loss: 0.3709 - val_accuracy: 0.5529\n",
      "Epoch 402/1600\n",
      "680/680 [==============================] - 1s 898us/step - loss: 0.1883 - accuracy: 0.5926 - val_loss: 0.3670 - val_accuracy: 0.5529\n",
      "Epoch 403/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1828 - accuracy: 0.5940 - val_loss: 0.3746 - val_accuracy: 0.5412\n",
      "Epoch 404/1600\n",
      "680/680 [==============================] - 1s 939us/step - loss: 0.1834 - accuracy: 0.5929 - val_loss: 0.4290 - val_accuracy: 0.5382\n",
      "Epoch 405/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1829 - accuracy: 0.5929 - val_loss: 0.3935 - val_accuracy: 0.5412\n",
      "Epoch 406/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1710 - accuracy: 0.5959 - val_loss: 0.3801 - val_accuracy: 0.5559\n",
      "Epoch 407/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1894 - accuracy: 0.5922 - val_loss: 0.3800 - val_accuracy: 0.5500\n",
      "Epoch 408/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1819 - accuracy: 0.5966 - val_loss: 0.3884 - val_accuracy: 0.5529\n",
      "Epoch 409/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1891 - accuracy: 0.5940 - val_loss: 0.4000 - val_accuracy: 0.5471\n",
      "Epoch 410/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1880 - accuracy: 0.5915 - val_loss: 0.3669 - val_accuracy: 0.5382\n",
      "Epoch 411/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1855 - accuracy: 0.5929 - val_loss: 0.3882 - val_accuracy: 0.5412\n",
      "Epoch 412/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1862 - accuracy: 0.5918 - val_loss: 0.3861 - val_accuracy: 0.5529\n",
      "Epoch 413/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1919 - accuracy: 0.5904 - val_loss: 0.3956 - val_accuracy: 0.5559\n",
      "Epoch 414/1600\n",
      "680/680 [==============================] - 1s 967us/step - loss: 0.1760 - accuracy: 0.5951 - val_loss: 0.3764 - val_accuracy: 0.5441\n",
      "Epoch 415/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1807 - accuracy: 0.5937 - val_loss: 0.3618 - val_accuracy: 0.5559\n",
      "Epoch 416/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1792 - accuracy: 0.5948 - val_loss: 0.3967 - val_accuracy: 0.5529\n",
      "Epoch 417/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1787 - accuracy: 0.5970 - val_loss: 0.4128 - val_accuracy: 0.5471\n",
      "Epoch 418/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.2033 - accuracy: 0.5889 - val_loss: 0.4257 - val_accuracy: 0.5676\n",
      "Epoch 419/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1785 - accuracy: 0.5959 - val_loss: 0.4014 - val_accuracy: 0.5353\n",
      "Epoch 420/1600\n",
      "680/680 [==============================] - 1s 904us/step - loss: 0.1931 - accuracy: 0.5933 - val_loss: 0.3962 - val_accuracy: 0.5441\n",
      "Epoch 421/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1797 - accuracy: 0.5955 - val_loss: 0.3784 - val_accuracy: 0.5559\n",
      "Epoch 422/1600\n",
      "680/680 [==============================] - 1s 898us/step - loss: 0.1887 - accuracy: 0.5926 - val_loss: 0.4151 - val_accuracy: 0.5529\n",
      "Epoch 423/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1867 - accuracy: 0.5937 - val_loss: 0.4388 - val_accuracy: 0.5324\n",
      "Epoch 424/1600\n",
      "680/680 [==============================] - 1s 945us/step - loss: 0.1905 - accuracy: 0.5900 - val_loss: 0.3532 - val_accuracy: 0.5588\n",
      "Epoch 425/1600\n",
      "680/680 [==============================] - 1s 935us/step - loss: 0.1761 - accuracy: 0.5966 - val_loss: 0.3890 - val_accuracy: 0.5529\n",
      "Epoch 426/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1781 - accuracy: 0.5959 - val_loss: 0.3892 - val_accuracy: 0.5500\n",
      "Epoch 427/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1768 - accuracy: 0.5948 - val_loss: 0.3909 - val_accuracy: 0.5529\n",
      "Epoch 428/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1842 - accuracy: 0.5926 - val_loss: 0.3990 - val_accuracy: 0.5441\n",
      "Epoch 429/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.1819 - accuracy: 0.5933 - val_loss: 0.4123 - val_accuracy: 0.5382\n",
      "Epoch 430/1600\n",
      "680/680 [==============================] - 1s 935us/step - loss: 0.1849 - accuracy: 0.5962 - val_loss: 0.3624 - val_accuracy: 0.5441\n",
      "Epoch 431/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1823 - accuracy: 0.5940 - val_loss: 0.3992 - val_accuracy: 0.5412\n",
      "Epoch 432/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1821 - accuracy: 0.5922 - val_loss: 0.4020 - val_accuracy: 0.5559\n",
      "Epoch 433/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1930 - accuracy: 0.5896 - val_loss: 0.3553 - val_accuracy: 0.5559\n",
      "Epoch 434/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1788 - accuracy: 0.5944 - val_loss: 0.4541 - val_accuracy: 0.5265\n",
      "Epoch 435/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.1991 - accuracy: 0.5889 - val_loss: 0.4156 - val_accuracy: 0.5647\n",
      "Epoch 436/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1806 - accuracy: 0.5944 - val_loss: 0.3862 - val_accuracy: 0.5441\n",
      "Epoch 437/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1820 - accuracy: 0.5951 - val_loss: 0.4483 - val_accuracy: 0.5353\n",
      "Epoch 438/1600\n",
      "680/680 [==============================] - 1s 903us/step - loss: 0.1875 - accuracy: 0.5926 - val_loss: 0.3772 - val_accuracy: 0.5441\n",
      "Epoch 439/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1984 - accuracy: 0.5911 - val_loss: 0.3910 - val_accuracy: 0.5559\n",
      "Epoch 440/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1957 - accuracy: 0.5918 - val_loss: 0.4455 - val_accuracy: 0.5353\n",
      "Epoch 441/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1905 - accuracy: 0.5889 - val_loss: 0.3897 - val_accuracy: 0.5529\n",
      "Epoch 442/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1794 - accuracy: 0.5966 - val_loss: 0.4210 - val_accuracy: 0.5353\n",
      "Epoch 443/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 864us/step - loss: 0.1982 - accuracy: 0.5948 - val_loss: 0.3804 - val_accuracy: 0.5529\n",
      "Epoch 444/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.1785 - accuracy: 0.5962 - val_loss: 0.3828 - val_accuracy: 0.5559\n",
      "Epoch 445/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1900 - accuracy: 0.5940 - val_loss: 0.3946 - val_accuracy: 0.5382\n",
      "Epoch 446/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1851 - accuracy: 0.5940 - val_loss: 0.3982 - val_accuracy: 0.5441\n",
      "Epoch 447/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1775 - accuracy: 0.5948 - val_loss: 0.4004 - val_accuracy: 0.5471\n",
      "Epoch 448/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1812 - accuracy: 0.5966 - val_loss: 0.3846 - val_accuracy: 0.5529\n",
      "Epoch 449/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1873 - accuracy: 0.5937 - val_loss: 0.4189 - val_accuracy: 0.5265\n",
      "Epoch 450/1600\n",
      "680/680 [==============================] - 1s 902us/step - loss: 0.1856 - accuracy: 0.5922 - val_loss: 0.4652 - val_accuracy: 0.5765\n",
      "Epoch 451/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1876 - accuracy: 0.5948 - val_loss: 0.3856 - val_accuracy: 0.5529\n",
      "Epoch 452/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1879 - accuracy: 0.5929 - val_loss: 0.3812 - val_accuracy: 0.5500\n",
      "Epoch 453/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1877 - accuracy: 0.5904 - val_loss: 0.4097 - val_accuracy: 0.5500\n",
      "Epoch 454/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1756 - accuracy: 0.5977 - val_loss: 0.3840 - val_accuracy: 0.5471\n",
      "Epoch 455/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1885 - accuracy: 0.5922 - val_loss: 0.3922 - val_accuracy: 0.5471\n",
      "Epoch 456/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1901 - accuracy: 0.5918 - val_loss: 0.4038 - val_accuracy: 0.5500\n",
      "Epoch 457/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.1858 - accuracy: 0.5907 - val_loss: 0.3665 - val_accuracy: 0.5618\n",
      "Epoch 458/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1875 - accuracy: 0.5944 - val_loss: 0.3616 - val_accuracy: 0.5500\n",
      "Epoch 459/1600\n",
      "680/680 [==============================] - 1s 941us/step - loss: 0.1864 - accuracy: 0.5929 - val_loss: 0.3967 - val_accuracy: 0.5412\n",
      "Epoch 460/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1921 - accuracy: 0.5911 - val_loss: 0.4016 - val_accuracy: 0.5618\n",
      "Epoch 461/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1681 - accuracy: 0.5992 - val_loss: 0.4216 - val_accuracy: 0.5412\n",
      "Epoch 462/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1871 - accuracy: 0.5929 - val_loss: 0.3939 - val_accuracy: 0.5618\n",
      "Epoch 463/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1736 - accuracy: 0.5977 - val_loss: 0.3916 - val_accuracy: 0.5412\n",
      "Epoch 464/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1982 - accuracy: 0.5904 - val_loss: 0.3855 - val_accuracy: 0.5500\n",
      "Epoch 465/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1813 - accuracy: 0.5948 - val_loss: 0.3803 - val_accuracy: 0.5471\n",
      "Epoch 466/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1777 - accuracy: 0.5962 - val_loss: 0.3703 - val_accuracy: 0.5559\n",
      "Epoch 467/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1740 - accuracy: 0.5959 - val_loss: 0.3815 - val_accuracy: 0.5529\n",
      "Epoch 468/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1776 - accuracy: 0.5962 - val_loss: 0.3889 - val_accuracy: 0.5500\n",
      "Epoch 469/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1803 - accuracy: 0.5944 - val_loss: 0.3812 - val_accuracy: 0.5500\n",
      "Epoch 470/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1899 - accuracy: 0.5929 - val_loss: 0.3478 - val_accuracy: 0.5618\n",
      "Epoch 471/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1886 - accuracy: 0.5922 - val_loss: 0.4293 - val_accuracy: 0.5235\n",
      "Epoch 472/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1766 - accuracy: 0.5944 - val_loss: 0.3974 - val_accuracy: 0.5471\n",
      "Epoch 473/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.1833 - accuracy: 0.5944 - val_loss: 0.4609 - val_accuracy: 0.5294\n",
      "Epoch 474/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.1935 - accuracy: 0.5904 - val_loss: 0.4028 - val_accuracy: 0.5647\n",
      "Epoch 475/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1918 - accuracy: 0.5933 - val_loss: 0.3839 - val_accuracy: 0.5559\n",
      "Epoch 476/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1837 - accuracy: 0.5962 - val_loss: 0.4187 - val_accuracy: 0.5265\n",
      "Epoch 477/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1714 - accuracy: 0.5985 - val_loss: 0.4251 - val_accuracy: 0.5559\n",
      "Epoch 478/1600\n",
      "680/680 [==============================] - 1s 898us/step - loss: 0.1874 - accuracy: 0.5944 - val_loss: 0.4018 - val_accuracy: 0.5382\n",
      "Epoch 479/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.1857 - accuracy: 0.5926 - val_loss: 0.4051 - val_accuracy: 0.5471\n",
      "Epoch 480/1600\n",
      "680/680 [==============================] - 1s 906us/step - loss: 0.1911 - accuracy: 0.5911 - val_loss: 0.3713 - val_accuracy: 0.5529\n",
      "Epoch 481/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1910 - accuracy: 0.5933 - val_loss: 0.3900 - val_accuracy: 0.5500\n",
      "Epoch 482/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1815 - accuracy: 0.5948 - val_loss: 0.4020 - val_accuracy: 0.5353\n",
      "Epoch 483/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1724 - accuracy: 0.5970 - val_loss: 0.4114 - val_accuracy: 0.5382\n",
      "Epoch 484/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1830 - accuracy: 0.5937 - val_loss: 0.3673 - val_accuracy: 0.5529\n",
      "Epoch 485/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1951 - accuracy: 0.5937 - val_loss: 0.4253 - val_accuracy: 0.5382\n",
      "Epoch 486/1600\n",
      "680/680 [==============================] - 1s 953us/step - loss: 0.1766 - accuracy: 0.5962 - val_loss: 0.3756 - val_accuracy: 0.5441\n",
      "Epoch 487/1600\n",
      "680/680 [==============================] - 1s 939us/step - loss: 0.1874 - accuracy: 0.5937 - val_loss: 0.4081 - val_accuracy: 0.5471\n",
      "Epoch 488/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.1800 - accuracy: 0.5944 - val_loss: 0.4180 - val_accuracy: 0.5412\n",
      "Epoch 489/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1794 - accuracy: 0.5940 - val_loss: 0.4030 - val_accuracy: 0.5441\n",
      "Epoch 490/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1754 - accuracy: 0.5970 - val_loss: 0.4066 - val_accuracy: 0.5471\n",
      "Epoch 491/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1899 - accuracy: 0.5955 - val_loss: 0.4233 - val_accuracy: 0.5353\n",
      "Epoch 492/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1825 - accuracy: 0.5922 - val_loss: 0.3792 - val_accuracy: 0.5588\n",
      "Epoch 493/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1832 - accuracy: 0.5951 - val_loss: 0.3860 - val_accuracy: 0.5500\n",
      "Epoch 494/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1880 - accuracy: 0.5926 - val_loss: 0.4532 - val_accuracy: 0.5294\n",
      "Epoch 495/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1819 - accuracy: 0.5933 - val_loss: 0.4056 - val_accuracy: 0.5441\n",
      "Epoch 496/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1892 - accuracy: 0.5951 - val_loss: 0.4318 - val_accuracy: 0.5412\n",
      "Epoch 497/1600\n",
      "680/680 [==============================] - 1s 901us/step - loss: 0.1808 - accuracy: 0.5962 - val_loss: 0.3894 - val_accuracy: 0.5676\n",
      "Epoch 498/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1893 - accuracy: 0.5922 - val_loss: 0.4154 - val_accuracy: 0.5412\n",
      "Epoch 499/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1869 - accuracy: 0.5944 - val_loss: 0.3981 - val_accuracy: 0.5706\n",
      "Epoch 500/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.1930 - accuracy: 0.5940 - val_loss: 0.4119 - val_accuracy: 0.5265\n",
      "Epoch 501/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1789 - accuracy: 0.5948 - val_loss: 0.4227 - val_accuracy: 0.5412\n",
      "Epoch 502/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1846 - accuracy: 0.5940 - val_loss: 0.3809 - val_accuracy: 0.5588\n",
      "Epoch 503/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1804 - accuracy: 0.5940 - val_loss: 0.4253 - val_accuracy: 0.5412\n",
      "Epoch 504/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1796 - accuracy: 0.5944 - val_loss: 0.3801 - val_accuracy: 0.5618\n",
      "Epoch 505/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1891 - accuracy: 0.5915 - val_loss: 0.4317 - val_accuracy: 0.5471\n",
      "Epoch 506/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1927 - accuracy: 0.5922 - val_loss: 0.3984 - val_accuracy: 0.5618\n",
      "Epoch 507/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1838 - accuracy: 0.5922 - val_loss: 0.3895 - val_accuracy: 0.5500\n",
      "Epoch 508/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1850 - accuracy: 0.5926 - val_loss: 0.4122 - val_accuracy: 0.5441\n",
      "Epoch 509/1600\n",
      "680/680 [==============================] - 1s 936us/step - loss: 0.1875 - accuracy: 0.5926 - val_loss: 0.4051 - val_accuracy: 0.5382\n",
      "Epoch 510/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1788 - accuracy: 0.5966 - val_loss: 0.4038 - val_accuracy: 0.5559\n",
      "Epoch 511/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1675 - accuracy: 0.5981 - val_loss: 0.4000 - val_accuracy: 0.5412\n",
      "Epoch 512/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1839 - accuracy: 0.5944 - val_loss: 0.3940 - val_accuracy: 0.5588\n",
      "Epoch 513/1600\n",
      "680/680 [==============================] - 1s 943us/step - loss: 0.1803 - accuracy: 0.5937 - val_loss: 0.4071 - val_accuracy: 0.5441\n",
      "Epoch 514/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1929 - accuracy: 0.5922 - val_loss: 0.3973 - val_accuracy: 0.5647\n",
      "Epoch 515/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1754 - accuracy: 0.5951 - val_loss: 0.3987 - val_accuracy: 0.5441\n",
      "Epoch 516/1600\n",
      "680/680 [==============================] - 1s 960us/step - loss: 0.1689 - accuracy: 0.5974 - val_loss: 0.4350 - val_accuracy: 0.5441\n",
      "Epoch 517/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1839 - accuracy: 0.5959 - val_loss: 0.4018 - val_accuracy: 0.5676\n",
      "Epoch 518/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1901 - accuracy: 0.5915 - val_loss: 0.4015 - val_accuracy: 0.5618\n",
      "Epoch 519/1600\n",
      "680/680 [==============================] - 1s 903us/step - loss: 0.1915 - accuracy: 0.5933 - val_loss: 0.4135 - val_accuracy: 0.5324\n",
      "Epoch 520/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1872 - accuracy: 0.5948 - val_loss: 0.3756 - val_accuracy: 0.5588\n",
      "Epoch 521/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1842 - accuracy: 0.5955 - val_loss: 0.3829 - val_accuracy: 0.5529\n",
      "Epoch 522/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1721 - accuracy: 0.5977 - val_loss: 0.4065 - val_accuracy: 0.5471\n",
      "Epoch 523/1600\n",
      "680/680 [==============================] - 1s 930us/step - loss: 0.1728 - accuracy: 0.5962 - val_loss: 0.4083 - val_accuracy: 0.5588\n",
      "Epoch 524/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1869 - accuracy: 0.5926 - val_loss: 0.4097 - val_accuracy: 0.5471\n",
      "Epoch 525/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1858 - accuracy: 0.5955 - val_loss: 0.3977 - val_accuracy: 0.5500\n",
      "Epoch 526/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1785 - accuracy: 0.5940 - val_loss: 0.4014 - val_accuracy: 0.5471\n",
      "Epoch 527/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1733 - accuracy: 0.5977 - val_loss: 0.4128 - val_accuracy: 0.5441\n",
      "Epoch 528/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1717 - accuracy: 0.5981 - val_loss: 0.4581 - val_accuracy: 0.5294\n",
      "Epoch 529/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1992 - accuracy: 0.5937 - val_loss: 0.4043 - val_accuracy: 0.5559\n",
      "Epoch 530/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1937 - accuracy: 0.5907 - val_loss: 0.4161 - val_accuracy: 0.5324\n",
      "Epoch 531/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1725 - accuracy: 0.5974 - val_loss: 0.4183 - val_accuracy: 0.5529\n",
      "Epoch 532/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1798 - accuracy: 0.5959 - val_loss: 0.4016 - val_accuracy: 0.5529\n",
      "Epoch 533/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1770 - accuracy: 0.5940 - val_loss: 0.4268 - val_accuracy: 0.5382\n",
      "Epoch 534/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1928 - accuracy: 0.5918 - val_loss: 0.4386 - val_accuracy: 0.5353\n",
      "Epoch 535/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1979 - accuracy: 0.5900 - val_loss: 0.3949 - val_accuracy: 0.5529\n",
      "Epoch 536/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1889 - accuracy: 0.5926 - val_loss: 0.4072 - val_accuracy: 0.5500\n",
      "Epoch 537/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1703 - accuracy: 0.5955 - val_loss: 0.3662 - val_accuracy: 0.5529\n",
      "Epoch 538/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1865 - accuracy: 0.5915 - val_loss: 0.4131 - val_accuracy: 0.5471\n",
      "Epoch 539/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1764 - accuracy: 0.5951 - val_loss: 0.3963 - val_accuracy: 0.5588\n",
      "Epoch 540/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1919 - accuracy: 0.5940 - val_loss: 0.3690 - val_accuracy: 0.5382\n",
      "Epoch 541/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1846 - accuracy: 0.5926 - val_loss: 0.3775 - val_accuracy: 0.5676\n",
      "Epoch 542/1600\n",
      "680/680 [==============================] - 1s 864us/step - loss: 0.1877 - accuracy: 0.5933 - val_loss: 0.4201 - val_accuracy: 0.5441\n",
      "Epoch 543/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1711 - accuracy: 0.5948 - val_loss: 0.3740 - val_accuracy: 0.5500\n",
      "Epoch 544/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1827 - accuracy: 0.5966 - val_loss: 0.4228 - val_accuracy: 0.5382\n",
      "Epoch 545/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1933 - accuracy: 0.5948 - val_loss: 0.3842 - val_accuracy: 0.5441\n",
      "Epoch 546/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1698 - accuracy: 0.5974 - val_loss: 0.3838 - val_accuracy: 0.5500\n",
      "Epoch 547/1600\n",
      "680/680 [==============================] - 1s 866us/step - loss: 0.1757 - accuracy: 0.5937 - val_loss: 0.4147 - val_accuracy: 0.5441\n",
      "Epoch 548/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1794 - accuracy: 0.5929 - val_loss: 0.4076 - val_accuracy: 0.5471\n",
      "Epoch 549/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1945 - accuracy: 0.5933 - val_loss: 0.4207 - val_accuracy: 0.5412\n",
      "Epoch 550/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1825 - accuracy: 0.5948 - val_loss: 0.3794 - val_accuracy: 0.5382\n",
      "Epoch 551/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1782 - accuracy: 0.5951 - val_loss: 0.4110 - val_accuracy: 0.5412\n",
      "Epoch 552/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1798 - accuracy: 0.5951 - val_loss: 0.4119 - val_accuracy: 0.5382\n",
      "Epoch 553/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 925us/step - loss: 0.1773 - accuracy: 0.5966 - val_loss: 0.3940 - val_accuracy: 0.5441\n",
      "Epoch 554/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1708 - accuracy: 0.5985 - val_loss: 0.3792 - val_accuracy: 0.5500\n",
      "Epoch 555/1600\n",
      "680/680 [==============================] - 1s 904us/step - loss: 0.1842 - accuracy: 0.5959 - val_loss: 0.4092 - val_accuracy: 0.5412\n",
      "Epoch 556/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1840 - accuracy: 0.5929 - val_loss: 0.3899 - val_accuracy: 0.5559\n",
      "Epoch 557/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1823 - accuracy: 0.5922 - val_loss: 0.4387 - val_accuracy: 0.5471\n",
      "Epoch 558/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1762 - accuracy: 0.5959 - val_loss: 0.4443 - val_accuracy: 0.5441\n",
      "Epoch 559/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1873 - accuracy: 0.5937 - val_loss: 0.3941 - val_accuracy: 0.5412\n",
      "Epoch 560/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1878 - accuracy: 0.5922 - val_loss: 0.4205 - val_accuracy: 0.5529\n",
      "Epoch 561/1600\n",
      "680/680 [==============================] - 1s 939us/step - loss: 0.1931 - accuracy: 0.5915 - val_loss: 0.3642 - val_accuracy: 0.5529\n",
      "Epoch 562/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1735 - accuracy: 0.5962 - val_loss: 0.3766 - val_accuracy: 0.5647\n",
      "Epoch 563/1600\n",
      "680/680 [==============================] - 1s 934us/step - loss: 0.1774 - accuracy: 0.5948 - val_loss: 0.4131 - val_accuracy: 0.5559\n",
      "Epoch 564/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1858 - accuracy: 0.5933 - val_loss: 0.3639 - val_accuracy: 0.5706\n",
      "Epoch 565/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1868 - accuracy: 0.5948 - val_loss: 0.4167 - val_accuracy: 0.5588\n",
      "Epoch 566/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.1769 - accuracy: 0.5951 - val_loss: 0.4115 - val_accuracy: 0.5559\n",
      "Epoch 567/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1728 - accuracy: 0.5944 - val_loss: 0.4321 - val_accuracy: 0.5441\n",
      "Epoch 568/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1875 - accuracy: 0.5951 - val_loss: 0.4263 - val_accuracy: 0.5441\n",
      "Epoch 569/1600\n",
      "680/680 [==============================] - 1s 901us/step - loss: 0.1850 - accuracy: 0.5918 - val_loss: 0.4275 - val_accuracy: 0.5382\n",
      "Epoch 570/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1951 - accuracy: 0.5918 - val_loss: 0.4042 - val_accuracy: 0.5529\n",
      "Epoch 571/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1903 - accuracy: 0.5926 - val_loss: 0.3981 - val_accuracy: 0.5500\n",
      "Epoch 572/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1697 - accuracy: 0.5970 - val_loss: 0.4144 - val_accuracy: 0.5500\n",
      "Epoch 573/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1898 - accuracy: 0.5948 - val_loss: 0.3863 - val_accuracy: 0.5559\n",
      "Epoch 574/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1783 - accuracy: 0.5951 - val_loss: 0.4272 - val_accuracy: 0.5412\n",
      "Epoch 575/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1846 - accuracy: 0.5951 - val_loss: 0.3876 - val_accuracy: 0.5500\n",
      "Epoch 576/1600\n",
      "680/680 [==============================] - 1s 934us/step - loss: 0.1899 - accuracy: 0.5951 - val_loss: 0.3900 - val_accuracy: 0.5471\n",
      "Epoch 577/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1875 - accuracy: 0.5948 - val_loss: 0.3775 - val_accuracy: 0.5559\n",
      "Epoch 578/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1795 - accuracy: 0.5959 - val_loss: 0.4195 - val_accuracy: 0.5353\n",
      "Epoch 579/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1851 - accuracy: 0.5922 - val_loss: 0.4001 - val_accuracy: 0.5500\n",
      "Epoch 580/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1778 - accuracy: 0.5977 - val_loss: 0.4302 - val_accuracy: 0.5441\n",
      "Epoch 581/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1750 - accuracy: 0.5966 - val_loss: 0.3827 - val_accuracy: 0.5500\n",
      "Epoch 582/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1857 - accuracy: 0.5966 - val_loss: 0.4267 - val_accuracy: 0.5647\n",
      "Epoch 583/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1807 - accuracy: 0.5948 - val_loss: 0.4130 - val_accuracy: 0.5441\n",
      "Epoch 584/1600\n",
      "680/680 [==============================] - 1s 863us/step - loss: 0.1848 - accuracy: 0.5940 - val_loss: 0.4368 - val_accuracy: 0.5559\n",
      "Epoch 585/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1957 - accuracy: 0.5896 - val_loss: 0.4076 - val_accuracy: 0.5500\n",
      "Epoch 586/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1817 - accuracy: 0.5962 - val_loss: 0.4061 - val_accuracy: 0.5529\n",
      "Epoch 587/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1818 - accuracy: 0.5951 - val_loss: 0.4158 - val_accuracy: 0.5500\n",
      "Epoch 588/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1754 - accuracy: 0.5959 - val_loss: 0.4394 - val_accuracy: 0.5324\n",
      "Epoch 589/1600\n",
      "680/680 [==============================] - 1s 938us/step - loss: 0.1879 - accuracy: 0.5933 - val_loss: 0.4083 - val_accuracy: 0.5412\n",
      "Epoch 590/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1835 - accuracy: 0.5940 - val_loss: 0.4266 - val_accuracy: 0.5529\n",
      "Epoch 591/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1735 - accuracy: 0.5992 - val_loss: 0.3986 - val_accuracy: 0.5500\n",
      "Epoch 592/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1738 - accuracy: 0.5988 - val_loss: 0.4235 - val_accuracy: 0.5265\n",
      "Epoch 593/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1901 - accuracy: 0.5940 - val_loss: 0.3877 - val_accuracy: 0.5618\n",
      "Epoch 594/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1880 - accuracy: 0.5926 - val_loss: 0.3964 - val_accuracy: 0.5559\n",
      "Epoch 595/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1943 - accuracy: 0.5929 - val_loss: 0.3835 - val_accuracy: 0.5441\n",
      "Epoch 596/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1880 - accuracy: 0.5933 - val_loss: 0.3812 - val_accuracy: 0.5618\n",
      "Epoch 597/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1672 - accuracy: 0.5970 - val_loss: 0.4050 - val_accuracy: 0.5529\n",
      "Epoch 598/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1892 - accuracy: 0.5922 - val_loss: 0.3850 - val_accuracy: 0.5412\n",
      "Epoch 599/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1928 - accuracy: 0.5937 - val_loss: 0.3840 - val_accuracy: 0.5559\n",
      "Epoch 600/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1726 - accuracy: 0.5985 - val_loss: 0.4025 - val_accuracy: 0.5471\n",
      "Epoch 601/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1748 - accuracy: 0.5977 - val_loss: 0.3826 - val_accuracy: 0.5529\n",
      "Epoch 602/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1826 - accuracy: 0.5966 - val_loss: 0.3761 - val_accuracy: 0.5559\n",
      "Epoch 603/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1721 - accuracy: 0.5970 - val_loss: 0.3955 - val_accuracy: 0.5735\n",
      "Epoch 604/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.2006 - accuracy: 0.5933 - val_loss: 0.3470 - val_accuracy: 0.5618\n",
      "Epoch 605/1600\n",
      "680/680 [==============================] - 1s 966us/step - loss: 0.1926 - accuracy: 0.5918 - val_loss: 0.4202 - val_accuracy: 0.5382\n",
      "Epoch 606/1600\n",
      "680/680 [==============================] - 1s 862us/step - loss: 0.1792 - accuracy: 0.5940 - val_loss: 0.4043 - val_accuracy: 0.5441\n",
      "Epoch 607/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1808 - accuracy: 0.5970 - val_loss: 0.3398 - val_accuracy: 0.5706\n",
      "Epoch 608/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1910 - accuracy: 0.5926 - val_loss: 0.3744 - val_accuracy: 0.5618\n",
      "Epoch 609/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1832 - accuracy: 0.5951 - val_loss: 0.4409 - val_accuracy: 0.5294\n",
      "Epoch 610/1600\n",
      "680/680 [==============================] - 1s 906us/step - loss: 0.1844 - accuracy: 0.5959 - val_loss: 0.3943 - val_accuracy: 0.5529\n",
      "Epoch 611/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1786 - accuracy: 0.5951 - val_loss: 0.3731 - val_accuracy: 0.5529\n",
      "Epoch 612/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1810 - accuracy: 0.5966 - val_loss: 0.4133 - val_accuracy: 0.5382\n",
      "Epoch 613/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1778 - accuracy: 0.5951 - val_loss: 0.3829 - val_accuracy: 0.5588\n",
      "Epoch 614/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1705 - accuracy: 0.5988 - val_loss: 0.4150 - val_accuracy: 0.5441\n",
      "Epoch 615/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1848 - accuracy: 0.5937 - val_loss: 0.4289 - val_accuracy: 0.5471\n",
      "Epoch 616/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1713 - accuracy: 0.5977 - val_loss: 0.3868 - val_accuracy: 0.5618\n",
      "Epoch 617/1600\n",
      "680/680 [==============================] - 1s 866us/step - loss: 0.2057 - accuracy: 0.5911 - val_loss: 0.4208 - val_accuracy: 0.5618\n",
      "Epoch 618/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1769 - accuracy: 0.5962 - val_loss: 0.4279 - val_accuracy: 0.5412\n",
      "Epoch 619/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1802 - accuracy: 0.5948 - val_loss: 0.4145 - val_accuracy: 0.5588\n",
      "Epoch 620/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1889 - accuracy: 0.5940 - val_loss: 0.3969 - val_accuracy: 0.5529\n",
      "Epoch 621/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1781 - accuracy: 0.5944 - val_loss: 0.4486 - val_accuracy: 0.5353\n",
      "Epoch 622/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1838 - accuracy: 0.5922 - val_loss: 0.4366 - val_accuracy: 0.5588\n",
      "Epoch 623/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1952 - accuracy: 0.5918 - val_loss: 0.3944 - val_accuracy: 0.5471\n",
      "Epoch 624/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1802 - accuracy: 0.5955 - val_loss: 0.4481 - val_accuracy: 0.5412\n",
      "Epoch 625/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1705 - accuracy: 0.5955 - val_loss: 0.3938 - val_accuracy: 0.5559\n",
      "Epoch 626/1600\n",
      "680/680 [==============================] - 1s 936us/step - loss: 0.1780 - accuracy: 0.5940 - val_loss: 0.3975 - val_accuracy: 0.5500\n",
      "Epoch 627/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1817 - accuracy: 0.5966 - val_loss: 0.4194 - val_accuracy: 0.5441\n",
      "Epoch 628/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1936 - accuracy: 0.5900 - val_loss: 0.4227 - val_accuracy: 0.5588\n",
      "Epoch 629/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1797 - accuracy: 0.5926 - val_loss: 0.4209 - val_accuracy: 0.5441\n",
      "Epoch 630/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1818 - accuracy: 0.5955 - val_loss: 0.4382 - val_accuracy: 0.5382\n",
      "Epoch 631/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1958 - accuracy: 0.5918 - val_loss: 0.4478 - val_accuracy: 0.5382\n",
      "Epoch 632/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1843 - accuracy: 0.5948 - val_loss: 0.3896 - val_accuracy: 0.5588\n",
      "Epoch 633/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1770 - accuracy: 0.5959 - val_loss: 0.4123 - val_accuracy: 0.5559\n",
      "Epoch 634/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1763 - accuracy: 0.5966 - val_loss: 0.4045 - val_accuracy: 0.5412\n",
      "Epoch 635/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1745 - accuracy: 0.5970 - val_loss: 0.3918 - val_accuracy: 0.5441\n",
      "Epoch 636/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1865 - accuracy: 0.5933 - val_loss: 0.4277 - val_accuracy: 0.5441\n",
      "Epoch 637/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1854 - accuracy: 0.5929 - val_loss: 0.3868 - val_accuracy: 0.5353\n",
      "Epoch 638/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1807 - accuracy: 0.5922 - val_loss: 0.4147 - val_accuracy: 0.5471\n",
      "Epoch 639/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1905 - accuracy: 0.5933 - val_loss: 0.4121 - val_accuracy: 0.5441\n",
      "Epoch 640/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1738 - accuracy: 0.5970 - val_loss: 0.4197 - val_accuracy: 0.5559\n",
      "Epoch 641/1600\n",
      "680/680 [==============================] - 1s 898us/step - loss: 0.1859 - accuracy: 0.5944 - val_loss: 0.4146 - val_accuracy: 0.5471\n",
      "Epoch 642/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1841 - accuracy: 0.5922 - val_loss: 0.4140 - val_accuracy: 0.5500\n",
      "Epoch 643/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1733 - accuracy: 0.5955 - val_loss: 0.4404 - val_accuracy: 0.5382\n",
      "Epoch 644/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1745 - accuracy: 0.5948 - val_loss: 0.4091 - val_accuracy: 0.5588\n",
      "Epoch 645/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1839 - accuracy: 0.5955 - val_loss: 0.4308 - val_accuracy: 0.5441\n",
      "Epoch 646/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1769 - accuracy: 0.5955 - val_loss: 0.4061 - val_accuracy: 0.5500\n",
      "Epoch 647/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.1733 - accuracy: 0.5955 - val_loss: 0.4272 - val_accuracy: 0.5471\n",
      "Epoch 648/1600\n",
      "680/680 [==============================] - 1s 963us/step - loss: 0.1992 - accuracy: 0.5907 - val_loss: 0.4085 - val_accuracy: 0.5471\n",
      "Epoch 649/1600\n",
      "680/680 [==============================] - 1s 972us/step - loss: 0.1802 - accuracy: 0.5959 - val_loss: 0.4239 - val_accuracy: 0.5412\n",
      "Epoch 650/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1891 - accuracy: 0.5911 - val_loss: 0.4049 - val_accuracy: 0.5500\n",
      "Epoch 651/1600\n",
      "680/680 [==============================] - 1s 934us/step - loss: 0.1889 - accuracy: 0.5933 - val_loss: 0.4122 - val_accuracy: 0.5441\n",
      "Epoch 652/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1714 - accuracy: 0.5981 - val_loss: 0.4435 - val_accuracy: 0.5324\n",
      "Epoch 653/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1699 - accuracy: 0.5966 - val_loss: 0.3841 - val_accuracy: 0.5559\n",
      "Epoch 654/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1865 - accuracy: 0.5944 - val_loss: 0.3983 - val_accuracy: 0.5471\n",
      "Epoch 655/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1896 - accuracy: 0.5907 - val_loss: 0.4071 - val_accuracy: 0.5412\n",
      "Epoch 656/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1771 - accuracy: 0.5948 - val_loss: 0.4233 - val_accuracy: 0.5382\n",
      "Epoch 657/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1767 - accuracy: 0.5944 - val_loss: 0.4104 - val_accuracy: 0.5529\n",
      "Epoch 658/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1773 - accuracy: 0.5955 - val_loss: 0.4374 - val_accuracy: 0.5441\n",
      "Epoch 659/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1807 - accuracy: 0.5962 - val_loss: 0.3711 - val_accuracy: 0.5471\n",
      "Epoch 660/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1848 - accuracy: 0.5933 - val_loss: 0.4154 - val_accuracy: 0.5353\n",
      "Epoch 661/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1945 - accuracy: 0.5904 - val_loss: 0.4096 - val_accuracy: 0.5500\n",
      "Epoch 662/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1747 - accuracy: 0.5948 - val_loss: 0.4273 - val_accuracy: 0.5471\n",
      "Epoch 663/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 893us/step - loss: 0.1801 - accuracy: 0.5981 - val_loss: 0.4129 - val_accuracy: 0.5382\n",
      "Epoch 664/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1958 - accuracy: 0.5900 - val_loss: 0.4253 - val_accuracy: 0.5353\n",
      "Epoch 665/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1740 - accuracy: 0.5966 - val_loss: 0.4477 - val_accuracy: 0.5353\n",
      "Epoch 666/1600\n",
      "680/680 [==============================] - 1s 933us/step - loss: 0.1941 - accuracy: 0.5922 - val_loss: 0.4137 - val_accuracy: 0.5441\n",
      "Epoch 667/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1821 - accuracy: 0.5940 - val_loss: 0.3914 - val_accuracy: 0.5559\n",
      "Epoch 668/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1787 - accuracy: 0.5966 - val_loss: 0.4045 - val_accuracy: 0.5559\n",
      "Epoch 669/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1808 - accuracy: 0.5940 - val_loss: 0.4017 - val_accuracy: 0.5618\n",
      "Epoch 670/1600\n",
      "680/680 [==============================] - 1s 964us/step - loss: 0.1852 - accuracy: 0.5944 - val_loss: 0.4082 - val_accuracy: 0.5471\n",
      "Epoch 671/1600\n",
      "680/680 [==============================] - 1s 864us/step - loss: 0.1784 - accuracy: 0.5962 - val_loss: 0.4381 - val_accuracy: 0.5441\n",
      "Epoch 672/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1961 - accuracy: 0.5911 - val_loss: 0.4136 - val_accuracy: 0.5324\n",
      "Epoch 673/1600\n",
      "680/680 [==============================] - 1s 947us/step - loss: 0.1699 - accuracy: 0.5974 - val_loss: 0.4370 - val_accuracy: 0.5412\n",
      "Epoch 674/1600\n",
      "680/680 [==============================] - 1s 906us/step - loss: 0.1802 - accuracy: 0.5955 - val_loss: 0.4063 - val_accuracy: 0.5588\n",
      "Epoch 675/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1780 - accuracy: 0.5940 - val_loss: 0.4095 - val_accuracy: 0.5353\n",
      "Epoch 676/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1888 - accuracy: 0.5944 - val_loss: 0.4017 - val_accuracy: 0.5441\n",
      "Epoch 677/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1776 - accuracy: 0.5944 - val_loss: 0.4017 - val_accuracy: 0.5735\n",
      "Epoch 678/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1658 - accuracy: 0.5996 - val_loss: 0.4171 - val_accuracy: 0.5353\n",
      "Epoch 679/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1881 - accuracy: 0.5940 - val_loss: 0.4184 - val_accuracy: 0.5353\n",
      "Epoch 680/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1812 - accuracy: 0.5933 - val_loss: 0.3982 - val_accuracy: 0.5618\n",
      "Epoch 681/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1906 - accuracy: 0.5948 - val_loss: 0.4049 - val_accuracy: 0.5471\n",
      "Epoch 682/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1826 - accuracy: 0.5937 - val_loss: 0.4223 - val_accuracy: 0.5471\n",
      "Epoch 683/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1889 - accuracy: 0.5926 - val_loss: 0.4093 - val_accuracy: 0.5471\n",
      "Epoch 684/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1790 - accuracy: 0.5940 - val_loss: 0.4216 - val_accuracy: 0.5500\n",
      "Epoch 685/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1852 - accuracy: 0.5915 - val_loss: 0.3924 - val_accuracy: 0.5382\n",
      "Epoch 686/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1764 - accuracy: 0.5959 - val_loss: 0.4189 - val_accuracy: 0.5382\n",
      "Epoch 687/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1772 - accuracy: 0.5966 - val_loss: 0.4013 - val_accuracy: 0.5441\n",
      "Epoch 688/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1817 - accuracy: 0.5951 - val_loss: 0.4232 - val_accuracy: 0.5529\n",
      "Epoch 689/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1832 - accuracy: 0.5948 - val_loss: 0.4238 - val_accuracy: 0.5618\n",
      "Epoch 690/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1842 - accuracy: 0.5944 - val_loss: 0.4091 - val_accuracy: 0.5500\n",
      "Epoch 691/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1835 - accuracy: 0.5940 - val_loss: 0.3907 - val_accuracy: 0.5588\n",
      "Epoch 692/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1690 - accuracy: 0.5959 - val_loss: 0.4183 - val_accuracy: 0.5412\n",
      "Epoch 693/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1791 - accuracy: 0.5955 - val_loss: 0.4106 - val_accuracy: 0.5500\n",
      "Epoch 694/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1848 - accuracy: 0.5926 - val_loss: 0.3836 - val_accuracy: 0.5529\n",
      "Epoch 695/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1627 - accuracy: 0.5999 - val_loss: 0.3974 - val_accuracy: 0.5471\n",
      "Epoch 696/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1887 - accuracy: 0.5933 - val_loss: 0.4017 - val_accuracy: 0.5588\n",
      "Epoch 697/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1791 - accuracy: 0.5940 - val_loss: 0.3995 - val_accuracy: 0.5529\n",
      "Epoch 698/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.1719 - accuracy: 0.5962 - val_loss: 0.3998 - val_accuracy: 0.5500\n",
      "Epoch 699/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1719 - accuracy: 0.5977 - val_loss: 0.4338 - val_accuracy: 0.5382\n",
      "Epoch 700/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1885 - accuracy: 0.5959 - val_loss: 0.3874 - val_accuracy: 0.5382\n",
      "Epoch 701/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1918 - accuracy: 0.5937 - val_loss: 0.4093 - val_accuracy: 0.5441\n",
      "Epoch 702/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1728 - accuracy: 0.5966 - val_loss: 0.4800 - val_accuracy: 0.5118\n",
      "Epoch 703/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1870 - accuracy: 0.5944 - val_loss: 0.4033 - val_accuracy: 0.5529\n",
      "Epoch 704/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1883 - accuracy: 0.5948 - val_loss: 0.3992 - val_accuracy: 0.5559\n",
      "Epoch 705/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1682 - accuracy: 0.5981 - val_loss: 0.4306 - val_accuracy: 0.5412\n",
      "Epoch 706/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1857 - accuracy: 0.5937 - val_loss: 0.4555 - val_accuracy: 0.5353\n",
      "Epoch 707/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1770 - accuracy: 0.5970 - val_loss: 0.4635 - val_accuracy: 0.5353\n",
      "Epoch 708/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1800 - accuracy: 0.5929 - val_loss: 0.4391 - val_accuracy: 0.5471\n",
      "Epoch 709/1600\n",
      "680/680 [==============================] - 1s 862us/step - loss: 0.1901 - accuracy: 0.5926 - val_loss: 0.4148 - val_accuracy: 0.5382\n",
      "Epoch 710/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1766 - accuracy: 0.5966 - val_loss: 0.4092 - val_accuracy: 0.5471\n",
      "Epoch 711/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1799 - accuracy: 0.5962 - val_loss: 0.4100 - val_accuracy: 0.5618\n",
      "Epoch 712/1600\n",
      "680/680 [==============================] - 1s 866us/step - loss: 0.1771 - accuracy: 0.5970 - val_loss: 0.4027 - val_accuracy: 0.5441\n",
      "Epoch 713/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1860 - accuracy: 0.5944 - val_loss: 0.4045 - val_accuracy: 0.5412\n",
      "Epoch 714/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.1697 - accuracy: 0.5981 - val_loss: 0.4092 - val_accuracy: 0.5441\n",
      "Epoch 715/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1893 - accuracy: 0.5955 - val_loss: 0.4179 - val_accuracy: 0.5441\n",
      "Epoch 716/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1838 - accuracy: 0.5929 - val_loss: 0.4078 - val_accuracy: 0.5500\n",
      "Epoch 717/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1781 - accuracy: 0.5948 - val_loss: 0.4318 - val_accuracy: 0.5500\n",
      "Epoch 718/1600\n",
      "680/680 [==============================] - 1s 942us/step - loss: 0.1769 - accuracy: 0.5959 - val_loss: 0.4143 - val_accuracy: 0.5471\n",
      "Epoch 719/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.1782 - accuracy: 0.5981 - val_loss: 0.3744 - val_accuracy: 0.5588\n",
      "Epoch 720/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1789 - accuracy: 0.5974 - val_loss: 0.4097 - val_accuracy: 0.5647\n",
      "Epoch 721/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1748 - accuracy: 0.5988 - val_loss: 0.4328 - val_accuracy: 0.5441\n",
      "Epoch 722/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1812 - accuracy: 0.5985 - val_loss: 0.4250 - val_accuracy: 0.5353\n",
      "Epoch 723/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1893 - accuracy: 0.5926 - val_loss: 0.4198 - val_accuracy: 0.5441\n",
      "Epoch 724/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1770 - accuracy: 0.5929 - val_loss: 0.4424 - val_accuracy: 0.5559\n",
      "Epoch 725/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1717 - accuracy: 0.5962 - val_loss: 0.4016 - val_accuracy: 0.5529\n",
      "Epoch 726/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1924 - accuracy: 0.5937 - val_loss: 0.5060 - val_accuracy: 0.5118\n",
      "Epoch 727/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1962 - accuracy: 0.5926 - val_loss: 0.4507 - val_accuracy: 0.5382\n",
      "Epoch 728/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1655 - accuracy: 0.6003 - val_loss: 0.4838 - val_accuracy: 0.5265\n",
      "Epoch 729/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1807 - accuracy: 0.5937 - val_loss: 0.4039 - val_accuracy: 0.5412\n",
      "Epoch 730/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1770 - accuracy: 0.5970 - val_loss: 0.4291 - val_accuracy: 0.5382\n",
      "Epoch 731/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1719 - accuracy: 0.5977 - val_loss: 0.4428 - val_accuracy: 0.5324\n",
      "Epoch 732/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1900 - accuracy: 0.5929 - val_loss: 0.4339 - val_accuracy: 0.5559\n",
      "Epoch 733/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1771 - accuracy: 0.5937 - val_loss: 0.3939 - val_accuracy: 0.5382\n",
      "Epoch 734/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1669 - accuracy: 0.5985 - val_loss: 0.3876 - val_accuracy: 0.5471\n",
      "Epoch 735/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1756 - accuracy: 0.5966 - val_loss: 0.3894 - val_accuracy: 0.5471\n",
      "Epoch 736/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1864 - accuracy: 0.5944 - val_loss: 0.4280 - val_accuracy: 0.5412\n",
      "Epoch 737/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1767 - accuracy: 0.5959 - val_loss: 0.4319 - val_accuracy: 0.5382\n",
      "Epoch 738/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1758 - accuracy: 0.5974 - val_loss: 0.4205 - val_accuracy: 0.5441\n",
      "Epoch 739/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1835 - accuracy: 0.5948 - val_loss: 0.4520 - val_accuracy: 0.5353\n",
      "Epoch 740/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1818 - accuracy: 0.5959 - val_loss: 0.4463 - val_accuracy: 0.5412\n",
      "Epoch 741/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1708 - accuracy: 0.5985 - val_loss: 0.4483 - val_accuracy: 0.5471\n",
      "Epoch 742/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1725 - accuracy: 0.5966 - val_loss: 0.4007 - val_accuracy: 0.5500\n",
      "Epoch 743/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1876 - accuracy: 0.5940 - val_loss: 0.4118 - val_accuracy: 0.5353\n",
      "Epoch 744/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1788 - accuracy: 0.5974 - val_loss: 0.4142 - val_accuracy: 0.5382\n",
      "Epoch 745/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1746 - accuracy: 0.5962 - val_loss: 0.4620 - val_accuracy: 0.5412\n",
      "Epoch 746/1600\n",
      "680/680 [==============================] - 1s 965us/step - loss: 0.1769 - accuracy: 0.5966 - val_loss: 0.4586 - val_accuracy: 0.5441\n",
      "Epoch 747/1600\n",
      "680/680 [==============================] - 1s 942us/step - loss: 0.1703 - accuracy: 0.5985 - val_loss: 0.4338 - val_accuracy: 0.5559\n",
      "Epoch 748/1600\n",
      "680/680 [==============================] - 1s 944us/step - loss: 0.1959 - accuracy: 0.5933 - val_loss: 0.4188 - val_accuracy: 0.5382\n",
      "Epoch 749/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1761 - accuracy: 0.5970 - val_loss: 0.4115 - val_accuracy: 0.5500\n",
      "Epoch 750/1600\n",
      "680/680 [==============================] - 1s 930us/step - loss: 0.1766 - accuracy: 0.5970 - val_loss: 0.3997 - val_accuracy: 0.5412\n",
      "Epoch 751/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1827 - accuracy: 0.5951 - val_loss: 0.4161 - val_accuracy: 0.5382\n",
      "Epoch 752/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.2083 - accuracy: 0.5893 - val_loss: 0.4454 - val_accuracy: 0.5353\n",
      "Epoch 753/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1784 - accuracy: 0.5951 - val_loss: 0.3817 - val_accuracy: 0.5382\n",
      "Epoch 754/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1745 - accuracy: 0.5962 - val_loss: 0.4251 - val_accuracy: 0.5441\n",
      "Epoch 755/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1700 - accuracy: 0.5988 - val_loss: 0.4418 - val_accuracy: 0.5324\n",
      "Epoch 756/1600\n",
      "680/680 [==============================] - 1s 968us/step - loss: 0.1892 - accuracy: 0.5948 - val_loss: 0.3940 - val_accuracy: 0.5441\n",
      "Epoch 757/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1824 - accuracy: 0.5951 - val_loss: 0.4161 - val_accuracy: 0.5412\n",
      "Epoch 758/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1717 - accuracy: 0.5962 - val_loss: 0.3915 - val_accuracy: 0.5441\n",
      "Epoch 759/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1684 - accuracy: 0.5981 - val_loss: 0.4306 - val_accuracy: 0.5294\n",
      "Epoch 760/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1827 - accuracy: 0.5929 - val_loss: 0.4200 - val_accuracy: 0.5529\n",
      "Epoch 761/1600\n",
      "680/680 [==============================] - 1s 903us/step - loss: 0.1848 - accuracy: 0.5951 - val_loss: 0.3958 - val_accuracy: 0.5500\n",
      "Epoch 762/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1855 - accuracy: 0.5940 - val_loss: 0.4340 - val_accuracy: 0.5324\n",
      "Epoch 763/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1846 - accuracy: 0.5948 - val_loss: 0.4224 - val_accuracy: 0.5353\n",
      "Epoch 764/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1736 - accuracy: 0.5981 - val_loss: 0.4033 - val_accuracy: 0.5471\n",
      "Epoch 765/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1716 - accuracy: 0.5970 - val_loss: 0.4227 - val_accuracy: 0.5324\n",
      "Epoch 766/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1859 - accuracy: 0.5955 - val_loss: 0.4709 - val_accuracy: 0.5265\n",
      "Epoch 767/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1852 - accuracy: 0.5944 - val_loss: 0.4122 - val_accuracy: 0.5441\n",
      "Epoch 768/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1756 - accuracy: 0.5962 - val_loss: 0.4110 - val_accuracy: 0.5471\n",
      "Epoch 769/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1902 - accuracy: 0.5911 - val_loss: 0.4312 - val_accuracy: 0.5529\n",
      "Epoch 770/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1783 - accuracy: 0.5959 - val_loss: 0.4091 - val_accuracy: 0.5412\n",
      "Epoch 771/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1794 - accuracy: 0.5951 - val_loss: 0.4190 - val_accuracy: 0.5441\n",
      "Epoch 772/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1684 - accuracy: 0.5977 - val_loss: 0.4248 - val_accuracy: 0.5353\n",
      "Epoch 773/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 924us/step - loss: 0.1893 - accuracy: 0.5933 - val_loss: 0.4036 - val_accuracy: 0.5529\n",
      "Epoch 774/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1668 - accuracy: 0.5974 - val_loss: 0.4447 - val_accuracy: 0.5441\n",
      "Epoch 775/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.2049 - accuracy: 0.5922 - val_loss: 0.4371 - val_accuracy: 0.5353\n",
      "Epoch 776/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1838 - accuracy: 0.5937 - val_loss: 0.4229 - val_accuracy: 0.5441\n",
      "Epoch 777/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1782 - accuracy: 0.5951 - val_loss: 0.4130 - val_accuracy: 0.5529\n",
      "Epoch 778/1600\n",
      "680/680 [==============================] - 1s 930us/step - loss: 0.1748 - accuracy: 0.5933 - val_loss: 0.4156 - val_accuracy: 0.5412\n",
      "Epoch 779/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1856 - accuracy: 0.5948 - val_loss: 0.4259 - val_accuracy: 0.5500\n",
      "Epoch 780/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1781 - accuracy: 0.5955 - val_loss: 0.4136 - val_accuracy: 0.5412\n",
      "Epoch 781/1600\n",
      "680/680 [==============================] - 1s 943us/step - loss: 0.1790 - accuracy: 0.5933 - val_loss: 0.4244 - val_accuracy: 0.5382\n",
      "Epoch 782/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1721 - accuracy: 0.5966 - val_loss: 0.4150 - val_accuracy: 0.5324\n",
      "Epoch 783/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1585 - accuracy: 0.6014 - val_loss: 0.4339 - val_accuracy: 0.5324\n",
      "Epoch 784/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1959 - accuracy: 0.5915 - val_loss: 0.4088 - val_accuracy: 0.5441\n",
      "Epoch 785/1600\n",
      "680/680 [==============================] - 1s 938us/step - loss: 0.1864 - accuracy: 0.5948 - val_loss: 0.4422 - val_accuracy: 0.5412\n",
      "Epoch 786/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1756 - accuracy: 0.5981 - val_loss: 0.4704 - val_accuracy: 0.5206\n",
      "Epoch 787/1600\n",
      "680/680 [==============================] - 1s 907us/step - loss: 0.1773 - accuracy: 0.5933 - val_loss: 0.4507 - val_accuracy: 0.5382\n",
      "Epoch 788/1600\n",
      "680/680 [==============================] - 1s 934us/step - loss: 0.1702 - accuracy: 0.5992 - val_loss: 0.4311 - val_accuracy: 0.5412\n",
      "Epoch 789/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1754 - accuracy: 0.5977 - val_loss: 0.4495 - val_accuracy: 0.5676\n",
      "Epoch 790/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1873 - accuracy: 0.5948 - val_loss: 0.3964 - val_accuracy: 0.5382\n",
      "Epoch 791/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1768 - accuracy: 0.5951 - val_loss: 0.4032 - val_accuracy: 0.5529\n",
      "Epoch 792/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1861 - accuracy: 0.5915 - val_loss: 0.4183 - val_accuracy: 0.5382\n",
      "Epoch 793/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1764 - accuracy: 0.5948 - val_loss: 0.4372 - val_accuracy: 0.5382\n",
      "Epoch 794/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1712 - accuracy: 0.5974 - val_loss: 0.3986 - val_accuracy: 0.5382\n",
      "Epoch 795/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1990 - accuracy: 0.5915 - val_loss: 0.4474 - val_accuracy: 0.5294\n",
      "Epoch 796/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1892 - accuracy: 0.5929 - val_loss: 0.4266 - val_accuracy: 0.5647\n",
      "Epoch 797/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1744 - accuracy: 0.5966 - val_loss: 0.4157 - val_accuracy: 0.5382\n",
      "Epoch 798/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1765 - accuracy: 0.5974 - val_loss: 0.4048 - val_accuracy: 0.5382\n",
      "Epoch 799/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1808 - accuracy: 0.5951 - val_loss: 0.4366 - val_accuracy: 0.5324\n",
      "Epoch 800/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1759 - accuracy: 0.5966 - val_loss: 0.3688 - val_accuracy: 0.5471\n",
      "Epoch 801/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1881 - accuracy: 0.5944 - val_loss: 0.4013 - val_accuracy: 0.5529\n",
      "Epoch 802/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1782 - accuracy: 0.5940 - val_loss: 0.4115 - val_accuracy: 0.5382\n",
      "Epoch 803/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.1649 - accuracy: 0.5966 - val_loss: 0.4400 - val_accuracy: 0.5441\n",
      "Epoch 804/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1796 - accuracy: 0.5959 - val_loss: 0.4256 - val_accuracy: 0.5471\n",
      "Epoch 805/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1756 - accuracy: 0.5981 - val_loss: 0.4572 - val_accuracy: 0.5441\n",
      "Epoch 806/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1729 - accuracy: 0.5955 - val_loss: 0.4562 - val_accuracy: 0.5412\n",
      "Epoch 807/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1781 - accuracy: 0.5959 - val_loss: 0.4157 - val_accuracy: 0.5441\n",
      "Epoch 808/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1843 - accuracy: 0.5944 - val_loss: 0.4181 - val_accuracy: 0.5441\n",
      "Epoch 809/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1747 - accuracy: 0.5970 - val_loss: 0.4523 - val_accuracy: 0.5324\n",
      "Epoch 810/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1776 - accuracy: 0.5974 - val_loss: 0.4246 - val_accuracy: 0.5412\n",
      "Epoch 811/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1816 - accuracy: 0.5948 - val_loss: 0.4266 - val_accuracy: 0.5412\n",
      "Epoch 812/1600\n",
      "680/680 [==============================] - 1s 862us/step - loss: 0.1786 - accuracy: 0.5948 - val_loss: 0.4560 - val_accuracy: 0.5265\n",
      "Epoch 813/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1701 - accuracy: 0.5974 - val_loss: 0.4328 - val_accuracy: 0.5412\n",
      "Epoch 814/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1727 - accuracy: 0.5959 - val_loss: 0.4252 - val_accuracy: 0.5618\n",
      "Epoch 815/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1693 - accuracy: 0.5985 - val_loss: 0.4152 - val_accuracy: 0.5500\n",
      "Epoch 816/1600\n",
      "680/680 [==============================] - 1s 907us/step - loss: 0.1809 - accuracy: 0.5959 - val_loss: 0.4566 - val_accuracy: 0.5412\n",
      "Epoch 817/1600\n",
      "680/680 [==============================] - 1s 935us/step - loss: 0.1891 - accuracy: 0.5926 - val_loss: 0.4336 - val_accuracy: 0.5412\n",
      "Epoch 818/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1786 - accuracy: 0.5929 - val_loss: 0.4084 - val_accuracy: 0.5382\n",
      "Epoch 819/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1874 - accuracy: 0.5955 - val_loss: 0.4076 - val_accuracy: 0.5529\n",
      "Epoch 820/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1844 - accuracy: 0.5974 - val_loss: 0.4259 - val_accuracy: 0.5412\n",
      "Epoch 821/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1925 - accuracy: 0.5922 - val_loss: 0.4754 - val_accuracy: 0.5471\n",
      "Epoch 822/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1968 - accuracy: 0.5918 - val_loss: 0.4539 - val_accuracy: 0.5441\n",
      "Epoch 823/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1717 - accuracy: 0.5977 - val_loss: 0.3948 - val_accuracy: 0.5588\n",
      "Epoch 824/1600\n",
      "680/680 [==============================] - 1s 906us/step - loss: 0.1767 - accuracy: 0.5974 - val_loss: 0.4399 - val_accuracy: 0.5324\n",
      "Epoch 825/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1807 - accuracy: 0.5962 - val_loss: 0.4417 - val_accuracy: 0.5412\n",
      "Epoch 826/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1749 - accuracy: 0.5962 - val_loss: 0.4501 - val_accuracy: 0.5412\n",
      "Epoch 827/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1788 - accuracy: 0.5962 - val_loss: 0.4444 - val_accuracy: 0.5324\n",
      "Epoch 828/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1740 - accuracy: 0.5955 - val_loss: 0.4335 - val_accuracy: 0.5382\n",
      "Epoch 829/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1938 - accuracy: 0.5933 - val_loss: 0.4264 - val_accuracy: 0.5441\n",
      "Epoch 830/1600\n",
      "680/680 [==============================] - 1s 864us/step - loss: 0.1808 - accuracy: 0.5955 - val_loss: 0.4130 - val_accuracy: 0.5412\n",
      "Epoch 831/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1639 - accuracy: 0.5996 - val_loss: 0.4530 - val_accuracy: 0.5324\n",
      "Epoch 832/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1834 - accuracy: 0.5933 - val_loss: 0.4580 - val_accuracy: 0.5412\n",
      "Epoch 833/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.2012 - accuracy: 0.5907 - val_loss: 0.4535 - val_accuracy: 0.5353\n",
      "Epoch 834/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1833 - accuracy: 0.5937 - val_loss: 0.4390 - val_accuracy: 0.5500\n",
      "Epoch 835/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1756 - accuracy: 0.5977 - val_loss: 0.4551 - val_accuracy: 0.5353\n",
      "Epoch 836/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1689 - accuracy: 0.5962 - val_loss: 0.4232 - val_accuracy: 0.5529\n",
      "Epoch 837/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1668 - accuracy: 0.5985 - val_loss: 0.4761 - val_accuracy: 0.5412\n",
      "Epoch 838/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1778 - accuracy: 0.5955 - val_loss: 0.4526 - val_accuracy: 0.5529\n",
      "Epoch 839/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1912 - accuracy: 0.5929 - val_loss: 0.4570 - val_accuracy: 0.5235\n",
      "Epoch 840/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.1751 - accuracy: 0.5951 - val_loss: 0.4347 - val_accuracy: 0.5500\n",
      "Epoch 841/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1806 - accuracy: 0.5951 - val_loss: 0.4598 - val_accuracy: 0.5382\n",
      "Epoch 842/1600\n",
      "680/680 [==============================] - 1s 902us/step - loss: 0.1824 - accuracy: 0.5962 - val_loss: 0.4518 - val_accuracy: 0.5471\n",
      "Epoch 843/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1749 - accuracy: 0.5977 - val_loss: 0.4316 - val_accuracy: 0.5471\n",
      "Epoch 844/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1842 - accuracy: 0.5926 - val_loss: 0.4242 - val_accuracy: 0.5471\n",
      "Epoch 845/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1684 - accuracy: 0.5970 - val_loss: 0.4329 - val_accuracy: 0.5559\n",
      "Epoch 846/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1733 - accuracy: 0.5985 - val_loss: 0.4060 - val_accuracy: 0.5500\n",
      "Epoch 847/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.1828 - accuracy: 0.5951 - val_loss: 0.4329 - val_accuracy: 0.5500\n",
      "Epoch 848/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1795 - accuracy: 0.5937 - val_loss: 0.4434 - val_accuracy: 0.5500\n",
      "Epoch 849/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1875 - accuracy: 0.5955 - val_loss: 0.4626 - val_accuracy: 0.5441\n",
      "Epoch 850/1600\n",
      "680/680 [==============================] - 1s 864us/step - loss: 0.1824 - accuracy: 0.5944 - val_loss: 0.4320 - val_accuracy: 0.5471\n",
      "Epoch 851/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1896 - accuracy: 0.5937 - val_loss: 0.4559 - val_accuracy: 0.5412\n",
      "Epoch 852/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1687 - accuracy: 0.5937 - val_loss: 0.4344 - val_accuracy: 0.5382\n",
      "Epoch 853/1600\n",
      "680/680 [==============================] - 1s 868us/step - loss: 0.1812 - accuracy: 0.5948 - val_loss: 0.3901 - val_accuracy: 0.5441\n",
      "Epoch 854/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1924 - accuracy: 0.5929 - val_loss: 0.4309 - val_accuracy: 0.5382\n",
      "Epoch 855/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1868 - accuracy: 0.5955 - val_loss: 0.4450 - val_accuracy: 0.5441\n",
      "Epoch 856/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1679 - accuracy: 0.5985 - val_loss: 0.4657 - val_accuracy: 0.5471\n",
      "Epoch 857/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1726 - accuracy: 0.5944 - val_loss: 0.4559 - val_accuracy: 0.5324\n",
      "Epoch 858/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1848 - accuracy: 0.5948 - val_loss: 0.4123 - val_accuracy: 0.5441\n",
      "Epoch 859/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1736 - accuracy: 0.5951 - val_loss: 0.4017 - val_accuracy: 0.5412\n",
      "Epoch 860/1600\n",
      "680/680 [==============================] - 1s 868us/step - loss: 0.1792 - accuracy: 0.5951 - val_loss: 0.4077 - val_accuracy: 0.5441\n",
      "Epoch 861/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1712 - accuracy: 0.5962 - val_loss: 0.4073 - val_accuracy: 0.5500\n",
      "Epoch 862/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.2208 - accuracy: 0.5878 - val_loss: 0.4311 - val_accuracy: 0.5441\n",
      "Epoch 863/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1718 - accuracy: 0.5977 - val_loss: 0.4400 - val_accuracy: 0.5412\n",
      "Epoch 864/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1705 - accuracy: 0.5992 - val_loss: 0.4505 - val_accuracy: 0.5206\n",
      "Epoch 865/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1821 - accuracy: 0.5966 - val_loss: 0.4632 - val_accuracy: 0.5471\n",
      "Epoch 866/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1766 - accuracy: 0.5966 - val_loss: 0.4728 - val_accuracy: 0.5294\n",
      "Epoch 867/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1850 - accuracy: 0.5948 - val_loss: 0.4448 - val_accuracy: 0.5324\n",
      "Epoch 868/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1751 - accuracy: 0.5966 - val_loss: 0.4258 - val_accuracy: 0.5618\n",
      "Epoch 869/1600\n",
      "680/680 [==============================] - 1s 936us/step - loss: 0.1751 - accuracy: 0.5996 - val_loss: 0.4521 - val_accuracy: 0.5412\n",
      "Epoch 870/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1670 - accuracy: 0.5977 - val_loss: 0.4690 - val_accuracy: 0.5382\n",
      "Epoch 871/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1825 - accuracy: 0.5940 - val_loss: 0.4610 - val_accuracy: 0.5382\n",
      "Epoch 872/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1778 - accuracy: 0.5970 - val_loss: 0.4393 - val_accuracy: 0.5412\n",
      "Epoch 873/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1797 - accuracy: 0.5974 - val_loss: 0.4356 - val_accuracy: 0.5412\n",
      "Epoch 874/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1754 - accuracy: 0.5996 - val_loss: 0.4295 - val_accuracy: 0.5471\n",
      "Epoch 875/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1738 - accuracy: 0.5937 - val_loss: 0.4293 - val_accuracy: 0.5529\n",
      "Epoch 876/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1746 - accuracy: 0.5988 - val_loss: 0.4675 - val_accuracy: 0.5176\n",
      "Epoch 877/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1832 - accuracy: 0.5962 - val_loss: 0.4491 - val_accuracy: 0.5412\n",
      "Epoch 878/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.1807 - accuracy: 0.5966 - val_loss: 0.4407 - val_accuracy: 0.5324\n",
      "Epoch 879/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1898 - accuracy: 0.5933 - val_loss: 0.4451 - val_accuracy: 0.5382\n",
      "Epoch 880/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1875 - accuracy: 0.5922 - val_loss: 0.4138 - val_accuracy: 0.5412\n",
      "Epoch 881/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1694 - accuracy: 0.5970 - val_loss: 0.4243 - val_accuracy: 0.5471\n",
      "Epoch 882/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1847 - accuracy: 0.5933 - val_loss: 0.4022 - val_accuracy: 0.5559\n",
      "Epoch 883/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 914us/step - loss: 0.1699 - accuracy: 0.5985 - val_loss: 0.4276 - val_accuracy: 0.5441\n",
      "Epoch 884/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1847 - accuracy: 0.5959 - val_loss: 0.4396 - val_accuracy: 0.5412\n",
      "Epoch 885/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1904 - accuracy: 0.5951 - val_loss: 0.4208 - val_accuracy: 0.5618\n",
      "Epoch 886/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1760 - accuracy: 0.5959 - val_loss: 0.4379 - val_accuracy: 0.5353\n",
      "Epoch 887/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1706 - accuracy: 0.5966 - val_loss: 0.4485 - val_accuracy: 0.5441\n",
      "Epoch 888/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1866 - accuracy: 0.5922 - val_loss: 0.4423 - val_accuracy: 0.5559\n",
      "Epoch 889/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1812 - accuracy: 0.5940 - val_loss: 0.4433 - val_accuracy: 0.5441\n",
      "Epoch 890/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1724 - accuracy: 0.6010 - val_loss: 0.4233 - val_accuracy: 0.5382\n",
      "Epoch 891/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1780 - accuracy: 0.5974 - val_loss: 0.4907 - val_accuracy: 0.5206\n",
      "Epoch 892/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1836 - accuracy: 0.5926 - val_loss: 0.4434 - val_accuracy: 0.5382\n",
      "Epoch 893/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1783 - accuracy: 0.5966 - val_loss: 0.4118 - val_accuracy: 0.5412\n",
      "Epoch 894/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1744 - accuracy: 0.5970 - val_loss: 0.4498 - val_accuracy: 0.5294\n",
      "Epoch 895/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1855 - accuracy: 0.5944 - val_loss: 0.4225 - val_accuracy: 0.5382\n",
      "Epoch 896/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1839 - accuracy: 0.5955 - val_loss: 0.4200 - val_accuracy: 0.5441\n",
      "Epoch 897/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1733 - accuracy: 0.5970 - val_loss: 0.4264 - val_accuracy: 0.5441\n",
      "Epoch 898/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1738 - accuracy: 0.5966 - val_loss: 0.4454 - val_accuracy: 0.5382\n",
      "Epoch 899/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1775 - accuracy: 0.5977 - val_loss: 0.4573 - val_accuracy: 0.5412\n",
      "Epoch 900/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1913 - accuracy: 0.5929 - val_loss: 0.4681 - val_accuracy: 0.5294\n",
      "Epoch 901/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1833 - accuracy: 0.5940 - val_loss: 0.3768 - val_accuracy: 0.5529\n",
      "Epoch 902/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1888 - accuracy: 0.5937 - val_loss: 0.4689 - val_accuracy: 0.5353\n",
      "Epoch 903/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1849 - accuracy: 0.5933 - val_loss: 0.4311 - val_accuracy: 0.5382\n",
      "Epoch 904/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1686 - accuracy: 0.5985 - val_loss: 0.4758 - val_accuracy: 0.5529\n",
      "Epoch 905/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1817 - accuracy: 0.5948 - val_loss: 0.4252 - val_accuracy: 0.5382\n",
      "Epoch 906/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1851 - accuracy: 0.5966 - val_loss: 0.4314 - val_accuracy: 0.5412\n",
      "Epoch 907/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1855 - accuracy: 0.5944 - val_loss: 0.4279 - val_accuracy: 0.5500\n",
      "Epoch 908/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1751 - accuracy: 0.5962 - val_loss: 0.4210 - val_accuracy: 0.5471\n",
      "Epoch 909/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1937 - accuracy: 0.5944 - val_loss: 0.4486 - val_accuracy: 0.5382\n",
      "Epoch 910/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1782 - accuracy: 0.5929 - val_loss: 0.4356 - val_accuracy: 0.5471\n",
      "Epoch 911/1600\n",
      "680/680 [==============================] - 1s 863us/step - loss: 0.1747 - accuracy: 0.5966 - val_loss: 0.4259 - val_accuracy: 0.5471\n",
      "Epoch 912/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1631 - accuracy: 0.6021 - val_loss: 0.4580 - val_accuracy: 0.5382\n",
      "Epoch 913/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1895 - accuracy: 0.5926 - val_loss: 0.4172 - val_accuracy: 0.5382\n",
      "Epoch 914/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1789 - accuracy: 0.5933 - val_loss: 0.4212 - val_accuracy: 0.5441\n",
      "Epoch 915/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1758 - accuracy: 0.5955 - val_loss: 0.4387 - val_accuracy: 0.5382\n",
      "Epoch 916/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1746 - accuracy: 0.5966 - val_loss: 0.4296 - val_accuracy: 0.5412\n",
      "Epoch 917/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1797 - accuracy: 0.5933 - val_loss: 0.4176 - val_accuracy: 0.5471\n",
      "Epoch 918/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1789 - accuracy: 0.5985 - val_loss: 0.4703 - val_accuracy: 0.5324\n",
      "Epoch 919/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1697 - accuracy: 0.5974 - val_loss: 0.3895 - val_accuracy: 0.5647\n",
      "Epoch 920/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1844 - accuracy: 0.5926 - val_loss: 0.4207 - val_accuracy: 0.5588\n",
      "Epoch 921/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1740 - accuracy: 0.5974 - val_loss: 0.4554 - val_accuracy: 0.5353\n",
      "Epoch 922/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1737 - accuracy: 0.5966 - val_loss: 0.4081 - val_accuracy: 0.5412\n",
      "Epoch 923/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1799 - accuracy: 0.5940 - val_loss: 0.4495 - val_accuracy: 0.5382\n",
      "Epoch 924/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1765 - accuracy: 0.5988 - val_loss: 0.4404 - val_accuracy: 0.5382\n",
      "Epoch 925/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1690 - accuracy: 0.5992 - val_loss: 0.4539 - val_accuracy: 0.5471\n",
      "Epoch 926/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1778 - accuracy: 0.5944 - val_loss: 0.4452 - val_accuracy: 0.5412\n",
      "Epoch 927/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1800 - accuracy: 0.5948 - val_loss: 0.4355 - val_accuracy: 0.5412\n",
      "Epoch 928/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1665 - accuracy: 0.5977 - val_loss: 0.4314 - val_accuracy: 0.5412\n",
      "Epoch 929/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1771 - accuracy: 0.5955 - val_loss: 0.3747 - val_accuracy: 0.5588\n",
      "Epoch 930/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1950 - accuracy: 0.5940 - val_loss: 0.4029 - val_accuracy: 0.5529\n",
      "Epoch 931/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1700 - accuracy: 0.5977 - val_loss: 0.4329 - val_accuracy: 0.5294\n",
      "Epoch 932/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1830 - accuracy: 0.5955 - val_loss: 0.4076 - val_accuracy: 0.5471\n",
      "Epoch 933/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1755 - accuracy: 0.5970 - val_loss: 0.4399 - val_accuracy: 0.5353\n",
      "Epoch 934/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1794 - accuracy: 0.5955 - val_loss: 0.4524 - val_accuracy: 0.5412\n",
      "Epoch 935/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1899 - accuracy: 0.5904 - val_loss: 0.4323 - val_accuracy: 0.5382\n",
      "Epoch 936/1600\n",
      "680/680 [==============================] - 1s 868us/step - loss: 0.1739 - accuracy: 0.5962 - val_loss: 0.4291 - val_accuracy: 0.5471\n",
      "Epoch 937/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1697 - accuracy: 0.5962 - val_loss: 0.4259 - val_accuracy: 0.5529\n",
      "Epoch 938/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1940 - accuracy: 0.5926 - val_loss: 0.4070 - val_accuracy: 0.5529\n",
      "Epoch 939/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1793 - accuracy: 0.5926 - val_loss: 0.4294 - val_accuracy: 0.5382\n",
      "Epoch 940/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1730 - accuracy: 0.5962 - val_loss: 0.4138 - val_accuracy: 0.5500\n",
      "Epoch 941/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1781 - accuracy: 0.5933 - val_loss: 0.4358 - val_accuracy: 0.5471\n",
      "Epoch 942/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1874 - accuracy: 0.5937 - val_loss: 0.4234 - val_accuracy: 0.5353\n",
      "Epoch 943/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1712 - accuracy: 0.5959 - val_loss: 0.3970 - val_accuracy: 0.5412\n",
      "Epoch 944/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1738 - accuracy: 0.5966 - val_loss: 0.4057 - val_accuracy: 0.5324\n",
      "Epoch 945/1600\n",
      "680/680 [==============================] - 1s 968us/step - loss: 0.1868 - accuracy: 0.5929 - val_loss: 0.4284 - val_accuracy: 0.5441\n",
      "Epoch 946/1600\n",
      "680/680 [==============================] - 1s 949us/step - loss: 0.1809 - accuracy: 0.5951 - val_loss: 0.4342 - val_accuracy: 0.5412\n",
      "Epoch 947/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1865 - accuracy: 0.5951 - val_loss: 0.4098 - val_accuracy: 0.5412\n",
      "Epoch 948/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1702 - accuracy: 0.5981 - val_loss: 0.4292 - val_accuracy: 0.5353\n",
      "Epoch 949/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1718 - accuracy: 0.5977 - val_loss: 0.4110 - val_accuracy: 0.5412\n",
      "Epoch 950/1600\n",
      "680/680 [==============================] - 1s 990us/step - loss: 0.1839 - accuracy: 0.5937 - val_loss: 0.4091 - val_accuracy: 0.5412\n",
      "Epoch 951/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1841 - accuracy: 0.5948 - val_loss: 0.4269 - val_accuracy: 0.5441\n",
      "Epoch 952/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1824 - accuracy: 0.5929 - val_loss: 0.4432 - val_accuracy: 0.5324\n",
      "Epoch 953/1600\n",
      "680/680 [==============================] - 1s 963us/step - loss: 0.1748 - accuracy: 0.5962 - val_loss: 0.4312 - val_accuracy: 0.5382\n",
      "Epoch 954/1600\n",
      "680/680 [==============================] - 1s 949us/step - loss: 0.1756 - accuracy: 0.5951 - val_loss: 0.4273 - val_accuracy: 0.5353\n",
      "Epoch 955/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1684 - accuracy: 0.5988 - val_loss: 0.4153 - val_accuracy: 0.5412\n",
      "Epoch 956/1600\n",
      "680/680 [==============================] - 1s 854us/step - loss: 0.1724 - accuracy: 0.5981 - val_loss: 0.4452 - val_accuracy: 0.5265\n",
      "Epoch 957/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1753 - accuracy: 0.5974 - val_loss: 0.4200 - val_accuracy: 0.5588\n",
      "Epoch 958/1600\n",
      "680/680 [==============================] - 1s 941us/step - loss: 0.1936 - accuracy: 0.5937 - val_loss: 0.4279 - val_accuracy: 0.5382\n",
      "Epoch 959/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1679 - accuracy: 0.5948 - val_loss: 0.4372 - val_accuracy: 0.5471\n",
      "Epoch 960/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1677 - accuracy: 0.5985 - val_loss: 0.4385 - val_accuracy: 0.5441\n",
      "Epoch 961/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1818 - accuracy: 0.5926 - val_loss: 0.4491 - val_accuracy: 0.5353\n",
      "Epoch 962/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1775 - accuracy: 0.5966 - val_loss: 0.4342 - val_accuracy: 0.5529\n",
      "Epoch 963/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1776 - accuracy: 0.5955 - val_loss: 0.4446 - val_accuracy: 0.5382\n",
      "Epoch 964/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1692 - accuracy: 0.5985 - val_loss: 0.4325 - val_accuracy: 0.5412\n",
      "Epoch 965/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1747 - accuracy: 0.5970 - val_loss: 0.4190 - val_accuracy: 0.5382\n",
      "Epoch 966/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1745 - accuracy: 0.5962 - val_loss: 0.4246 - val_accuracy: 0.5529\n",
      "Epoch 967/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1707 - accuracy: 0.5974 - val_loss: 0.4291 - val_accuracy: 0.5441\n",
      "Epoch 968/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1884 - accuracy: 0.5948 - val_loss: 0.4500 - val_accuracy: 0.5353\n",
      "Epoch 969/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1756 - accuracy: 0.5959 - val_loss: 0.4260 - val_accuracy: 0.5324\n",
      "Epoch 970/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1843 - accuracy: 0.5922 - val_loss: 0.4441 - val_accuracy: 0.5294\n",
      "Epoch 971/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1659 - accuracy: 0.5985 - val_loss: 0.4148 - val_accuracy: 0.5500\n",
      "Epoch 972/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1888 - accuracy: 0.5915 - val_loss: 0.4650 - val_accuracy: 0.5353\n",
      "Epoch 973/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1714 - accuracy: 0.5977 - val_loss: 0.4693 - val_accuracy: 0.5353\n",
      "Epoch 974/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1679 - accuracy: 0.5981 - val_loss: 0.4014 - val_accuracy: 0.5647\n",
      "Epoch 975/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2000 - accuracy: 0.5911 - val_loss: 0.4384 - val_accuracy: 0.5441\n",
      "Epoch 976/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.5974 - val_loss: 0.4458 - val_accuracy: 0.5294\n",
      "Epoch 977/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1726 - accuracy: 0.5955 - val_loss: 0.4231 - val_accuracy: 0.5559\n",
      "Epoch 978/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1815 - accuracy: 0.5944 - val_loss: 0.4438 - val_accuracy: 0.5324\n",
      "Epoch 979/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1676 - accuracy: 0.5970 - val_loss: 0.4197 - val_accuracy: 0.5500\n",
      "Epoch 980/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1654 - accuracy: 0.5988 - val_loss: 0.4161 - val_accuracy: 0.5235\n",
      "Epoch 981/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1791 - accuracy: 0.5944 - val_loss: 0.4051 - val_accuracy: 0.5471\n",
      "Epoch 982/1600\n",
      "680/680 [==============================] - 1s 943us/step - loss: 0.1738 - accuracy: 0.5970 - val_loss: 0.4183 - val_accuracy: 0.5471\n",
      "Epoch 983/1600\n",
      "680/680 [==============================] - 1s 930us/step - loss: 0.1882 - accuracy: 0.5918 - val_loss: 0.4003 - val_accuracy: 0.5412\n",
      "Epoch 984/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1701 - accuracy: 0.5985 - val_loss: 0.4282 - val_accuracy: 0.5353\n",
      "Epoch 985/1600\n",
      "680/680 [==============================] - 1s 965us/step - loss: 0.1847 - accuracy: 0.5948 - val_loss: 0.4531 - val_accuracy: 0.5294\n",
      "Epoch 986/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1741 - accuracy: 0.5966 - val_loss: 0.4804 - val_accuracy: 0.5265\n",
      "Epoch 987/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1776 - accuracy: 0.5970 - val_loss: 0.3986 - val_accuracy: 0.5441\n",
      "Epoch 988/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1744 - accuracy: 0.5959 - val_loss: 0.4389 - val_accuracy: 0.5471\n",
      "Epoch 989/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1718 - accuracy: 0.5977 - val_loss: 0.4457 - val_accuracy: 0.5353\n",
      "Epoch 990/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1774 - accuracy: 0.5977 - val_loss: 0.4179 - val_accuracy: 0.5500\n",
      "Epoch 991/1600\n",
      "680/680 [==============================] - 1s 866us/step - loss: 0.1762 - accuracy: 0.5988 - val_loss: 0.4035 - val_accuracy: 0.5529\n",
      "Epoch 992/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1670 - accuracy: 0.5999 - val_loss: 0.4308 - val_accuracy: 0.5353\n",
      "Epoch 993/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 879us/step - loss: 0.1816 - accuracy: 0.5948 - val_loss: 0.4065 - val_accuracy: 0.5559\n",
      "Epoch 994/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1984 - accuracy: 0.5904 - val_loss: 0.4340 - val_accuracy: 0.5441\n",
      "Epoch 995/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1765 - accuracy: 0.5951 - val_loss: 0.4116 - val_accuracy: 0.5441\n",
      "Epoch 996/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1885 - accuracy: 0.5933 - val_loss: 0.4698 - val_accuracy: 0.5206\n",
      "Epoch 997/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1729 - accuracy: 0.5977 - val_loss: 0.4226 - val_accuracy: 0.5471\n",
      "Epoch 998/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1679 - accuracy: 0.5977 - val_loss: 0.4201 - val_accuracy: 0.5441\n",
      "Epoch 999/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1815 - accuracy: 0.5948 - val_loss: 0.4013 - val_accuracy: 0.5500\n",
      "Epoch 1000/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1907 - accuracy: 0.5933 - val_loss: 0.4527 - val_accuracy: 0.5353\n",
      "Epoch 1001/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1743 - accuracy: 0.5974 - val_loss: 0.4523 - val_accuracy: 0.5353\n",
      "Epoch 1002/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1722 - accuracy: 0.5962 - val_loss: 0.3949 - val_accuracy: 0.5500\n",
      "Epoch 1003/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1805 - accuracy: 0.5974 - val_loss: 0.4187 - val_accuracy: 0.5500\n",
      "Epoch 1004/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1753 - accuracy: 0.5962 - val_loss: 0.4471 - val_accuracy: 0.5382\n",
      "Epoch 1005/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1599 - accuracy: 0.6007 - val_loss: 0.4314 - val_accuracy: 0.5441\n",
      "Epoch 1006/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1754 - accuracy: 0.5955 - val_loss: 0.5028 - val_accuracy: 0.5353\n",
      "Epoch 1007/1600\n",
      "680/680 [==============================] - 1s 862us/step - loss: 0.1905 - accuracy: 0.5933 - val_loss: 0.4630 - val_accuracy: 0.5176\n",
      "Epoch 1008/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1861 - accuracy: 0.5933 - val_loss: 0.4309 - val_accuracy: 0.5529\n",
      "Epoch 1009/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1736 - accuracy: 0.5962 - val_loss: 0.4405 - val_accuracy: 0.5647\n",
      "Epoch 1010/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1847 - accuracy: 0.5937 - val_loss: 0.4321 - val_accuracy: 0.5382\n",
      "Epoch 1011/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1638 - accuracy: 0.5985 - val_loss: 0.4265 - val_accuracy: 0.5471\n",
      "Epoch 1012/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1819 - accuracy: 0.5959 - val_loss: 0.4597 - val_accuracy: 0.5324\n",
      "Epoch 1013/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1766 - accuracy: 0.5937 - val_loss: 0.4315 - val_accuracy: 0.5324\n",
      "Epoch 1014/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1699 - accuracy: 0.5977 - val_loss: 0.4282 - val_accuracy: 0.5471\n",
      "Epoch 1015/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1657 - accuracy: 0.5985 - val_loss: 0.4116 - val_accuracy: 0.5529\n",
      "Epoch 1016/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1740 - accuracy: 0.5985 - val_loss: 0.4323 - val_accuracy: 0.5471\n",
      "Epoch 1017/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1855 - accuracy: 0.5959 - val_loss: 0.3811 - val_accuracy: 0.5588\n",
      "Epoch 1018/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1722 - accuracy: 0.5970 - val_loss: 0.4561 - val_accuracy: 0.5382\n",
      "Epoch 1019/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1729 - accuracy: 0.5974 - val_loss: 0.4487 - val_accuracy: 0.5324\n",
      "Epoch 1020/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.1856 - accuracy: 0.5933 - val_loss: 0.4395 - val_accuracy: 0.5471\n",
      "Epoch 1021/1600\n",
      "680/680 [==============================] - 1s 907us/step - loss: 0.1683 - accuracy: 0.5992 - val_loss: 0.4462 - val_accuracy: 0.5382\n",
      "Epoch 1022/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1742 - accuracy: 0.5962 - val_loss: 0.4095 - val_accuracy: 0.5500\n",
      "Epoch 1023/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.1675 - accuracy: 0.5999 - val_loss: 0.4408 - val_accuracy: 0.5353\n",
      "Epoch 1024/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1640 - accuracy: 0.5985 - val_loss: 0.4211 - val_accuracy: 0.5471\n",
      "Epoch 1025/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1707 - accuracy: 0.5970 - val_loss: 0.4111 - val_accuracy: 0.5618\n",
      "Epoch 1026/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.2000 - accuracy: 0.5911 - val_loss: 0.4291 - val_accuracy: 0.5500\n",
      "Epoch 1027/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1795 - accuracy: 0.5959 - val_loss: 0.4217 - val_accuracy: 0.5382\n",
      "Epoch 1028/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1827 - accuracy: 0.5970 - val_loss: 0.4289 - val_accuracy: 0.5382\n",
      "Epoch 1029/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1892 - accuracy: 0.5948 - val_loss: 0.4150 - val_accuracy: 0.5441\n",
      "Epoch 1030/1600\n",
      "680/680 [==============================] - 1s 863us/step - loss: 0.1700 - accuracy: 0.5977 - val_loss: 0.4723 - val_accuracy: 0.5235\n",
      "Epoch 1031/1600\n",
      "680/680 [==============================] - 1s 904us/step - loss: 0.1933 - accuracy: 0.5929 - val_loss: 0.4181 - val_accuracy: 0.5382\n",
      "Epoch 1032/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1888 - accuracy: 0.5940 - val_loss: 0.4215 - val_accuracy: 0.5235\n",
      "Epoch 1033/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1826 - accuracy: 0.5937 - val_loss: 0.4022 - val_accuracy: 0.5500\n",
      "Epoch 1034/1600\n",
      "680/680 [==============================] - 1s 866us/step - loss: 0.1695 - accuracy: 0.5974 - val_loss: 0.4042 - val_accuracy: 0.5471\n",
      "Epoch 1035/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1773 - accuracy: 0.5962 - val_loss: 0.4275 - val_accuracy: 0.5441\n",
      "Epoch 1036/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1658 - accuracy: 0.5977 - val_loss: 0.4464 - val_accuracy: 0.5412\n",
      "Epoch 1037/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1945 - accuracy: 0.5933 - val_loss: 0.4103 - val_accuracy: 0.5412\n",
      "Epoch 1038/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1648 - accuracy: 0.5988 - val_loss: 0.3692 - val_accuracy: 0.5559\n",
      "Epoch 1039/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1862 - accuracy: 0.5955 - val_loss: 0.4195 - val_accuracy: 0.5412\n",
      "Epoch 1040/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1808 - accuracy: 0.5955 - val_loss: 0.4359 - val_accuracy: 0.5353\n",
      "Epoch 1041/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1750 - accuracy: 0.5955 - val_loss: 0.4498 - val_accuracy: 0.5353\n",
      "Epoch 1042/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1823 - accuracy: 0.5944 - val_loss: 0.4117 - val_accuracy: 0.5441\n",
      "Epoch 1043/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1790 - accuracy: 0.5970 - val_loss: 0.4471 - val_accuracy: 0.5441\n",
      "Epoch 1044/1600\n",
      "680/680 [==============================] - 1s 868us/step - loss: 0.1864 - accuracy: 0.5944 - val_loss: 0.3881 - val_accuracy: 0.5500\n",
      "Epoch 1045/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1796 - accuracy: 0.5974 - val_loss: 0.4305 - val_accuracy: 0.5412\n",
      "Epoch 1046/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1726 - accuracy: 0.5970 - val_loss: 0.4342 - val_accuracy: 0.5324\n",
      "Epoch 1047/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1777 - accuracy: 0.5940 - val_loss: 0.4800 - val_accuracy: 0.5324\n",
      "Epoch 1048/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1800 - accuracy: 0.5959 - val_loss: 0.3930 - val_accuracy: 0.5441\n",
      "Epoch 1049/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1793 - accuracy: 0.5962 - val_loss: 0.4259 - val_accuracy: 0.5382\n",
      "Epoch 1050/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1796 - accuracy: 0.5951 - val_loss: 0.4211 - val_accuracy: 0.5353\n",
      "Epoch 1051/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1631 - accuracy: 0.5974 - val_loss: 0.4188 - val_accuracy: 0.5265\n",
      "Epoch 1052/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1738 - accuracy: 0.5966 - val_loss: 0.4199 - val_accuracy: 0.5529\n",
      "Epoch 1053/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1762 - accuracy: 0.5937 - val_loss: 0.4757 - val_accuracy: 0.5294\n",
      "Epoch 1054/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1753 - accuracy: 0.5966 - val_loss: 0.4136 - val_accuracy: 0.5500\n",
      "Epoch 1055/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1728 - accuracy: 0.5955 - val_loss: 0.4297 - val_accuracy: 0.5618\n",
      "Epoch 1056/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1967 - accuracy: 0.5922 - val_loss: 0.4047 - val_accuracy: 0.5471\n",
      "Epoch 1057/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1633 - accuracy: 0.5992 - val_loss: 0.4319 - val_accuracy: 0.5382\n",
      "Epoch 1058/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1905 - accuracy: 0.5959 - val_loss: 0.4126 - val_accuracy: 0.5529\n",
      "Epoch 1059/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1613 - accuracy: 0.5992 - val_loss: 0.4277 - val_accuracy: 0.5412\n",
      "Epoch 1060/1600\n",
      "680/680 [==============================] - 1s 906us/step - loss: 0.1741 - accuracy: 0.5966 - val_loss: 0.4409 - val_accuracy: 0.5382\n",
      "Epoch 1061/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1870 - accuracy: 0.5948 - val_loss: 0.4293 - val_accuracy: 0.5382\n",
      "Epoch 1062/1600\n",
      "680/680 [==============================] - 1s 868us/step - loss: 0.1611 - accuracy: 0.5985 - val_loss: 0.4395 - val_accuracy: 0.5294\n",
      "Epoch 1063/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1855 - accuracy: 0.5951 - val_loss: 0.4124 - val_accuracy: 0.5471\n",
      "Epoch 1064/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1784 - accuracy: 0.5974 - val_loss: 0.4021 - val_accuracy: 0.5500\n",
      "Epoch 1065/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1799 - accuracy: 0.5944 - val_loss: 0.4032 - val_accuracy: 0.5412\n",
      "Epoch 1066/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1902 - accuracy: 0.5955 - val_loss: 0.3964 - val_accuracy: 0.5559\n",
      "Epoch 1067/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1752 - accuracy: 0.5966 - val_loss: 0.4107 - val_accuracy: 0.5382\n",
      "Epoch 1068/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1850 - accuracy: 0.5959 - val_loss: 0.3968 - val_accuracy: 0.5382\n",
      "Epoch 1069/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1886 - accuracy: 0.5929 - val_loss: 0.4552 - val_accuracy: 0.5206\n",
      "Epoch 1070/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1906 - accuracy: 0.5922 - val_loss: 0.4663 - val_accuracy: 0.5265\n",
      "Epoch 1071/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1623 - accuracy: 0.5974 - val_loss: 0.3908 - val_accuracy: 0.5500\n",
      "Epoch 1072/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1796 - accuracy: 0.5951 - val_loss: 0.4362 - val_accuracy: 0.5353\n",
      "Epoch 1073/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1705 - accuracy: 0.5951 - val_loss: 0.4023 - val_accuracy: 0.5441\n",
      "Epoch 1074/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1700 - accuracy: 0.5992 - val_loss: 0.4209 - val_accuracy: 0.5441\n",
      "Epoch 1075/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1896 - accuracy: 0.5962 - val_loss: 0.3951 - val_accuracy: 0.5529\n",
      "Epoch 1076/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1784 - accuracy: 0.5970 - val_loss: 0.4151 - val_accuracy: 0.5529\n",
      "Epoch 1077/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1733 - accuracy: 0.5970 - val_loss: 0.4217 - val_accuracy: 0.5618\n",
      "Epoch 1078/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1830 - accuracy: 0.5951 - val_loss: 0.4486 - val_accuracy: 0.5412\n",
      "Epoch 1079/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1767 - accuracy: 0.5966 - val_loss: 0.4518 - val_accuracy: 0.5324\n",
      "Epoch 1080/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1719 - accuracy: 0.5985 - val_loss: 0.4417 - val_accuracy: 0.5500\n",
      "Epoch 1081/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1747 - accuracy: 0.5959 - val_loss: 0.4284 - val_accuracy: 0.5235\n",
      "Epoch 1082/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1828 - accuracy: 0.5962 - val_loss: 0.4478 - val_accuracy: 0.5412\n",
      "Epoch 1083/1600\n",
      "680/680 [==============================] - 1s 902us/step - loss: 0.1788 - accuracy: 0.5929 - val_loss: 0.4260 - val_accuracy: 0.5529\n",
      "Epoch 1084/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1731 - accuracy: 0.5966 - val_loss: 0.4164 - val_accuracy: 0.5412\n",
      "Epoch 1085/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1737 - accuracy: 0.5940 - val_loss: 0.4253 - val_accuracy: 0.5441\n",
      "Epoch 1086/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1764 - accuracy: 0.5962 - val_loss: 0.4445 - val_accuracy: 0.5353\n",
      "Epoch 1087/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1793 - accuracy: 0.5962 - val_loss: 0.4736 - val_accuracy: 0.5265\n",
      "Epoch 1088/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.1679 - accuracy: 0.5988 - val_loss: 0.4434 - val_accuracy: 0.5324\n",
      "Epoch 1089/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1607 - accuracy: 0.5988 - val_loss: 0.4208 - val_accuracy: 0.5294\n",
      "Epoch 1090/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1815 - accuracy: 0.5951 - val_loss: 0.4847 - val_accuracy: 0.5353\n",
      "Epoch 1091/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1852 - accuracy: 0.5937 - val_loss: 0.4033 - val_accuracy: 0.5471\n",
      "Epoch 1092/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1735 - accuracy: 0.5959 - val_loss: 0.4261 - val_accuracy: 0.5500\n",
      "Epoch 1093/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1799 - accuracy: 0.5959 - val_loss: 0.4501 - val_accuracy: 0.5412\n",
      "Epoch 1094/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1887 - accuracy: 0.5933 - val_loss: 0.4234 - val_accuracy: 0.5353\n",
      "Epoch 1095/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1666 - accuracy: 0.5996 - val_loss: 0.4310 - val_accuracy: 0.5441\n",
      "Epoch 1096/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1921 - accuracy: 0.5944 - val_loss: 0.4191 - val_accuracy: 0.5382\n",
      "Epoch 1097/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1801 - accuracy: 0.5944 - val_loss: 0.4354 - val_accuracy: 0.5382\n",
      "Epoch 1098/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1761 - accuracy: 0.5955 - val_loss: 0.4004 - val_accuracy: 0.5559\n",
      "Epoch 1099/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1730 - accuracy: 0.5977 - val_loss: 0.4511 - val_accuracy: 0.5382\n",
      "Epoch 1100/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1692 - accuracy: 0.5966 - val_loss: 0.4781 - val_accuracy: 0.5647\n",
      "Epoch 1101/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.1707 - accuracy: 0.5981 - val_loss: 0.4275 - val_accuracy: 0.5471\n",
      "Epoch 1102/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1681 - accuracy: 0.5992 - val_loss: 0.4506 - val_accuracy: 0.5441\n",
      "Epoch 1103/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 886us/step - loss: 0.1896 - accuracy: 0.5933 - val_loss: 0.4405 - val_accuracy: 0.5382\n",
      "Epoch 1104/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1705 - accuracy: 0.5955 - val_loss: 0.3942 - val_accuracy: 0.5559\n",
      "Epoch 1105/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1686 - accuracy: 0.5977 - val_loss: 0.4290 - val_accuracy: 0.5471\n",
      "Epoch 1106/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1731 - accuracy: 0.5988 - val_loss: 0.4368 - val_accuracy: 0.5441\n",
      "Epoch 1107/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1669 - accuracy: 0.5970 - val_loss: 0.4490 - val_accuracy: 0.5353\n",
      "Epoch 1108/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1791 - accuracy: 0.5977 - val_loss: 0.4146 - val_accuracy: 0.5529\n",
      "Epoch 1109/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.2028 - accuracy: 0.5922 - val_loss: 0.4133 - val_accuracy: 0.5559\n",
      "Epoch 1110/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1714 - accuracy: 0.5985 - val_loss: 0.4468 - val_accuracy: 0.5412\n",
      "Epoch 1111/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1700 - accuracy: 0.5985 - val_loss: 0.4418 - val_accuracy: 0.5294\n",
      "Epoch 1112/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1721 - accuracy: 0.5974 - val_loss: 0.4454 - val_accuracy: 0.5353\n",
      "Epoch 1113/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1769 - accuracy: 0.5996 - val_loss: 0.4439 - val_accuracy: 0.5353\n",
      "Epoch 1114/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1732 - accuracy: 0.5988 - val_loss: 0.4532 - val_accuracy: 0.5294\n",
      "Epoch 1115/1600\n",
      "680/680 [==============================] - 1s 864us/step - loss: 0.1801 - accuracy: 0.5955 - val_loss: 0.4607 - val_accuracy: 0.5353\n",
      "Epoch 1116/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.1753 - accuracy: 0.5992 - val_loss: 0.4549 - val_accuracy: 0.5353\n",
      "Epoch 1117/1600\n",
      "680/680 [==============================] - 1s 857us/step - loss: 0.1877 - accuracy: 0.5922 - val_loss: 0.4391 - val_accuracy: 0.5382\n",
      "Epoch 1118/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1723 - accuracy: 0.5981 - val_loss: 0.4046 - val_accuracy: 0.5382\n",
      "Epoch 1119/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.2008 - accuracy: 0.5900 - val_loss: 0.4414 - val_accuracy: 0.5412\n",
      "Epoch 1120/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1745 - accuracy: 0.5962 - val_loss: 0.4226 - val_accuracy: 0.5471\n",
      "Epoch 1121/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1734 - accuracy: 0.5977 - val_loss: 0.4631 - val_accuracy: 0.5294\n",
      "Epoch 1122/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1681 - accuracy: 0.5962 - val_loss: 0.4204 - val_accuracy: 0.5500\n",
      "Epoch 1123/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1789 - accuracy: 0.5937 - val_loss: 0.4434 - val_accuracy: 0.5206\n",
      "Epoch 1124/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1780 - accuracy: 0.5974 - val_loss: 0.4422 - val_accuracy: 0.5294\n",
      "Epoch 1125/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1758 - accuracy: 0.5962 - val_loss: 0.4266 - val_accuracy: 0.5441\n",
      "Epoch 1126/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1762 - accuracy: 0.5959 - val_loss: 0.4329 - val_accuracy: 0.5324\n",
      "Epoch 1127/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1818 - accuracy: 0.5974 - val_loss: 0.4520 - val_accuracy: 0.5588\n",
      "Epoch 1128/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1837 - accuracy: 0.5962 - val_loss: 0.4239 - val_accuracy: 0.5471\n",
      "Epoch 1129/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1796 - accuracy: 0.5970 - val_loss: 0.4478 - val_accuracy: 0.5324\n",
      "Epoch 1130/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1863 - accuracy: 0.5951 - val_loss: 0.4002 - val_accuracy: 0.5441\n",
      "Epoch 1131/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1802 - accuracy: 0.5955 - val_loss: 0.4164 - val_accuracy: 0.5382\n",
      "Epoch 1132/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1785 - accuracy: 0.5959 - val_loss: 0.4127 - val_accuracy: 0.5353\n",
      "Epoch 1133/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1798 - accuracy: 0.5999 - val_loss: 0.4715 - val_accuracy: 0.5382\n",
      "Epoch 1134/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1779 - accuracy: 0.5955 - val_loss: 0.4458 - val_accuracy: 0.5353\n",
      "Epoch 1135/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1714 - accuracy: 0.5977 - val_loss: 0.4546 - val_accuracy: 0.5441\n",
      "Epoch 1136/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1741 - accuracy: 0.5970 - val_loss: 0.4406 - val_accuracy: 0.5441\n",
      "Epoch 1137/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1943 - accuracy: 0.5915 - val_loss: 0.4410 - val_accuracy: 0.5382\n",
      "Epoch 1138/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1793 - accuracy: 0.5944 - val_loss: 0.4122 - val_accuracy: 0.5500\n",
      "Epoch 1139/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1735 - accuracy: 0.5977 - val_loss: 0.4438 - val_accuracy: 0.5588\n",
      "Epoch 1140/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1718 - accuracy: 0.5974 - val_loss: 0.4423 - val_accuracy: 0.5441\n",
      "Epoch 1141/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.1684 - accuracy: 0.5985 - val_loss: 0.4268 - val_accuracy: 0.5500\n",
      "Epoch 1142/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1651 - accuracy: 0.5996 - val_loss: 0.4711 - val_accuracy: 0.5382\n",
      "Epoch 1143/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1790 - accuracy: 0.5962 - val_loss: 0.4042 - val_accuracy: 0.5471\n",
      "Epoch 1144/1600\n",
      "680/680 [==============================] - 1s 941us/step - loss: 0.1884 - accuracy: 0.5959 - val_loss: 0.4455 - val_accuracy: 0.5441\n",
      "Epoch 1145/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.1696 - accuracy: 0.5988 - val_loss: 0.4616 - val_accuracy: 0.5382\n",
      "Epoch 1146/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1823 - accuracy: 0.5940 - val_loss: 0.4443 - val_accuracy: 0.5441\n",
      "Epoch 1147/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1803 - accuracy: 0.5959 - val_loss: 0.4480 - val_accuracy: 0.5441\n",
      "Epoch 1148/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1853 - accuracy: 0.5951 - val_loss: 0.4319 - val_accuracy: 0.5441\n",
      "Epoch 1149/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.1804 - accuracy: 0.5951 - val_loss: 0.4331 - val_accuracy: 0.5382\n",
      "Epoch 1150/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1862 - accuracy: 0.5929 - val_loss: 0.4414 - val_accuracy: 0.5353\n",
      "Epoch 1151/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1687 - accuracy: 0.5981 - val_loss: 0.4581 - val_accuracy: 0.5382\n",
      "Epoch 1152/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1713 - accuracy: 0.5959 - val_loss: 0.4136 - val_accuracy: 0.5500\n",
      "Epoch 1153/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1845 - accuracy: 0.5962 - val_loss: 0.4145 - val_accuracy: 0.5471\n",
      "Epoch 1154/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1699 - accuracy: 0.5959 - val_loss: 0.4379 - val_accuracy: 0.5353\n",
      "Epoch 1155/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1765 - accuracy: 0.5962 - val_loss: 0.4338 - val_accuracy: 0.5294\n",
      "Epoch 1156/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1820 - accuracy: 0.5937 - val_loss: 0.4096 - val_accuracy: 0.5382\n",
      "Epoch 1157/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1692 - accuracy: 0.5970 - val_loss: 0.4421 - val_accuracy: 0.5412\n",
      "Epoch 1158/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 877us/step - loss: 0.1824 - accuracy: 0.5962 - val_loss: 0.4718 - val_accuracy: 0.5353\n",
      "Epoch 1159/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1725 - accuracy: 0.5940 - val_loss: 0.4224 - val_accuracy: 0.5500\n",
      "Epoch 1160/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1815 - accuracy: 0.5944 - val_loss: 0.4624 - val_accuracy: 0.5265\n",
      "Epoch 1161/1600\n",
      "680/680 [==============================] - 1s 943us/step - loss: 0.1875 - accuracy: 0.5966 - val_loss: 0.4449 - val_accuracy: 0.5382\n",
      "Epoch 1162/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1667 - accuracy: 0.5988 - val_loss: 0.4340 - val_accuracy: 0.5353\n",
      "Epoch 1163/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1626 - accuracy: 0.5988 - val_loss: 0.4429 - val_accuracy: 0.5441\n",
      "Epoch 1164/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1750 - accuracy: 0.5974 - val_loss: 0.4348 - val_accuracy: 0.5471\n",
      "Epoch 1165/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1874 - accuracy: 0.5926 - val_loss: 0.4373 - val_accuracy: 0.5412\n",
      "Epoch 1166/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1732 - accuracy: 0.5970 - val_loss: 0.4221 - val_accuracy: 0.5382\n",
      "Epoch 1167/1600\n",
      "680/680 [==============================] - 1s 907us/step - loss: 0.1677 - accuracy: 0.5981 - val_loss: 0.4281 - val_accuracy: 0.5412\n",
      "Epoch 1168/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1910 - accuracy: 0.5937 - val_loss: 0.4530 - val_accuracy: 0.5294\n",
      "Epoch 1169/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.2021 - accuracy: 0.5896 - val_loss: 0.4082 - val_accuracy: 0.5265\n",
      "Epoch 1170/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1885 - accuracy: 0.5911 - val_loss: 0.4514 - val_accuracy: 0.5382\n",
      "Epoch 1171/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1759 - accuracy: 0.5955 - val_loss: 0.4567 - val_accuracy: 0.5500\n",
      "Epoch 1172/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1634 - accuracy: 0.5988 - val_loss: 0.4394 - val_accuracy: 0.5382\n",
      "Epoch 1173/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.1661 - accuracy: 0.6014 - val_loss: 0.4506 - val_accuracy: 0.5441\n",
      "Epoch 1174/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1670 - accuracy: 0.5999 - val_loss: 0.4445 - val_accuracy: 0.5500\n",
      "Epoch 1175/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1834 - accuracy: 0.5929 - val_loss: 0.4520 - val_accuracy: 0.5441\n",
      "Epoch 1176/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1845 - accuracy: 0.5933 - val_loss: 0.4479 - val_accuracy: 0.5412\n",
      "Epoch 1177/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1739 - accuracy: 0.5951 - val_loss: 0.4216 - val_accuracy: 0.5500\n",
      "Epoch 1178/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1670 - accuracy: 0.5996 - val_loss: 0.4530 - val_accuracy: 0.5412\n",
      "Epoch 1179/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1821 - accuracy: 0.5966 - val_loss: 0.4337 - val_accuracy: 0.5471\n",
      "Epoch 1180/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1650 - accuracy: 0.5966 - val_loss: 0.4705 - val_accuracy: 0.5382\n",
      "Epoch 1181/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1743 - accuracy: 0.5959 - val_loss: 0.4624 - val_accuracy: 0.5353\n",
      "Epoch 1182/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1695 - accuracy: 0.5977 - val_loss: 0.4643 - val_accuracy: 0.5353\n",
      "Epoch 1183/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1762 - accuracy: 0.5962 - val_loss: 0.4359 - val_accuracy: 0.5294\n",
      "Epoch 1184/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1623 - accuracy: 0.5985 - val_loss: 0.4348 - val_accuracy: 0.5529\n",
      "Epoch 1185/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1652 - accuracy: 0.5985 - val_loss: 0.4136 - val_accuracy: 0.5500\n",
      "Epoch 1186/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1773 - accuracy: 0.5966 - val_loss: 0.4784 - val_accuracy: 0.5265\n",
      "Epoch 1187/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1766 - accuracy: 0.5962 - val_loss: 0.4358 - val_accuracy: 0.5382\n",
      "Epoch 1188/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1697 - accuracy: 0.5966 - val_loss: 0.4247 - val_accuracy: 0.5471\n",
      "Epoch 1189/1600\n",
      "680/680 [==============================] - 1s 1000us/step - loss: 0.1784 - accuracy: 0.5966 - val_loss: 0.4467 - val_accuracy: 0.5588\n",
      "Epoch 1190/1600\n",
      "680/680 [==============================] - 1s 967us/step - loss: 0.1719 - accuracy: 0.5977 - val_loss: 0.4237 - val_accuracy: 0.5471\n",
      "Epoch 1191/1600\n",
      "680/680 [==============================] - 1s 998us/step - loss: 0.1632 - accuracy: 0.5992 - val_loss: 0.4473 - val_accuracy: 0.5471\n",
      "Epoch 1192/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.1820 - accuracy: 0.5951 - val_loss: 0.4413 - val_accuracy: 0.5441\n",
      "Epoch 1193/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1745 - accuracy: 0.5948 - val_loss: 0.4239 - val_accuracy: 0.5441\n",
      "Epoch 1194/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1834 - accuracy: 0.5948 - val_loss: 0.4319 - val_accuracy: 0.5500\n",
      "Epoch 1195/1600\n",
      "680/680 [==============================] - 1s 938us/step - loss: 0.1675 - accuracy: 0.5996 - val_loss: 0.4427 - val_accuracy: 0.5382\n",
      "Epoch 1196/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1648 - accuracy: 0.5992 - val_loss: 0.4297 - val_accuracy: 0.5382\n",
      "Epoch 1197/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1745 - accuracy: 0.5974 - val_loss: 0.4817 - val_accuracy: 0.5265\n",
      "Epoch 1198/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1823 - accuracy: 0.5929 - val_loss: 0.4587 - val_accuracy: 0.5382\n",
      "Epoch 1199/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1659 - accuracy: 0.6003 - val_loss: 0.4330 - val_accuracy: 0.5294\n",
      "Epoch 1200/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1776 - accuracy: 0.5940 - val_loss: 0.4450 - val_accuracy: 0.5382\n",
      "Epoch 1201/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1732 - accuracy: 0.5985 - val_loss: 0.4121 - val_accuracy: 0.5412\n",
      "Epoch 1202/1600\n",
      "680/680 [==============================] - 1s 862us/step - loss: 0.1791 - accuracy: 0.5951 - val_loss: 0.4033 - val_accuracy: 0.5500\n",
      "Epoch 1203/1600\n",
      "680/680 [==============================] - 1s 901us/step - loss: 0.1739 - accuracy: 0.5948 - val_loss: 0.4233 - val_accuracy: 0.5324\n",
      "Epoch 1204/1600\n",
      "680/680 [==============================] - 1s 854us/step - loss: 0.1753 - accuracy: 0.5966 - val_loss: 0.4960 - val_accuracy: 0.5324\n",
      "Epoch 1205/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1777 - accuracy: 0.5959 - val_loss: 0.4457 - val_accuracy: 0.5382\n",
      "Epoch 1206/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1714 - accuracy: 0.5985 - val_loss: 0.4336 - val_accuracy: 0.5588\n",
      "Epoch 1207/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1735 - accuracy: 0.5974 - val_loss: 0.4870 - val_accuracy: 0.5382\n",
      "Epoch 1208/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1960 - accuracy: 0.5951 - val_loss: 0.4530 - val_accuracy: 0.5294\n",
      "Epoch 1209/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.1726 - accuracy: 0.5966 - val_loss: 0.4338 - val_accuracy: 0.5412\n",
      "Epoch 1210/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1697 - accuracy: 0.5970 - val_loss: 0.4648 - val_accuracy: 0.5176\n",
      "Epoch 1211/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1841 - accuracy: 0.5926 - val_loss: 0.4649 - val_accuracy: 0.5412\n",
      "Epoch 1212/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1752 - accuracy: 0.5955 - val_loss: 0.4454 - val_accuracy: 0.5353\n",
      "Epoch 1213/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 921us/step - loss: 0.1730 - accuracy: 0.5974 - val_loss: 0.4547 - val_accuracy: 0.5324\n",
      "Epoch 1214/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1762 - accuracy: 0.5948 - val_loss: 0.4301 - val_accuracy: 0.5471\n",
      "Epoch 1215/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1711 - accuracy: 0.5988 - val_loss: 0.4514 - val_accuracy: 0.5412\n",
      "Epoch 1216/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1821 - accuracy: 0.5937 - val_loss: 0.4296 - val_accuracy: 0.5324\n",
      "Epoch 1217/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1666 - accuracy: 0.5962 - val_loss: 0.4323 - val_accuracy: 0.5412\n",
      "Epoch 1218/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1794 - accuracy: 0.5959 - val_loss: 0.4317 - val_accuracy: 0.5471\n",
      "Epoch 1219/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1761 - accuracy: 0.5977 - val_loss: 0.4679 - val_accuracy: 0.5441\n",
      "Epoch 1220/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1620 - accuracy: 0.6003 - val_loss: 0.4643 - val_accuracy: 0.5588\n",
      "Epoch 1221/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1690 - accuracy: 0.5977 - val_loss: 0.4410 - val_accuracy: 0.5382\n",
      "Epoch 1222/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1968 - accuracy: 0.5933 - val_loss: 0.4090 - val_accuracy: 0.5529\n",
      "Epoch 1223/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1702 - accuracy: 0.5962 - val_loss: 0.4206 - val_accuracy: 0.5412\n",
      "Epoch 1224/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1731 - accuracy: 0.5959 - val_loss: 0.4319 - val_accuracy: 0.5588\n",
      "Epoch 1225/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1810 - accuracy: 0.5951 - val_loss: 0.4451 - val_accuracy: 0.5471\n",
      "Epoch 1226/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1864 - accuracy: 0.5944 - val_loss: 0.4364 - val_accuracy: 0.5382\n",
      "Epoch 1227/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1823 - accuracy: 0.5926 - val_loss: 0.4330 - val_accuracy: 0.5441\n",
      "Epoch 1228/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1626 - accuracy: 0.5981 - val_loss: 0.4243 - val_accuracy: 0.5353\n",
      "Epoch 1229/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1753 - accuracy: 0.5951 - val_loss: 0.4527 - val_accuracy: 0.5441\n",
      "Epoch 1230/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1652 - accuracy: 0.5996 - val_loss: 0.4130 - val_accuracy: 0.5441\n",
      "Epoch 1231/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1893 - accuracy: 0.5915 - val_loss: 0.4306 - val_accuracy: 0.5441\n",
      "Epoch 1232/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1779 - accuracy: 0.5962 - val_loss: 0.4313 - val_accuracy: 0.5294\n",
      "Epoch 1233/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1743 - accuracy: 0.5955 - val_loss: 0.4739 - val_accuracy: 0.5324\n",
      "Epoch 1234/1600\n",
      "680/680 [==============================] - 1s 904us/step - loss: 0.1902 - accuracy: 0.5907 - val_loss: 0.4312 - val_accuracy: 0.5412\n",
      "Epoch 1235/1600\n",
      "680/680 [==============================] - 1s 930us/step - loss: 0.1730 - accuracy: 0.5977 - val_loss: 0.4736 - val_accuracy: 0.5324\n",
      "Epoch 1236/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1666 - accuracy: 0.5962 - val_loss: 0.4951 - val_accuracy: 0.5294\n",
      "Epoch 1237/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1763 - accuracy: 0.5974 - val_loss: 0.4097 - val_accuracy: 0.5412\n",
      "Epoch 1238/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1764 - accuracy: 0.5955 - val_loss: 0.4445 - val_accuracy: 0.5412\n",
      "Epoch 1239/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1783 - accuracy: 0.5951 - val_loss: 0.4575 - val_accuracy: 0.5500\n",
      "Epoch 1240/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1708 - accuracy: 0.5970 - val_loss: 0.4551 - val_accuracy: 0.5441\n",
      "Epoch 1241/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.1691 - accuracy: 0.5981 - val_loss: 0.4190 - val_accuracy: 0.5353\n",
      "Epoch 1242/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1738 - accuracy: 0.5970 - val_loss: 0.4614 - val_accuracy: 0.5324\n",
      "Epoch 1243/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1867 - accuracy: 0.5922 - val_loss: 0.4591 - val_accuracy: 0.5353\n",
      "Epoch 1244/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1672 - accuracy: 0.5999 - val_loss: 0.4399 - val_accuracy: 0.5324\n",
      "Epoch 1245/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1676 - accuracy: 0.5974 - val_loss: 0.4460 - val_accuracy: 0.5353\n",
      "Epoch 1246/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1821 - accuracy: 0.5937 - val_loss: 0.4721 - val_accuracy: 0.5412\n",
      "Epoch 1247/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1809 - accuracy: 0.5955 - val_loss: 0.4707 - val_accuracy: 0.5235\n",
      "Epoch 1248/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1762 - accuracy: 0.5940 - val_loss: 0.4423 - val_accuracy: 0.5353\n",
      "Epoch 1249/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1768 - accuracy: 0.5951 - val_loss: 0.4532 - val_accuracy: 0.5382\n",
      "Epoch 1250/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1799 - accuracy: 0.5948 - val_loss: 0.4633 - val_accuracy: 0.5441\n",
      "Epoch 1251/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1670 - accuracy: 0.5974 - val_loss: 0.4443 - val_accuracy: 0.5382\n",
      "Epoch 1252/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1737 - accuracy: 0.5966 - val_loss: 0.4209 - val_accuracy: 0.5559\n",
      "Epoch 1253/1600\n",
      "680/680 [==============================] - 1s 864us/step - loss: 0.1641 - accuracy: 0.5988 - val_loss: 0.4568 - val_accuracy: 0.5412\n",
      "Epoch 1254/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1641 - accuracy: 0.5981 - val_loss: 0.4503 - val_accuracy: 0.5382\n",
      "Epoch 1255/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1877 - accuracy: 0.5951 - val_loss: 0.4567 - val_accuracy: 0.5412\n",
      "Epoch 1256/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1762 - accuracy: 0.5955 - val_loss: 0.4184 - val_accuracy: 0.5441\n",
      "Epoch 1257/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1630 - accuracy: 0.6003 - val_loss: 0.4562 - val_accuracy: 0.5324\n",
      "Epoch 1258/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1884 - accuracy: 0.5944 - val_loss: 0.4448 - val_accuracy: 0.5500\n",
      "Epoch 1259/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1696 - accuracy: 0.5959 - val_loss: 0.4697 - val_accuracy: 0.5441\n",
      "Epoch 1260/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1769 - accuracy: 0.5974 - val_loss: 0.4315 - val_accuracy: 0.5441\n",
      "Epoch 1261/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1808 - accuracy: 0.5955 - val_loss: 0.4350 - val_accuracy: 0.5382\n",
      "Epoch 1262/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1722 - accuracy: 0.5951 - val_loss: 0.4227 - val_accuracy: 0.5559\n",
      "Epoch 1263/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1720 - accuracy: 0.5970 - val_loss: 0.4238 - val_accuracy: 0.5471\n",
      "Epoch 1264/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1718 - accuracy: 0.5966 - val_loss: 0.4432 - val_accuracy: 0.5500\n",
      "Epoch 1265/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1754 - accuracy: 0.5959 - val_loss: 0.4503 - val_accuracy: 0.5500\n",
      "Epoch 1266/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1594 - accuracy: 0.5996 - val_loss: 0.4407 - val_accuracy: 0.5471\n",
      "Epoch 1267/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1937 - accuracy: 0.5918 - val_loss: 0.4423 - val_accuracy: 0.5471\n",
      "Epoch 1268/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 932us/step - loss: 0.1878 - accuracy: 0.5966 - val_loss: 0.4282 - val_accuracy: 0.5382\n",
      "Epoch 1269/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1566 - accuracy: 0.6010 - val_loss: 0.4352 - val_accuracy: 0.5382\n",
      "Epoch 1270/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1813 - accuracy: 0.5933 - val_loss: 0.4754 - val_accuracy: 0.5294\n",
      "Epoch 1271/1600\n",
      "680/680 [==============================] - 1s 907us/step - loss: 0.1764 - accuracy: 0.5962 - val_loss: 0.4575 - val_accuracy: 0.5265\n",
      "Epoch 1272/1600\n",
      "680/680 [==============================] - 1s 973us/step - loss: 0.1797 - accuracy: 0.5929 - val_loss: 0.4489 - val_accuracy: 0.5412\n",
      "Epoch 1273/1600\n",
      "680/680 [==============================] - 1s 980us/step - loss: 0.1652 - accuracy: 0.5988 - val_loss: 0.4232 - val_accuracy: 0.5412\n",
      "Epoch 1274/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1907 - accuracy: 0.5904 - val_loss: 0.5026 - val_accuracy: 0.5176\n",
      "Epoch 1275/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1732 - accuracy: 0.5966 - val_loss: 0.4440 - val_accuracy: 0.5412\n",
      "Epoch 1276/1600\n",
      "680/680 [==============================] - 1s 930us/step - loss: 0.1692 - accuracy: 0.5951 - val_loss: 0.4413 - val_accuracy: 0.5441\n",
      "Epoch 1277/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1791 - accuracy: 0.5959 - val_loss: 0.4627 - val_accuracy: 0.5471\n",
      "Epoch 1278/1600\n",
      "680/680 [==============================] - 1s 952us/step - loss: 0.1787 - accuracy: 0.5940 - val_loss: 0.4382 - val_accuracy: 0.5500\n",
      "Epoch 1279/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1676 - accuracy: 0.5985 - val_loss: 0.4344 - val_accuracy: 0.5471\n",
      "Epoch 1280/1600\n",
      "680/680 [==============================] - 1s 935us/step - loss: 0.1662 - accuracy: 0.5966 - val_loss: 0.4466 - val_accuracy: 0.5412\n",
      "Epoch 1281/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1799 - accuracy: 0.5929 - val_loss: 0.4302 - val_accuracy: 0.5471\n",
      "Epoch 1282/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1803 - accuracy: 0.5948 - val_loss: 0.4607 - val_accuracy: 0.5412\n",
      "Epoch 1283/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1724 - accuracy: 0.5970 - val_loss: 0.4621 - val_accuracy: 0.5353\n",
      "Epoch 1284/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1830 - accuracy: 0.5926 - val_loss: 0.4604 - val_accuracy: 0.5235\n",
      "Epoch 1285/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1786 - accuracy: 0.5937 - val_loss: 0.4329 - val_accuracy: 0.5441\n",
      "Epoch 1286/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1703 - accuracy: 0.5985 - val_loss: 0.4538 - val_accuracy: 0.5471\n",
      "Epoch 1287/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1773 - accuracy: 0.5970 - val_loss: 0.4186 - val_accuracy: 0.5471\n",
      "Epoch 1288/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1662 - accuracy: 0.5988 - val_loss: 0.4499 - val_accuracy: 0.5441\n",
      "Epoch 1289/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1904 - accuracy: 0.5926 - val_loss: 0.4318 - val_accuracy: 0.5500\n",
      "Epoch 1290/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1783 - accuracy: 0.5951 - val_loss: 0.4551 - val_accuracy: 0.5441\n",
      "Epoch 1291/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1736 - accuracy: 0.5966 - val_loss: 0.4363 - val_accuracy: 0.5500\n",
      "Epoch 1292/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1831 - accuracy: 0.5937 - val_loss: 0.4435 - val_accuracy: 0.5441\n",
      "Epoch 1293/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1726 - accuracy: 0.5948 - val_loss: 0.4501 - val_accuracy: 0.5382\n",
      "Epoch 1294/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1699 - accuracy: 0.5985 - val_loss: 0.4419 - val_accuracy: 0.5441\n",
      "Epoch 1295/1600\n",
      "680/680 [==============================] - 1s 902us/step - loss: 0.1799 - accuracy: 0.5962 - val_loss: 0.4542 - val_accuracy: 0.5324\n",
      "Epoch 1296/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1713 - accuracy: 0.5962 - val_loss: 0.4560 - val_accuracy: 0.5382\n",
      "Epoch 1297/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1768 - accuracy: 0.5955 - val_loss: 0.4670 - val_accuracy: 0.5294\n",
      "Epoch 1298/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1681 - accuracy: 0.5977 - val_loss: 0.4491 - val_accuracy: 0.5500\n",
      "Epoch 1299/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1726 - accuracy: 0.5944 - val_loss: 0.4233 - val_accuracy: 0.5500\n",
      "Epoch 1300/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1707 - accuracy: 0.5977 - val_loss: 0.4818 - val_accuracy: 0.5412\n",
      "Epoch 1301/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1852 - accuracy: 0.5959 - val_loss: 0.4413 - val_accuracy: 0.5441\n",
      "Epoch 1302/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1672 - accuracy: 0.5966 - val_loss: 0.4546 - val_accuracy: 0.5471\n",
      "Epoch 1303/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.2025 - accuracy: 0.5889 - val_loss: 0.4340 - val_accuracy: 0.5529\n",
      "Epoch 1304/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1743 - accuracy: 0.5951 - val_loss: 0.4304 - val_accuracy: 0.5324\n",
      "Epoch 1305/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1653 - accuracy: 0.5977 - val_loss: 0.4635 - val_accuracy: 0.5382\n",
      "Epoch 1306/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1734 - accuracy: 0.5966 - val_loss: 0.4376 - val_accuracy: 0.5265\n",
      "Epoch 1307/1600\n",
      "680/680 [==============================] - 1s 898us/step - loss: 0.1814 - accuracy: 0.5940 - val_loss: 0.4541 - val_accuracy: 0.5441\n",
      "Epoch 1308/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1575 - accuracy: 0.5999 - val_loss: 0.4456 - val_accuracy: 0.5412\n",
      "Epoch 1309/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1839 - accuracy: 0.5937 - val_loss: 0.4427 - val_accuracy: 0.5441\n",
      "Epoch 1310/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1744 - accuracy: 0.5955 - val_loss: 0.4502 - val_accuracy: 0.5529\n",
      "Epoch 1311/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1653 - accuracy: 0.6003 - val_loss: 0.4387 - val_accuracy: 0.5353\n",
      "Epoch 1312/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1676 - accuracy: 0.5992 - val_loss: 0.4588 - val_accuracy: 0.5471\n",
      "Epoch 1313/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1747 - accuracy: 0.5985 - val_loss: 0.4551 - val_accuracy: 0.5412\n",
      "Epoch 1314/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1834 - accuracy: 0.5955 - val_loss: 0.4836 - val_accuracy: 0.5412\n",
      "Epoch 1315/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1868 - accuracy: 0.5922 - val_loss: 0.4608 - val_accuracy: 0.5382\n",
      "Epoch 1316/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1674 - accuracy: 0.5962 - val_loss: 0.4345 - val_accuracy: 0.5412\n",
      "Epoch 1317/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.1714 - accuracy: 0.5948 - val_loss: 0.3995 - val_accuracy: 0.5382\n",
      "Epoch 1318/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1713 - accuracy: 0.5962 - val_loss: 0.4562 - val_accuracy: 0.5412\n",
      "Epoch 1319/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.1700 - accuracy: 0.5959 - val_loss: 0.4480 - val_accuracy: 0.5441\n",
      "Epoch 1320/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1862 - accuracy: 0.5926 - val_loss: 0.4350 - val_accuracy: 0.5471\n",
      "Epoch 1321/1600\n",
      "680/680 [==============================] - 1s 907us/step - loss: 0.1689 - accuracy: 0.5959 - val_loss: 0.4086 - val_accuracy: 0.5353\n",
      "Epoch 1322/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1729 - accuracy: 0.5966 - val_loss: 0.4082 - val_accuracy: 0.5412\n",
      "Epoch 1323/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 899us/step - loss: 0.1897 - accuracy: 0.5940 - val_loss: 0.4457 - val_accuracy: 0.5441\n",
      "Epoch 1324/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1686 - accuracy: 0.5962 - val_loss: 0.4435 - val_accuracy: 0.5353\n",
      "Epoch 1325/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1800 - accuracy: 0.5970 - val_loss: 0.4867 - val_accuracy: 0.5235\n",
      "Epoch 1326/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1758 - accuracy: 0.5940 - val_loss: 0.4294 - val_accuracy: 0.5412\n",
      "Epoch 1327/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1702 - accuracy: 0.5985 - val_loss: 0.4677 - val_accuracy: 0.5412\n",
      "Epoch 1328/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1746 - accuracy: 0.5944 - val_loss: 0.4345 - val_accuracy: 0.5471\n",
      "Epoch 1329/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1773 - accuracy: 0.5951 - val_loss: 0.4575 - val_accuracy: 0.5441\n",
      "Epoch 1330/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1608 - accuracy: 0.5992 - val_loss: 0.4662 - val_accuracy: 0.5529\n",
      "Epoch 1331/1600\n",
      "680/680 [==============================] - 1s 901us/step - loss: 0.1882 - accuracy: 0.5915 - val_loss: 0.4356 - val_accuracy: 0.5529\n",
      "Epoch 1332/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.1768 - accuracy: 0.5948 - val_loss: 0.4469 - val_accuracy: 0.5382\n",
      "Epoch 1333/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1703 - accuracy: 0.5959 - val_loss: 0.4528 - val_accuracy: 0.5382\n",
      "Epoch 1334/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1735 - accuracy: 0.5970 - val_loss: 0.4258 - val_accuracy: 0.5353\n",
      "Epoch 1335/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1716 - accuracy: 0.5966 - val_loss: 0.4252 - val_accuracy: 0.5500\n",
      "Epoch 1336/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1695 - accuracy: 0.5955 - val_loss: 0.4347 - val_accuracy: 0.5412\n",
      "Epoch 1337/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1766 - accuracy: 0.5951 - val_loss: 0.4440 - val_accuracy: 0.5382\n",
      "Epoch 1338/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1667 - accuracy: 0.5974 - val_loss: 0.4457 - val_accuracy: 0.5382\n",
      "Epoch 1339/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1675 - accuracy: 0.5977 - val_loss: 0.4877 - val_accuracy: 0.5235\n",
      "Epoch 1340/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1800 - accuracy: 0.5929 - val_loss: 0.4351 - val_accuracy: 0.5529\n",
      "Epoch 1341/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.1735 - accuracy: 0.5959 - val_loss: 0.4570 - val_accuracy: 0.5441\n",
      "Epoch 1342/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1798 - accuracy: 0.5981 - val_loss: 0.4393 - val_accuracy: 0.5324\n",
      "Epoch 1343/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1782 - accuracy: 0.5951 - val_loss: 0.4391 - val_accuracy: 0.5471\n",
      "Epoch 1344/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1637 - accuracy: 0.5988 - val_loss: 0.4470 - val_accuracy: 0.5441\n",
      "Epoch 1345/1600\n",
      "680/680 [==============================] - 1s 903us/step - loss: 0.1752 - accuracy: 0.5933 - val_loss: 0.4587 - val_accuracy: 0.5500\n",
      "Epoch 1346/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1829 - accuracy: 0.5926 - val_loss: 0.4659 - val_accuracy: 0.5382\n",
      "Epoch 1347/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1642 - accuracy: 0.5977 - val_loss: 0.4561 - val_accuracy: 0.5412\n",
      "Epoch 1348/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1829 - accuracy: 0.5937 - val_loss: 0.4851 - val_accuracy: 0.5412\n",
      "Epoch 1349/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1756 - accuracy: 0.5951 - val_loss: 0.4353 - val_accuracy: 0.5382\n",
      "Epoch 1350/1600\n",
      "680/680 [==============================] - 1s 903us/step - loss: 0.1787 - accuracy: 0.5922 - val_loss: 0.4550 - val_accuracy: 0.5618\n",
      "Epoch 1351/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1679 - accuracy: 0.5985 - val_loss: 0.4556 - val_accuracy: 0.5412\n",
      "Epoch 1352/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1858 - accuracy: 0.5951 - val_loss: 0.4581 - val_accuracy: 0.5471\n",
      "Epoch 1353/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1837 - accuracy: 0.5922 - val_loss: 0.4545 - val_accuracy: 0.5294\n",
      "Epoch 1354/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1678 - accuracy: 0.5974 - val_loss: 0.4541 - val_accuracy: 0.5412\n",
      "Epoch 1355/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1866 - accuracy: 0.5911 - val_loss: 0.4739 - val_accuracy: 0.5412\n",
      "Epoch 1356/1600\n",
      "680/680 [==============================] - 1s 936us/step - loss: 0.1663 - accuracy: 0.5966 - val_loss: 0.4071 - val_accuracy: 0.5471\n",
      "Epoch 1357/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1657 - accuracy: 0.5966 - val_loss: 0.4781 - val_accuracy: 0.5441\n",
      "Epoch 1358/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1734 - accuracy: 0.5940 - val_loss: 0.4453 - val_accuracy: 0.5441\n",
      "Epoch 1359/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1731 - accuracy: 0.5959 - val_loss: 0.4544 - val_accuracy: 0.5441\n",
      "Epoch 1360/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1689 - accuracy: 0.5962 - val_loss: 0.4689 - val_accuracy: 0.5500\n",
      "Epoch 1361/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1550 - accuracy: 0.6003 - val_loss: 0.4490 - val_accuracy: 0.5471\n",
      "Epoch 1362/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.1670 - accuracy: 0.5962 - val_loss: 0.4483 - val_accuracy: 0.5500\n",
      "Epoch 1363/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1927 - accuracy: 0.5918 - val_loss: 0.4717 - val_accuracy: 0.5412\n",
      "Epoch 1364/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1778 - accuracy: 0.5959 - val_loss: 0.4382 - val_accuracy: 0.5471\n",
      "Epoch 1365/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1728 - accuracy: 0.5955 - val_loss: 0.4617 - val_accuracy: 0.5353\n",
      "Epoch 1366/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1703 - accuracy: 0.5970 - val_loss: 0.4744 - val_accuracy: 0.5353\n",
      "Epoch 1367/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1698 - accuracy: 0.5970 - val_loss: 0.4554 - val_accuracy: 0.5500\n",
      "Epoch 1368/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1680 - accuracy: 0.5985 - val_loss: 0.4497 - val_accuracy: 0.5441\n",
      "Epoch 1369/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1716 - accuracy: 0.5955 - val_loss: 0.4752 - val_accuracy: 0.5294\n",
      "Epoch 1370/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1964 - accuracy: 0.5926 - val_loss: 0.4444 - val_accuracy: 0.5559\n",
      "Epoch 1371/1600\n",
      "680/680 [==============================] - 1s 902us/step - loss: 0.1659 - accuracy: 0.5959 - val_loss: 0.4567 - val_accuracy: 0.5500\n",
      "Epoch 1372/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1701 - accuracy: 0.5959 - val_loss: 0.4622 - val_accuracy: 0.5412\n",
      "Epoch 1373/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1726 - accuracy: 0.5955 - val_loss: 0.4484 - val_accuracy: 0.5471\n",
      "Epoch 1374/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1766 - accuracy: 0.5962 - val_loss: 0.4752 - val_accuracy: 0.5412\n",
      "Epoch 1375/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1605 - accuracy: 0.5988 - val_loss: 0.4751 - val_accuracy: 0.5412\n",
      "Epoch 1376/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1638 - accuracy: 0.5962 - val_loss: 0.4439 - val_accuracy: 0.5500\n",
      "Epoch 1377/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1801 - accuracy: 0.5944 - val_loss: 0.4610 - val_accuracy: 0.5471\n",
      "Epoch 1378/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 927us/step - loss: 0.1713 - accuracy: 0.5962 - val_loss: 0.4684 - val_accuracy: 0.5471\n",
      "Epoch 1379/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1793 - accuracy: 0.5951 - val_loss: 0.4878 - val_accuracy: 0.5294\n",
      "Epoch 1380/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1764 - accuracy: 0.5948 - val_loss: 0.4511 - val_accuracy: 0.5529\n",
      "Epoch 1381/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1718 - accuracy: 0.5977 - val_loss: 0.4760 - val_accuracy: 0.5294\n",
      "Epoch 1382/1600\n",
      "680/680 [==============================] - 1s 973us/step - loss: 0.1669 - accuracy: 0.5959 - val_loss: 0.4865 - val_accuracy: 0.5588\n",
      "Epoch 1383/1600\n",
      "680/680 [==============================] - 1s 949us/step - loss: 0.1810 - accuracy: 0.5926 - val_loss: 0.4529 - val_accuracy: 0.5324\n",
      "Epoch 1384/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1753 - accuracy: 0.5951 - val_loss: 0.4353 - val_accuracy: 0.5500\n",
      "Epoch 1385/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1746 - accuracy: 0.5940 - val_loss: 0.4597 - val_accuracy: 0.5412\n",
      "Epoch 1386/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1810 - accuracy: 0.5951 - val_loss: 0.4569 - val_accuracy: 0.5265\n",
      "Epoch 1387/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1690 - accuracy: 0.5962 - val_loss: 0.4409 - val_accuracy: 0.5441\n",
      "Epoch 1388/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1778 - accuracy: 0.5948 - val_loss: 0.4687 - val_accuracy: 0.5176\n",
      "Epoch 1389/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1675 - accuracy: 0.5962 - val_loss: 0.4608 - val_accuracy: 0.5382\n",
      "Epoch 1390/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1638 - accuracy: 0.5996 - val_loss: 0.4611 - val_accuracy: 0.5471\n",
      "Epoch 1391/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1721 - accuracy: 0.5951 - val_loss: 0.4760 - val_accuracy: 0.5353\n",
      "Epoch 1392/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1628 - accuracy: 0.5981 - val_loss: 0.4530 - val_accuracy: 0.5500\n",
      "Epoch 1393/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1892 - accuracy: 0.5937 - val_loss: 0.4627 - val_accuracy: 0.5559\n",
      "Epoch 1394/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1678 - accuracy: 0.5951 - val_loss: 0.4624 - val_accuracy: 0.5500\n",
      "Epoch 1395/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1695 - accuracy: 0.5977 - val_loss: 0.4562 - val_accuracy: 0.5412\n",
      "Epoch 1396/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1604 - accuracy: 0.5996 - val_loss: 0.4525 - val_accuracy: 0.5382\n",
      "Epoch 1397/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1831 - accuracy: 0.5940 - val_loss: 0.4738 - val_accuracy: 0.5441\n",
      "Epoch 1398/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1768 - accuracy: 0.5922 - val_loss: 0.4604 - val_accuracy: 0.5441\n",
      "Epoch 1399/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1744 - accuracy: 0.5948 - val_loss: 0.4698 - val_accuracy: 0.5441\n",
      "Epoch 1400/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1662 - accuracy: 0.5970 - val_loss: 0.4753 - val_accuracy: 0.5294\n",
      "Epoch 1401/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.1961 - accuracy: 0.5904 - val_loss: 0.4630 - val_accuracy: 0.5441\n",
      "Epoch 1402/1600\n",
      "680/680 [==============================] - 1s 910us/step - loss: 0.1710 - accuracy: 0.5948 - val_loss: 0.4558 - val_accuracy: 0.5353\n",
      "Epoch 1403/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1669 - accuracy: 0.5962 - val_loss: 0.4387 - val_accuracy: 0.5441\n",
      "Epoch 1404/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1749 - accuracy: 0.5948 - val_loss: 0.4626 - val_accuracy: 0.5294\n",
      "Epoch 1405/1600\n",
      "680/680 [==============================] - 1s 925us/step - loss: 0.1714 - accuracy: 0.5951 - val_loss: 0.4466 - val_accuracy: 0.5500\n",
      "Epoch 1406/1600\n",
      "680/680 [==============================] - 1s 907us/step - loss: 0.1558 - accuracy: 0.6010 - val_loss: 0.4683 - val_accuracy: 0.5353\n",
      "Epoch 1407/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1899 - accuracy: 0.5889 - val_loss: 0.4434 - val_accuracy: 0.5441\n",
      "Epoch 1408/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1831 - accuracy: 0.5937 - val_loss: 0.4585 - val_accuracy: 0.5412\n",
      "Epoch 1409/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1617 - accuracy: 0.5985 - val_loss: 0.4555 - val_accuracy: 0.5500\n",
      "Epoch 1410/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1824 - accuracy: 0.5933 - val_loss: 0.4532 - val_accuracy: 0.5324\n",
      "Epoch 1411/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1744 - accuracy: 0.5962 - val_loss: 0.4747 - val_accuracy: 0.5324\n",
      "Epoch 1412/1600\n",
      "680/680 [==============================] - 1s 913us/step - loss: 0.1657 - accuracy: 0.5951 - val_loss: 0.4507 - val_accuracy: 0.5412\n",
      "Epoch 1413/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1673 - accuracy: 0.5955 - val_loss: 0.4350 - val_accuracy: 0.5500\n",
      "Epoch 1414/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1848 - accuracy: 0.5911 - val_loss: 0.4512 - val_accuracy: 0.5412\n",
      "Epoch 1415/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1631 - accuracy: 0.5970 - val_loss: 0.4540 - val_accuracy: 0.5471\n",
      "Epoch 1416/1600\n",
      "680/680 [==============================] - 1s 927us/step - loss: 0.1659 - accuracy: 0.5962 - val_loss: 0.4622 - val_accuracy: 0.5382\n",
      "Epoch 1417/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1679 - accuracy: 0.5966 - val_loss: 0.4464 - val_accuracy: 0.5382\n",
      "Epoch 1418/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.1709 - accuracy: 0.5974 - val_loss: 0.5027 - val_accuracy: 0.5118\n",
      "Epoch 1419/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1683 - accuracy: 0.5966 - val_loss: 0.4732 - val_accuracy: 0.5324\n",
      "Epoch 1420/1600\n",
      "680/680 [==============================] - 1s 932us/step - loss: 0.1583 - accuracy: 0.5999 - val_loss: 0.4923 - val_accuracy: 0.5265\n",
      "Epoch 1421/1600\n",
      "680/680 [==============================] - 1s 904us/step - loss: 0.1837 - accuracy: 0.5944 - val_loss: 0.4386 - val_accuracy: 0.5382\n",
      "Epoch 1422/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1834 - accuracy: 0.5918 - val_loss: 0.4516 - val_accuracy: 0.5471\n",
      "Epoch 1423/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1662 - accuracy: 0.5992 - val_loss: 0.4524 - val_accuracy: 0.5176\n",
      "Epoch 1424/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.2009 - accuracy: 0.5918 - val_loss: 0.4616 - val_accuracy: 0.5441\n",
      "Epoch 1425/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1821 - accuracy: 0.5933 - val_loss: 0.4482 - val_accuracy: 0.5382\n",
      "Epoch 1426/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1684 - accuracy: 0.5974 - val_loss: 0.4496 - val_accuracy: 0.5441\n",
      "Epoch 1427/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1658 - accuracy: 0.5985 - val_loss: 0.4577 - val_accuracy: 0.5294\n",
      "Epoch 1428/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1834 - accuracy: 0.5944 - val_loss: 0.4536 - val_accuracy: 0.5294\n",
      "Epoch 1429/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1787 - accuracy: 0.5926 - val_loss: 0.4535 - val_accuracy: 0.5441\n",
      "Epoch 1430/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1700 - accuracy: 0.5966 - val_loss: 0.4565 - val_accuracy: 0.5412\n",
      "Epoch 1431/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1656 - accuracy: 0.5988 - val_loss: 0.4860 - val_accuracy: 0.5353\n",
      "Epoch 1432/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1812 - accuracy: 0.5937 - val_loss: 0.4898 - val_accuracy: 0.5382\n",
      "Epoch 1433/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 887us/step - loss: 0.1725 - accuracy: 0.5962 - val_loss: 0.4909 - val_accuracy: 0.5353\n",
      "Epoch 1434/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1642 - accuracy: 0.5985 - val_loss: 0.4842 - val_accuracy: 0.5235\n",
      "Epoch 1435/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1716 - accuracy: 0.5951 - val_loss: 0.4911 - val_accuracy: 0.5206\n",
      "Epoch 1436/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1699 - accuracy: 0.5955 - val_loss: 0.4655 - val_accuracy: 0.5324\n",
      "Epoch 1437/1600\n",
      "680/680 [==============================] - 1s 929us/step - loss: 0.1766 - accuracy: 0.5959 - val_loss: 0.4634 - val_accuracy: 0.5441\n",
      "Epoch 1438/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1680 - accuracy: 0.5959 - val_loss: 0.4473 - val_accuracy: 0.5441\n",
      "Epoch 1439/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1654 - accuracy: 0.5985 - val_loss: 0.4653 - val_accuracy: 0.5441\n",
      "Epoch 1440/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1703 - accuracy: 0.5970 - val_loss: 0.4256 - val_accuracy: 0.5441\n",
      "Epoch 1441/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1790 - accuracy: 0.5937 - val_loss: 0.4505 - val_accuracy: 0.5471\n",
      "Epoch 1442/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1678 - accuracy: 0.5981 - val_loss: 0.4936 - val_accuracy: 0.5353\n",
      "Epoch 1443/1600\n",
      "680/680 [==============================] - 1s 893us/step - loss: 0.1773 - accuracy: 0.5970 - val_loss: 0.4513 - val_accuracy: 0.5353\n",
      "Epoch 1444/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1806 - accuracy: 0.5951 - val_loss: 0.4806 - val_accuracy: 0.5441\n",
      "Epoch 1445/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1902 - accuracy: 0.5933 - val_loss: 0.4672 - val_accuracy: 0.5294\n",
      "Epoch 1446/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1841 - accuracy: 0.5937 - val_loss: 0.4465 - val_accuracy: 0.5382\n",
      "Epoch 1447/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1662 - accuracy: 0.5977 - val_loss: 0.4575 - val_accuracy: 0.5324\n",
      "Epoch 1448/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.1660 - accuracy: 0.5951 - val_loss: 0.4400 - val_accuracy: 0.5441\n",
      "Epoch 1449/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1664 - accuracy: 0.5974 - val_loss: 0.4362 - val_accuracy: 0.5412\n",
      "Epoch 1450/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1673 - accuracy: 0.5966 - val_loss: 0.4347 - val_accuracy: 0.5441\n",
      "Epoch 1451/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1695 - accuracy: 0.5974 - val_loss: 0.4433 - val_accuracy: 0.5500\n",
      "Epoch 1452/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1671 - accuracy: 0.5977 - val_loss: 0.4535 - val_accuracy: 0.5441\n",
      "Epoch 1453/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1809 - accuracy: 0.5937 - val_loss: 0.4624 - val_accuracy: 0.5412\n",
      "Epoch 1454/1600\n",
      "680/680 [==============================] - 1s 931us/step - loss: 0.1787 - accuracy: 0.5948 - val_loss: 0.4870 - val_accuracy: 0.5500\n",
      "Epoch 1455/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1685 - accuracy: 0.5962 - val_loss: 0.4620 - val_accuracy: 0.5441\n",
      "Epoch 1456/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1643 - accuracy: 0.5985 - val_loss: 0.4722 - val_accuracy: 0.5353\n",
      "Epoch 1457/1600\n",
      "680/680 [==============================] - 1s 926us/step - loss: 0.1650 - accuracy: 0.5974 - val_loss: 0.4741 - val_accuracy: 0.5441\n",
      "Epoch 1458/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1916 - accuracy: 0.5922 - val_loss: 0.4786 - val_accuracy: 0.5324\n",
      "Epoch 1459/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1760 - accuracy: 0.5944 - val_loss: 0.4616 - val_accuracy: 0.5353\n",
      "Epoch 1460/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1664 - accuracy: 0.5974 - val_loss: 0.4468 - val_accuracy: 0.5382\n",
      "Epoch 1461/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1639 - accuracy: 0.5977 - val_loss: 0.4608 - val_accuracy: 0.5382\n",
      "Epoch 1462/1600\n",
      "680/680 [==============================] - 1s 860us/step - loss: 0.1634 - accuracy: 0.5996 - val_loss: 0.4720 - val_accuracy: 0.5412\n",
      "Epoch 1463/1600\n",
      "680/680 [==============================] - 1s 872us/step - loss: 0.1765 - accuracy: 0.5962 - val_loss: 0.4605 - val_accuracy: 0.5382\n",
      "Epoch 1464/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1666 - accuracy: 0.5985 - val_loss: 0.4428 - val_accuracy: 0.5441\n",
      "Epoch 1465/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1817 - accuracy: 0.5951 - val_loss: 0.4755 - val_accuracy: 0.5412\n",
      "Epoch 1466/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.1624 - accuracy: 0.5970 - val_loss: 0.4581 - val_accuracy: 0.5382\n",
      "Epoch 1467/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1691 - accuracy: 0.5977 - val_loss: 0.4517 - val_accuracy: 0.5471\n",
      "Epoch 1468/1600\n",
      "680/680 [==============================] - 1s 905us/step - loss: 0.1726 - accuracy: 0.5981 - val_loss: 0.4450 - val_accuracy: 0.5471\n",
      "Epoch 1469/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1580 - accuracy: 0.5996 - val_loss: 0.4211 - val_accuracy: 0.5559\n",
      "Epoch 1470/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1764 - accuracy: 0.5955 - val_loss: 0.4513 - val_accuracy: 0.5324\n",
      "Epoch 1471/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1774 - accuracy: 0.5962 - val_loss: 0.4925 - val_accuracy: 0.5294\n",
      "Epoch 1472/1600\n",
      "680/680 [==============================] - 1s 863us/step - loss: 0.1738 - accuracy: 0.5962 - val_loss: 0.4522 - val_accuracy: 0.5441\n",
      "Epoch 1473/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1719 - accuracy: 0.5974 - val_loss: 0.4519 - val_accuracy: 0.5353\n",
      "Epoch 1474/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.1675 - accuracy: 0.5988 - val_loss: 0.4158 - val_accuracy: 0.5500\n",
      "Epoch 1475/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1776 - accuracy: 0.5937 - val_loss: 0.4533 - val_accuracy: 0.5382\n",
      "Epoch 1476/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1872 - accuracy: 0.5933 - val_loss: 0.4445 - val_accuracy: 0.5500\n",
      "Epoch 1477/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1610 - accuracy: 0.5985 - val_loss: 0.4535 - val_accuracy: 0.5500\n",
      "Epoch 1478/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1710 - accuracy: 0.5951 - val_loss: 0.4807 - val_accuracy: 0.5353\n",
      "Epoch 1479/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1783 - accuracy: 0.5940 - val_loss: 0.4435 - val_accuracy: 0.5353\n",
      "Epoch 1480/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1540 - accuracy: 0.6003 - val_loss: 0.4706 - val_accuracy: 0.5412\n",
      "Epoch 1481/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1684 - accuracy: 0.5970 - val_loss: 0.4897 - val_accuracy: 0.5500\n",
      "Epoch 1482/1600\n",
      "680/680 [==============================] - 1s 887us/step - loss: 0.1809 - accuracy: 0.5955 - val_loss: 0.4687 - val_accuracy: 0.5412\n",
      "Epoch 1483/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1834 - accuracy: 0.5915 - val_loss: 0.4344 - val_accuracy: 0.5382\n",
      "Epoch 1484/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1631 - accuracy: 0.5970 - val_loss: 0.4809 - val_accuracy: 0.5353\n",
      "Epoch 1485/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1861 - accuracy: 0.5937 - val_loss: 0.4614 - val_accuracy: 0.5529\n",
      "Epoch 1486/1600\n",
      "680/680 [==============================] - 1s 875us/step - loss: 0.1729 - accuracy: 0.5966 - val_loss: 0.4956 - val_accuracy: 0.5324\n",
      "Epoch 1487/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1668 - accuracy: 0.5977 - val_loss: 0.4704 - val_accuracy: 0.5353\n",
      "Epoch 1488/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 921us/step - loss: 0.1609 - accuracy: 0.5974 - val_loss: 0.4727 - val_accuracy: 0.5471\n",
      "Epoch 1489/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1734 - accuracy: 0.5962 - val_loss: 0.4273 - val_accuracy: 0.5353\n",
      "Epoch 1490/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1733 - accuracy: 0.5974 - val_loss: 0.4254 - val_accuracy: 0.5382\n",
      "Epoch 1491/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.1725 - accuracy: 0.5948 - val_loss: 0.5047 - val_accuracy: 0.5441\n",
      "Epoch 1492/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.1877 - accuracy: 0.5915 - val_loss: 0.4504 - val_accuracy: 0.5412\n",
      "Epoch 1493/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1713 - accuracy: 0.5955 - val_loss: 0.4856 - val_accuracy: 0.5441\n",
      "Epoch 1494/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1814 - accuracy: 0.5940 - val_loss: 0.4499 - val_accuracy: 0.5529\n",
      "Epoch 1495/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1632 - accuracy: 0.6007 - val_loss: 0.4808 - val_accuracy: 0.5324\n",
      "Epoch 1496/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1671 - accuracy: 0.5940 - val_loss: 0.4541 - val_accuracy: 0.5471\n",
      "Epoch 1497/1600\n",
      "680/680 [==============================] - 1s 898us/step - loss: 0.1842 - accuracy: 0.5940 - val_loss: 0.4316 - val_accuracy: 0.5500\n",
      "Epoch 1498/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1783 - accuracy: 0.5933 - val_loss: 0.4496 - val_accuracy: 0.5294\n",
      "Epoch 1499/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1769 - accuracy: 0.5959 - val_loss: 0.4954 - val_accuracy: 0.5412\n",
      "Epoch 1500/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1724 - accuracy: 0.5944 - val_loss: 0.4348 - val_accuracy: 0.5412\n",
      "Epoch 1501/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1679 - accuracy: 0.5966 - val_loss: 0.4620 - val_accuracy: 0.5294\n",
      "Epoch 1502/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1787 - accuracy: 0.5944 - val_loss: 0.4436 - val_accuracy: 0.5324\n",
      "Epoch 1503/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1600 - accuracy: 0.5996 - val_loss: 0.4511 - val_accuracy: 0.5353\n",
      "Epoch 1504/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1571 - accuracy: 0.5999 - val_loss: 0.4757 - val_accuracy: 0.5441\n",
      "Epoch 1505/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1906 - accuracy: 0.5926 - val_loss: 0.4299 - val_accuracy: 0.5412\n",
      "Epoch 1506/1600\n",
      "680/680 [==============================] - 1s 916us/step - loss: 0.1643 - accuracy: 0.5970 - val_loss: 0.4484 - val_accuracy: 0.5382\n",
      "Epoch 1507/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1817 - accuracy: 0.5915 - val_loss: 0.4880 - val_accuracy: 0.5441\n",
      "Epoch 1508/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1603 - accuracy: 0.5977 - val_loss: 0.4499 - val_accuracy: 0.5353\n",
      "Epoch 1509/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1850 - accuracy: 0.5926 - val_loss: 0.4570 - val_accuracy: 0.5382\n",
      "Epoch 1510/1600\n",
      "680/680 [==============================] - 1s 924us/step - loss: 0.1800 - accuracy: 0.5929 - val_loss: 0.4591 - val_accuracy: 0.5471\n",
      "Epoch 1511/1600\n",
      "680/680 [==============================] - 1s 909us/step - loss: 0.1575 - accuracy: 0.5988 - val_loss: 0.4869 - val_accuracy: 0.5441\n",
      "Epoch 1512/1600\n",
      "680/680 [==============================] - 1s 923us/step - loss: 0.1770 - accuracy: 0.5951 - val_loss: 0.4639 - val_accuracy: 0.5382\n",
      "Epoch 1513/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1649 - accuracy: 0.5977 - val_loss: 0.4573 - val_accuracy: 0.5412\n",
      "Epoch 1514/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1651 - accuracy: 0.5955 - val_loss: 0.4492 - val_accuracy: 0.5471\n",
      "Epoch 1515/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1698 - accuracy: 0.5970 - val_loss: 0.4644 - val_accuracy: 0.5471\n",
      "Epoch 1516/1600\n",
      "680/680 [==============================] - 1s 881us/step - loss: 0.1788 - accuracy: 0.5937 - val_loss: 0.4309 - val_accuracy: 0.5441\n",
      "Epoch 1517/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1565 - accuracy: 0.5985 - val_loss: 0.4297 - val_accuracy: 0.5529\n",
      "Epoch 1518/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1702 - accuracy: 0.5962 - val_loss: 0.4580 - val_accuracy: 0.5471\n",
      "Epoch 1519/1600\n",
      "680/680 [==============================] - 1s 911us/step - loss: 0.1767 - accuracy: 0.5959 - val_loss: 0.4608 - val_accuracy: 0.5294\n",
      "Epoch 1520/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1633 - accuracy: 0.5962 - val_loss: 0.4913 - val_accuracy: 0.5441\n",
      "Epoch 1521/1600\n",
      "680/680 [==============================] - 1s 914us/step - loss: 0.1866 - accuracy: 0.5929 - val_loss: 0.4638 - val_accuracy: 0.5353\n",
      "Epoch 1522/1600\n",
      "680/680 [==============================] - 1s 907us/step - loss: 0.1579 - accuracy: 0.5992 - val_loss: 0.4874 - val_accuracy: 0.5412\n",
      "Epoch 1523/1600\n",
      "680/680 [==============================] - 1s 888us/step - loss: 0.1780 - accuracy: 0.5926 - val_loss: 0.4722 - val_accuracy: 0.5471\n",
      "Epoch 1524/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1722 - accuracy: 0.5985 - val_loss: 0.4607 - val_accuracy: 0.5353\n",
      "Epoch 1525/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1843 - accuracy: 0.5951 - val_loss: 0.4654 - val_accuracy: 0.5324\n",
      "Epoch 1526/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1723 - accuracy: 0.5977 - val_loss: 0.4458 - val_accuracy: 0.5353\n",
      "Epoch 1527/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1711 - accuracy: 0.5977 - val_loss: 0.4699 - val_accuracy: 0.5353\n",
      "Epoch 1528/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1590 - accuracy: 0.5988 - val_loss: 0.4788 - val_accuracy: 0.5176\n",
      "Epoch 1529/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1788 - accuracy: 0.5929 - val_loss: 0.4523 - val_accuracy: 0.5471\n",
      "Epoch 1530/1600\n",
      "680/680 [==============================] - 1s 894us/step - loss: 0.1788 - accuracy: 0.5937 - val_loss: 0.4639 - val_accuracy: 0.5412\n",
      "Epoch 1531/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1547 - accuracy: 0.6003 - val_loss: 0.4775 - val_accuracy: 0.5353\n",
      "Epoch 1532/1600\n",
      "680/680 [==============================] - 1s 898us/step - loss: 0.1685 - accuracy: 0.5962 - val_loss: 0.4416 - val_accuracy: 0.5441\n",
      "Epoch 1533/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1805 - accuracy: 0.5933 - val_loss: 0.4554 - val_accuracy: 0.5324\n",
      "Epoch 1534/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1672 - accuracy: 0.5955 - val_loss: 0.4459 - val_accuracy: 0.5441\n",
      "Epoch 1535/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1818 - accuracy: 0.5970 - val_loss: 0.4450 - val_accuracy: 0.5265\n",
      "Epoch 1536/1600\n",
      "680/680 [==============================] - 1s 889us/step - loss: 0.1564 - accuracy: 0.5999 - val_loss: 0.4911 - val_accuracy: 0.5441\n",
      "Epoch 1537/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1718 - accuracy: 0.5974 - val_loss: 0.4873 - val_accuracy: 0.5412\n",
      "Epoch 1538/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1615 - accuracy: 0.5974 - val_loss: 0.4892 - val_accuracy: 0.5412\n",
      "Epoch 1539/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1789 - accuracy: 0.5940 - val_loss: 0.4471 - val_accuracy: 0.5529\n",
      "Epoch 1540/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1752 - accuracy: 0.5966 - val_loss: 0.4460 - val_accuracy: 0.5382\n",
      "Epoch 1541/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1659 - accuracy: 0.5962 - val_loss: 0.4665 - val_accuracy: 0.5559\n",
      "Epoch 1542/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1624 - accuracy: 0.6003 - val_loss: 0.4724 - val_accuracy: 0.5324\n",
      "Epoch 1543/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 883us/step - loss: 0.1702 - accuracy: 0.5974 - val_loss: 0.4817 - val_accuracy: 0.5324\n",
      "Epoch 1544/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1787 - accuracy: 0.5948 - val_loss: 0.5056 - val_accuracy: 0.5206\n",
      "Epoch 1545/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1720 - accuracy: 0.5974 - val_loss: 0.4501 - val_accuracy: 0.5294\n",
      "Epoch 1546/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1775 - accuracy: 0.5948 - val_loss: 0.4601 - val_accuracy: 0.5500\n",
      "Epoch 1547/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1661 - accuracy: 0.5970 - val_loss: 0.4732 - val_accuracy: 0.5441\n",
      "Epoch 1548/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1720 - accuracy: 0.5944 - val_loss: 0.4667 - val_accuracy: 0.5382\n",
      "Epoch 1549/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1755 - accuracy: 0.5948 - val_loss: 0.4523 - val_accuracy: 0.5382\n",
      "Epoch 1550/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1703 - accuracy: 0.5974 - val_loss: 0.4698 - val_accuracy: 0.5471\n",
      "Epoch 1551/1600\n",
      "680/680 [==============================] - 1s 890us/step - loss: 0.1684 - accuracy: 0.5974 - val_loss: 0.4726 - val_accuracy: 0.5471\n",
      "Epoch 1552/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1645 - accuracy: 0.6018 - val_loss: 0.4733 - val_accuracy: 0.5382\n",
      "Epoch 1553/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.1872 - accuracy: 0.5933 - val_loss: 0.4555 - val_accuracy: 0.5500\n",
      "Epoch 1554/1600\n",
      "680/680 [==============================] - 1s 974us/step - loss: 0.1839 - accuracy: 0.5926 - val_loss: 0.4798 - val_accuracy: 0.5265\n",
      "Epoch 1555/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1678 - accuracy: 0.5977 - val_loss: 0.4580 - val_accuracy: 0.5441\n",
      "Epoch 1556/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1607 - accuracy: 0.5981 - val_loss: 0.4769 - val_accuracy: 0.5294\n",
      "Epoch 1557/1600\n",
      "680/680 [==============================] - 1s 866us/step - loss: 0.1621 - accuracy: 0.5981 - val_loss: 0.4687 - val_accuracy: 0.5412\n",
      "Epoch 1558/1600\n",
      "680/680 [==============================] - 1s 879us/step - loss: 0.1684 - accuracy: 0.5966 - val_loss: 0.4688 - val_accuracy: 0.5294\n",
      "Epoch 1559/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1640 - accuracy: 0.5988 - val_loss: 0.4954 - val_accuracy: 0.5382\n",
      "Epoch 1560/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1919 - accuracy: 0.5940 - val_loss: 0.4540 - val_accuracy: 0.5265\n",
      "Epoch 1561/1600\n",
      "680/680 [==============================] - 1s 949us/step - loss: 0.1752 - accuracy: 0.5940 - val_loss: 0.4594 - val_accuracy: 0.5265\n",
      "Epoch 1562/1600\n",
      "680/680 [==============================] - 1s 920us/step - loss: 0.1571 - accuracy: 0.5988 - val_loss: 0.4671 - val_accuracy: 0.5353\n",
      "Epoch 1563/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1771 - accuracy: 0.5944 - val_loss: 0.4667 - val_accuracy: 0.5500\n",
      "Epoch 1564/1600\n",
      "680/680 [==============================] - 1s 874us/step - loss: 0.1649 - accuracy: 0.5970 - val_loss: 0.4672 - val_accuracy: 0.5500\n",
      "Epoch 1565/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.1698 - accuracy: 0.5981 - val_loss: 0.4667 - val_accuracy: 0.5324\n",
      "Epoch 1566/1600\n",
      "680/680 [==============================] - 1s 884us/step - loss: 0.1671 - accuracy: 0.5970 - val_loss: 0.4481 - val_accuracy: 0.5471\n",
      "Epoch 1567/1600\n",
      "680/680 [==============================] - 1s 917us/step - loss: 0.1612 - accuracy: 0.5977 - val_loss: 0.4888 - val_accuracy: 0.5382\n",
      "Epoch 1568/1600\n",
      "680/680 [==============================] - 1s 859us/step - loss: 0.1724 - accuracy: 0.5977 - val_loss: 0.4618 - val_accuracy: 0.5353\n",
      "Epoch 1569/1600\n",
      "680/680 [==============================] - 1s 912us/step - loss: 0.1652 - accuracy: 0.5981 - val_loss: 0.4512 - val_accuracy: 0.5382\n",
      "Epoch 1570/1600\n",
      "680/680 [==============================] - 1s 856us/step - loss: 0.1713 - accuracy: 0.5951 - val_loss: 0.4457 - val_accuracy: 0.5441\n",
      "Epoch 1571/1600\n",
      "680/680 [==============================] - 1s 895us/step - loss: 0.1715 - accuracy: 0.5996 - val_loss: 0.4510 - val_accuracy: 0.5265\n",
      "Epoch 1572/1600\n",
      "680/680 [==============================] - 1s 840us/step - loss: 0.1736 - accuracy: 0.5948 - val_loss: 0.4848 - val_accuracy: 0.5353\n",
      "Epoch 1573/1600\n",
      "680/680 [==============================] - 1s 945us/step - loss: 0.1769 - accuracy: 0.5944 - val_loss: 0.4657 - val_accuracy: 0.5412\n",
      "Epoch 1574/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1739 - accuracy: 0.5959 - val_loss: 0.4411 - val_accuracy: 0.5441\n",
      "Epoch 1575/1600\n",
      "680/680 [==============================] - 1s 864us/step - loss: 0.1671 - accuracy: 0.5974 - val_loss: 0.4650 - val_accuracy: 0.5500\n",
      "Epoch 1576/1600\n",
      "680/680 [==============================] - 1s 896us/step - loss: 0.1592 - accuracy: 0.6010 - val_loss: 0.4829 - val_accuracy: 0.5441\n",
      "Epoch 1577/1600\n",
      "680/680 [==============================] - 1s 852us/step - loss: 0.1595 - accuracy: 0.5977 - val_loss: 0.5068 - val_accuracy: 0.5029\n",
      "Epoch 1578/1600\n",
      "680/680 [==============================] - 1s 900us/step - loss: 0.1817 - accuracy: 0.5955 - val_loss: 0.4562 - val_accuracy: 0.5353\n",
      "Epoch 1579/1600\n",
      "680/680 [==============================] - 1s 882us/step - loss: 0.1681 - accuracy: 0.5977 - val_loss: 0.4644 - val_accuracy: 0.5353\n",
      "Epoch 1580/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1565 - accuracy: 0.5981 - val_loss: 0.4374 - val_accuracy: 0.5471\n",
      "Epoch 1581/1600\n",
      "680/680 [==============================] - 1s 859us/step - loss: 0.1924 - accuracy: 0.5933 - val_loss: 0.4646 - val_accuracy: 0.5265\n",
      "Epoch 1582/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1674 - accuracy: 0.5966 - val_loss: 0.4580 - val_accuracy: 0.5441\n",
      "Epoch 1583/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.1753 - accuracy: 0.5977 - val_loss: 0.4345 - val_accuracy: 0.5471\n",
      "Epoch 1584/1600\n",
      "680/680 [==============================] - 1s 862us/step - loss: 0.1757 - accuracy: 0.5955 - val_loss: 0.4677 - val_accuracy: 0.5412\n",
      "Epoch 1585/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1769 - accuracy: 0.5970 - val_loss: 0.5058 - val_accuracy: 0.5412\n",
      "Epoch 1586/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1638 - accuracy: 0.5977 - val_loss: 0.4889 - val_accuracy: 0.5441\n",
      "Epoch 1587/1600\n",
      "680/680 [==============================] - 1s 851us/step - loss: 0.1661 - accuracy: 0.5966 - val_loss: 0.4575 - val_accuracy: 0.5324\n",
      "Epoch 1588/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.1788 - accuracy: 0.5951 - val_loss: 0.4509 - val_accuracy: 0.5471\n",
      "Epoch 1589/1600\n",
      "680/680 [==============================] - 1s 937us/step - loss: 0.1744 - accuracy: 0.5988 - val_loss: 0.4687 - val_accuracy: 0.5382\n",
      "Epoch 1590/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.1644 - accuracy: 0.5970 - val_loss: 0.5131 - val_accuracy: 0.5353\n",
      "Epoch 1591/1600\n",
      "680/680 [==============================] - 1s 886us/step - loss: 0.1853 - accuracy: 0.5955 - val_loss: 0.4241 - val_accuracy: 0.5559\n",
      "Epoch 1592/1600\n",
      "680/680 [==============================] - 1s 842us/step - loss: 0.1776 - accuracy: 0.5951 - val_loss: 0.4494 - val_accuracy: 0.5471\n",
      "Epoch 1593/1600\n",
      "680/680 [==============================] - 1s 885us/step - loss: 0.1619 - accuracy: 0.5981 - val_loss: 0.5020 - val_accuracy: 0.5353\n",
      "Epoch 1594/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.1700 - accuracy: 0.5981 - val_loss: 0.4455 - val_accuracy: 0.5471\n",
      "Epoch 1595/1600\n",
      "680/680 [==============================] - 1s 865us/step - loss: 0.1782 - accuracy: 0.5959 - val_loss: 0.4811 - val_accuracy: 0.5441\n",
      "Epoch 1596/1600\n",
      "680/680 [==============================] - 1s 975us/step - loss: 0.1767 - accuracy: 0.5951 - val_loss: 0.4972 - val_accuracy: 0.5441\n",
      "Epoch 1597/1600\n",
      "680/680 [==============================] - 1s 844us/step - loss: 0.1828 - accuracy: 0.5933 - val_loss: 0.4886 - val_accuracy: 0.5353\n",
      "Epoch 1598/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 935us/step - loss: 0.1736 - accuracy: 0.5966 - val_loss: 0.4545 - val_accuracy: 0.5441\n",
      "Epoch 1599/1600\n",
      "680/680 [==============================] - 1s 840us/step - loss: 0.1876 - accuracy: 0.5937 - val_loss: 0.4849 - val_accuracy: 0.5324\n",
      "Epoch 1600/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1627 - accuracy: 0.5988 - val_loss: 0.4455 - val_accuracy: 0.5471\n"
     ]
    }
   ],
   "source": [
    "classTrain = Transformer.toClassification(activitiesTrain)\n",
    "classVal = Transformer.toClassification(activitiesValidate)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.01,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False\n",
    ")\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "l1Reg = 0.001\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=np.shape(trainData)[1], activation='softmax', kernel_regularizer = keras.regularizers.L2(l1Reg)))\n",
    "model.add(Dense(150, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model.add(Dense(75, activation='relu', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model.add(Dense(100, activation='relu', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model.add(Dense(100, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model.add(Dense(1, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "\n",
    "model.compile(loss='MeanSquaredError', optimizer=\"adam\", metrics=['accuracy'])\n",
    "history = model.fit(trainData, Transformer.toClassification(activitiesTrain), \n",
    "                    validation_data = (valData, classVal), epochs=1600, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3caf2ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x1fa2e247910>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "classify = Transformer.toClassification(activitiesTrain)\n",
    "constantGuess = (len(classify[classify == 1]))/len(classify)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,8))\n",
    "\n",
    "ax.plot(history.history[\"accuracy\"], color=\"green\")\n",
    "ax.plot(history.history[\"val_accuracy\"], color=\"purple\")\n",
    "ax.axhline(constantGuess, color=\"blue\", linestyle=\"dashed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b656c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 1.0216 - accuracy: 0.2805 - val_loss: 0.5836 - val_accuracy: 0.4206\n",
      "Epoch 2/1600\n",
      "680/680 [==============================] - 1s 860us/step - loss: 0.5192 - accuracy: 0.4682 - val_loss: 0.4951 - val_accuracy: 0.5118\n",
      "Epoch 3/1600\n",
      "680/680 [==============================] - 1s 824us/step - loss: 0.4457 - accuracy: 0.5013 - val_loss: 0.4601 - val_accuracy: 0.5000\n",
      "Epoch 4/1600\n",
      "680/680 [==============================] - 1s 825us/step - loss: 0.4104 - accuracy: 0.5105 - val_loss: 0.4457 - val_accuracy: 0.4971\n",
      "Epoch 5/1600\n",
      "680/680 [==============================] - 1s 845us/step - loss: 0.3924 - accuracy: 0.5208 - val_loss: 0.4380 - val_accuracy: 0.5059\n",
      "Epoch 6/1600\n",
      "680/680 [==============================] - 1s 818us/step - loss: 0.3745 - accuracy: 0.5212 - val_loss: 0.4542 - val_accuracy: 0.4853\n",
      "Epoch 7/1600\n",
      "680/680 [==============================] - 1s 868us/step - loss: 0.3626 - accuracy: 0.5252 - val_loss: 0.4380 - val_accuracy: 0.4941\n",
      "Epoch 8/1600\n",
      "680/680 [==============================] - 1s 908us/step - loss: 0.3506 - accuracy: 0.5318 - val_loss: 0.4383 - val_accuracy: 0.5294\n",
      "Epoch 9/1600\n",
      "680/680 [==============================] - 1s 867us/step - loss: 0.3394 - accuracy: 0.5333 - val_loss: 0.4410 - val_accuracy: 0.5294\n",
      "Epoch 10/1600\n",
      "680/680 [==============================] - 1s 801us/step - loss: 0.3312 - accuracy: 0.5333 - val_loss: 0.4429 - val_accuracy: 0.5294\n",
      "Epoch 11/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.3174 - accuracy: 0.5396 - val_loss: 0.4327 - val_accuracy: 0.5500\n",
      "Epoch 12/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.3117 - accuracy: 0.5444 - val_loss: 0.4374 - val_accuracy: 0.5471\n",
      "Epoch 13/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.3079 - accuracy: 0.5421 - val_loss: 0.4301 - val_accuracy: 0.5382\n",
      "Epoch 14/1600\n",
      "680/680 [==============================] - 1s 918us/step - loss: 0.3006 - accuracy: 0.5436 - val_loss: 0.4176 - val_accuracy: 0.5382\n",
      "Epoch 15/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.2938 - accuracy: 0.5477 - val_loss: 0.4523 - val_accuracy: 0.5441\n",
      "Epoch 16/1600\n",
      "680/680 [==============================] - 1s 780us/step - loss: 0.2852 - accuracy: 0.5513 - val_loss: 0.4348 - val_accuracy: 0.5353\n",
      "Epoch 17/1600\n",
      "680/680 [==============================] - 1s 878us/step - loss: 0.2834 - accuracy: 0.5550 - val_loss: 0.4263 - val_accuracy: 0.5324\n",
      "Epoch 18/1600\n",
      "680/680 [==============================] - 1s 880us/step - loss: 0.2800 - accuracy: 0.5569 - val_loss: 0.4081 - val_accuracy: 0.5353\n",
      "Epoch 19/1600\n",
      "680/680 [==============================] - 1s 824us/step - loss: 0.2747 - accuracy: 0.5547 - val_loss: 0.4130 - val_accuracy: 0.5353\n",
      "Epoch 20/1600\n",
      "680/680 [==============================] - 1s 802us/step - loss: 0.2733 - accuracy: 0.5572 - val_loss: 0.4367 - val_accuracy: 0.5471\n",
      "Epoch 21/1600\n",
      "680/680 [==============================] - 1s 816us/step - loss: 0.2733 - accuracy: 0.5576 - val_loss: 0.4098 - val_accuracy: 0.5412\n",
      "Epoch 22/1600\n",
      "680/680 [==============================] - 1s 798us/step - loss: 0.2695 - accuracy: 0.5583 - val_loss: 0.4210 - val_accuracy: 0.5382\n",
      "Epoch 23/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2659 - accuracy: 0.5605 - val_loss: 0.4167 - val_accuracy: 0.5500\n",
      "Epoch 24/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2544 - accuracy: 0.5631 - val_loss: 0.4263 - val_accuracy: 0.5412\n",
      "Epoch 25/1600\n",
      "680/680 [==============================] - 1s 937us/step - loss: 0.2483 - accuracy: 0.5675 - val_loss: 0.4029 - val_accuracy: 0.5529\n",
      "Epoch 26/1600\n",
      "680/680 [==============================] - 1s 969us/step - loss: 0.2552 - accuracy: 0.5635 - val_loss: 0.4356 - val_accuracy: 0.5588\n",
      "Epoch 27/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.2527 - accuracy: 0.5668 - val_loss: 0.4260 - val_accuracy: 0.5412\n",
      "Epoch 28/1600\n",
      "680/680 [==============================] - 1s 819us/step - loss: 0.2484 - accuracy: 0.5661 - val_loss: 0.4128 - val_accuracy: 0.5529\n",
      "Epoch 29/1600\n",
      "680/680 [==============================] - 1s 803us/step - loss: 0.2472 - accuracy: 0.5683 - val_loss: 0.4102 - val_accuracy: 0.5412\n",
      "Epoch 30/1600\n",
      "680/680 [==============================] - 1s 804us/step - loss: 0.2512 - accuracy: 0.5679 - val_loss: 0.4277 - val_accuracy: 0.5471\n",
      "Epoch 31/1600\n",
      "680/680 [==============================] - 1s 812us/step - loss: 0.2362 - accuracy: 0.5697 - val_loss: 0.4165 - val_accuracy: 0.5647\n",
      "Epoch 32/1600\n",
      "680/680 [==============================] - 1s 809us/step - loss: 0.2388 - accuracy: 0.5753 - val_loss: 0.4054 - val_accuracy: 0.5441\n",
      "Epoch 33/1600\n",
      "680/680 [==============================] - 1s 811us/step - loss: 0.2334 - accuracy: 0.5723 - val_loss: 0.4278 - val_accuracy: 0.5559\n",
      "Epoch 34/1600\n",
      "680/680 [==============================] - 1s 815us/step - loss: 0.2268 - accuracy: 0.5745 - val_loss: 0.4451 - val_accuracy: 0.5265\n",
      "Epoch 35/1600\n",
      "680/680 [==============================] - 1s 819us/step - loss: 0.2295 - accuracy: 0.5756 - val_loss: 0.4522 - val_accuracy: 0.5471\n",
      "Epoch 36/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2308 - accuracy: 0.5742 - val_loss: 0.4232 - val_accuracy: 0.5441\n",
      "Epoch 37/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2301 - accuracy: 0.5760 - val_loss: 0.4384 - val_accuracy: 0.5500\n",
      "Epoch 38/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2258 - accuracy: 0.5712 - val_loss: 0.4324 - val_accuracy: 0.5441\n",
      "Epoch 39/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2231 - accuracy: 0.5771 - val_loss: 0.4252 - val_accuracy: 0.5529\n",
      "Epoch 40/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2235 - accuracy: 0.5793 - val_loss: 0.4443 - val_accuracy: 0.5441\n",
      "Epoch 41/1600\n",
      "680/680 [==============================] - 1s 883us/step - loss: 0.2199 - accuracy: 0.5775 - val_loss: 0.4208 - val_accuracy: 0.5529\n",
      "Epoch 42/1600\n",
      "680/680 [==============================] - 1s 826us/step - loss: 0.2181 - accuracy: 0.5778 - val_loss: 0.4415 - val_accuracy: 0.5529\n",
      "Epoch 43/1600\n",
      "680/680 [==============================] - 1s 899us/step - loss: 0.2141 - accuracy: 0.5804 - val_loss: 0.4400 - val_accuracy: 0.5588\n",
      "Epoch 44/1600\n",
      "680/680 [==============================] - 1s 957us/step - loss: 0.2117 - accuracy: 0.5801 - val_loss: 0.4313 - val_accuracy: 0.5588\n",
      "Epoch 45/1600\n",
      "680/680 [==============================] - 1s 853us/step - loss: 0.2080 - accuracy: 0.5841 - val_loss: 0.4411 - val_accuracy: 0.5412\n",
      "Epoch 46/1600\n",
      "680/680 [==============================] - 1s 820us/step - loss: 0.2087 - accuracy: 0.5841 - val_loss: 0.4545 - val_accuracy: 0.5382\n",
      "Epoch 47/1600\n",
      "680/680 [==============================] - 1s 847us/step - loss: 0.2117 - accuracy: 0.5808 - val_loss: 0.4453 - val_accuracy: 0.5588\n",
      "Epoch 48/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.2055 - accuracy: 0.5841 - val_loss: 0.4463 - val_accuracy: 0.5412\n",
      "Epoch 49/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.2087 - accuracy: 0.5841 - val_loss: 0.4543 - val_accuracy: 0.5441\n",
      "Epoch 50/1600\n",
      "680/680 [==============================] - 1s 781us/step - loss: 0.1991 - accuracy: 0.5856 - val_loss: 0.4543 - val_accuracy: 0.5529\n",
      "Epoch 51/1600\n",
      "680/680 [==============================] - 1s 797us/step - loss: 0.1956 - accuracy: 0.5870 - val_loss: 0.4290 - val_accuracy: 0.5382\n",
      "Epoch 52/1600\n",
      "680/680 [==============================] - 1s 807us/step - loss: 0.2016 - accuracy: 0.5841 - val_loss: 0.4626 - val_accuracy: 0.5647\n",
      "Epoch 53/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.2015 - accuracy: 0.5856 - val_loss: 0.4673 - val_accuracy: 0.5735\n",
      "Epoch 54/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.2001 - accuracy: 0.5878 - val_loss: 0.4300 - val_accuracy: 0.5412\n",
      "Epoch 55/1600\n",
      "680/680 [==============================] - 1s 991us/step - loss: 0.1912 - accuracy: 0.5856 - val_loss: 0.4490 - val_accuracy: 0.5441\n",
      "Epoch 56/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1933 - accuracy: 0.5889 - val_loss: 0.4595 - val_accuracy: 0.5412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1963 - accuracy: 0.5859 - val_loss: 0.4472 - val_accuracy: 0.5588\n",
      "Epoch 58/1600\n",
      "680/680 [==============================] - 1s 954us/step - loss: 0.1968 - accuracy: 0.5852 - val_loss: 0.4266 - val_accuracy: 0.5618\n",
      "Epoch 59/1600\n",
      "680/680 [==============================] - 1s 956us/step - loss: 0.1830 - accuracy: 0.5900 - val_loss: 0.4394 - val_accuracy: 0.5618\n",
      "Epoch 60/1600\n",
      "680/680 [==============================] - 1s 853us/step - loss: 0.1939 - accuracy: 0.5878 - val_loss: 0.4747 - val_accuracy: 0.5588\n",
      "Epoch 61/1600\n",
      "680/680 [==============================] - 1s 834us/step - loss: 0.1887 - accuracy: 0.5878 - val_loss: 0.4374 - val_accuracy: 0.5500\n",
      "Epoch 62/1600\n",
      "680/680 [==============================] - 1s 827us/step - loss: 0.1870 - accuracy: 0.5904 - val_loss: 0.4497 - val_accuracy: 0.5588\n",
      "Epoch 63/1600\n",
      "680/680 [==============================] - 1s 824us/step - loss: 0.1934 - accuracy: 0.5915 - val_loss: 0.4467 - val_accuracy: 0.5412\n",
      "Epoch 64/1600\n",
      "680/680 [==============================] - 1s 778us/step - loss: 0.1820 - accuracy: 0.5926 - val_loss: 0.4337 - val_accuracy: 0.5412\n",
      "Epoch 65/1600\n",
      "680/680 [==============================] - 1s 891us/step - loss: 0.1891 - accuracy: 0.5889 - val_loss: 0.4058 - val_accuracy: 0.5559\n",
      "Epoch 66/1600\n",
      "680/680 [==============================] - 1s 801us/step - loss: 0.1846 - accuracy: 0.5881 - val_loss: 0.4430 - val_accuracy: 0.5588\n",
      "Epoch 67/1600\n",
      "680/680 [==============================] - 1s 921us/step - loss: 0.1886 - accuracy: 0.5900 - val_loss: 0.4212 - val_accuracy: 0.5500\n",
      "Epoch 68/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1852 - accuracy: 0.5896 - val_loss: 0.4424 - val_accuracy: 0.5588\n",
      "Epoch 69/1600\n",
      "680/680 [==============================] - 1s 857us/step - loss: 0.1795 - accuracy: 0.5918 - val_loss: 0.4486 - val_accuracy: 0.5559\n",
      "Epoch 70/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1740 - accuracy: 0.5929 - val_loss: 0.4572 - val_accuracy: 0.5529\n",
      "Epoch 71/1600\n",
      "680/680 [==============================] - 1s 928us/step - loss: 0.1758 - accuracy: 0.5933 - val_loss: 0.4401 - val_accuracy: 0.5706\n",
      "Epoch 72/1600\n",
      "680/680 [==============================] - 1s 801us/step - loss: 0.1877 - accuracy: 0.5885 - val_loss: 0.4726 - val_accuracy: 0.5559\n",
      "Epoch 73/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1795 - accuracy: 0.5915 - val_loss: 0.4224 - val_accuracy: 0.5500\n",
      "Epoch 74/1600\n",
      "680/680 [==============================] - 1s 915us/step - loss: 0.1766 - accuracy: 0.5922 - val_loss: 0.4782 - val_accuracy: 0.5588\n",
      "Epoch 75/1600\n",
      "680/680 [==============================] - 1s 919us/step - loss: 0.1720 - accuracy: 0.5944 - val_loss: 0.4508 - val_accuracy: 0.5618\n",
      "Epoch 76/1600\n",
      "680/680 [==============================] - 1s 922us/step - loss: 0.1811 - accuracy: 0.5944 - val_loss: 0.4270 - val_accuracy: 0.5647\n",
      "Epoch 77/1600\n",
      "680/680 [==============================] - 1s 877us/step - loss: 0.1799 - accuracy: 0.5922 - val_loss: 0.4826 - val_accuracy: 0.5647\n",
      "Epoch 78/1600\n",
      "680/680 [==============================] - 1s 955us/step - loss: 0.1754 - accuracy: 0.5915 - val_loss: 0.4612 - val_accuracy: 0.5559\n",
      "Epoch 79/1600\n",
      "680/680 [==============================] - 1s 853us/step - loss: 0.1658 - accuracy: 0.5962 - val_loss: 0.4397 - val_accuracy: 0.5559\n",
      "Epoch 80/1600\n",
      "680/680 [==============================] - 1s 847us/step - loss: 0.1693 - accuracy: 0.5933 - val_loss: 0.4528 - val_accuracy: 0.5353\n",
      "Epoch 81/1600\n",
      "680/680 [==============================] - 1s 892us/step - loss: 0.1745 - accuracy: 0.5933 - val_loss: 0.4511 - val_accuracy: 0.5618\n",
      "Epoch 82/1600\n",
      "680/680 [==============================] - 1s 819us/step - loss: 0.1744 - accuracy: 0.5929 - val_loss: 0.4526 - val_accuracy: 0.5529\n",
      "Epoch 83/1600\n",
      "680/680 [==============================] - 1s 807us/step - loss: 0.1717 - accuracy: 0.5959 - val_loss: 0.4174 - val_accuracy: 0.5529\n",
      "Epoch 84/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1671 - accuracy: 0.5940 - val_loss: 0.4502 - val_accuracy: 0.5529\n",
      "Epoch 85/1600\n",
      "680/680 [==============================] - 1s 802us/step - loss: 0.1671 - accuracy: 0.5948 - val_loss: 0.4330 - val_accuracy: 0.5618\n",
      "Epoch 86/1600\n",
      "680/680 [==============================] - 1s 817us/step - loss: 0.1686 - accuracy: 0.5944 - val_loss: 0.4377 - val_accuracy: 0.5412\n",
      "Epoch 87/1600\n",
      "680/680 [==============================] - 1s 897us/step - loss: 0.1807 - accuracy: 0.5926 - val_loss: 0.4490 - val_accuracy: 0.5529\n",
      "Epoch 88/1600\n",
      "680/680 [==============================] - 1s 763us/step - loss: 0.1669 - accuracy: 0.5970 - val_loss: 0.4346 - val_accuracy: 0.5618\n",
      "Epoch 89/1600\n",
      "680/680 [==============================] - 1s 756us/step - loss: 0.1696 - accuracy: 0.5974 - val_loss: 0.4515 - val_accuracy: 0.5559\n",
      "Epoch 90/1600\n",
      "680/680 [==============================] - 1s 756us/step - loss: 0.1828 - accuracy: 0.5929 - val_loss: 0.4837 - val_accuracy: 0.5618\n",
      "Epoch 91/1600\n",
      "680/680 [==============================] - 1s 756us/step - loss: 0.1663 - accuracy: 0.5955 - val_loss: 0.4694 - val_accuracy: 0.5647\n",
      "Epoch 92/1600\n",
      "680/680 [==============================] - 1s 764us/step - loss: 0.1745 - accuracy: 0.5915 - val_loss: 0.4427 - val_accuracy: 0.5559\n",
      "Epoch 93/1600\n",
      "680/680 [==============================] - 1s 757us/step - loss: 0.1590 - accuracy: 0.5977 - val_loss: 0.4578 - val_accuracy: 0.5559\n",
      "Epoch 94/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1680 - accuracy: 0.5966 - val_loss: 0.4168 - val_accuracy: 0.5441\n",
      "Epoch 95/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1648 - accuracy: 0.5955 - val_loss: 0.4454 - val_accuracy: 0.5588\n",
      "Epoch 96/1600\n",
      "680/680 [==============================] - 1s 767us/step - loss: 0.1565 - accuracy: 0.5996 - val_loss: 0.4468 - val_accuracy: 0.5353\n",
      "Epoch 97/1600\n",
      "680/680 [==============================] - 1s 827us/step - loss: 0.1631 - accuracy: 0.5977 - val_loss: 0.4446 - val_accuracy: 0.5529\n",
      "Epoch 98/1600\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.1628 - accuracy: 0.5974 - val_loss: 0.4165 - val_accuracy: 0.5500\n",
      "Epoch 99/1600\n",
      "680/680 [==============================] - 0s 696us/step - loss: 0.1637 - accuracy: 0.5959 - val_loss: 0.4441 - val_accuracy: 0.5618\n",
      "Epoch 100/1600\n",
      "680/680 [==============================] - 0s 687us/step - loss: 0.1651 - accuracy: 0.5974 - val_loss: 0.4352 - val_accuracy: 0.5647\n",
      "Epoch 101/1600\n",
      "680/680 [==============================] - 1s 812us/step - loss: 0.1534 - accuracy: 0.5999 - val_loss: 0.4416 - val_accuracy: 0.5559\n",
      "Epoch 102/1600\n",
      "680/680 [==============================] - 0s 688us/step - loss: 0.1733 - accuracy: 0.5926 - val_loss: 0.4619 - val_accuracy: 0.5618\n",
      "Epoch 103/1600\n",
      "680/680 [==============================] - 0s 719us/step - loss: 0.1636 - accuracy: 0.5966 - val_loss: 0.4548 - val_accuracy: 0.5412\n",
      "Epoch 104/1600\n",
      "680/680 [==============================] - 0s 709us/step - loss: 0.1587 - accuracy: 0.5970 - val_loss: 0.4353 - val_accuracy: 0.5588\n",
      "Epoch 105/1600\n",
      "680/680 [==============================] - 0s 707us/step - loss: 0.1684 - accuracy: 0.5962 - val_loss: 0.4468 - val_accuracy: 0.5559\n",
      "Epoch 106/1600\n",
      "680/680 [==============================] - 0s 706us/step - loss: 0.1643 - accuracy: 0.5951 - val_loss: 0.4106 - val_accuracy: 0.5559\n",
      "Epoch 107/1600\n",
      "680/680 [==============================] - 0s 719us/step - loss: 0.1545 - accuracy: 0.5999 - val_loss: 0.4659 - val_accuracy: 0.5588\n",
      "Epoch 108/1600\n",
      "680/680 [==============================] - 1s 755us/step - loss: 0.1627 - accuracy: 0.5962 - val_loss: 0.4304 - val_accuracy: 0.5588\n",
      "Epoch 109/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1659 - accuracy: 0.5962 - val_loss: 0.4477 - val_accuracy: 0.5676\n",
      "Epoch 110/1600\n",
      "680/680 [==============================] - 1s 760us/step - loss: 0.1642 - accuracy: 0.5988 - val_loss: 0.4663 - val_accuracy: 0.5471\n",
      "Epoch 111/1600\n",
      "680/680 [==============================] - 1s 751us/step - loss: 0.1653 - accuracy: 0.5974 - val_loss: 0.4572 - val_accuracy: 0.5559\n",
      "Epoch 112/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 759us/step - loss: 0.1551 - accuracy: 0.5988 - val_loss: 0.4319 - val_accuracy: 0.5618\n",
      "Epoch 113/1600\n",
      "680/680 [==============================] - 1s 758us/step - loss: 0.1620 - accuracy: 0.5981 - val_loss: 0.4454 - val_accuracy: 0.5588\n",
      "Epoch 114/1600\n",
      "680/680 [==============================] - 1s 764us/step - loss: 0.1766 - accuracy: 0.5922 - val_loss: 0.4142 - val_accuracy: 0.5559\n",
      "Epoch 115/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1699 - accuracy: 0.5937 - val_loss: 0.4236 - val_accuracy: 0.5559\n",
      "Epoch 116/1600\n",
      "680/680 [==============================] - 1s 757us/step - loss: 0.1603 - accuracy: 0.5988 - val_loss: 0.4354 - val_accuracy: 0.5588\n",
      "Epoch 117/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1556 - accuracy: 0.6003 - val_loss: 0.4146 - val_accuracy: 0.5559\n",
      "Epoch 118/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1686 - accuracy: 0.5940 - val_loss: 0.4312 - val_accuracy: 0.5618\n",
      "Epoch 119/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1742 - accuracy: 0.5970 - val_loss: 0.4719 - val_accuracy: 0.5559\n",
      "Epoch 120/1600\n",
      "680/680 [==============================] - 1s 834us/step - loss: 0.1709 - accuracy: 0.5955 - val_loss: 0.4425 - val_accuracy: 0.5529\n",
      "Epoch 121/1600\n",
      "680/680 [==============================] - 1s 764us/step - loss: 0.1686 - accuracy: 0.5944 - val_loss: 0.4304 - val_accuracy: 0.5588\n",
      "Epoch 122/1600\n",
      "680/680 [==============================] - 1s 774us/step - loss: 0.1613 - accuracy: 0.5966 - val_loss: 0.4315 - val_accuracy: 0.5588\n",
      "Epoch 123/1600\n",
      "680/680 [==============================] - 1s 758us/step - loss: 0.1518 - accuracy: 0.6014 - val_loss: 0.4699 - val_accuracy: 0.5618\n",
      "Epoch 124/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1629 - accuracy: 0.5981 - val_loss: 0.4838 - val_accuracy: 0.5588\n",
      "Epoch 125/1600\n",
      "680/680 [==============================] - 1s 797us/step - loss: 0.1661 - accuracy: 0.5959 - val_loss: 0.4770 - val_accuracy: 0.5588\n",
      "Epoch 126/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1741 - accuracy: 0.5937 - val_loss: 0.4426 - val_accuracy: 0.5559\n",
      "Epoch 127/1600\n",
      "680/680 [==============================] - 1s 797us/step - loss: 0.1671 - accuracy: 0.5981 - val_loss: 0.4463 - val_accuracy: 0.5559\n",
      "Epoch 128/1600\n",
      "680/680 [==============================] - 1s 751us/step - loss: 0.1579 - accuracy: 0.6007 - val_loss: 0.4506 - val_accuracy: 0.5588\n",
      "Epoch 129/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1560 - accuracy: 0.6007 - val_loss: 0.4710 - val_accuracy: 0.5559\n",
      "Epoch 130/1600\n",
      "680/680 [==============================] - 1s 761us/step - loss: 0.1714 - accuracy: 0.5955 - val_loss: 0.4956 - val_accuracy: 0.5559\n",
      "Epoch 131/1600\n",
      "680/680 [==============================] - 1s 757us/step - loss: 0.1789 - accuracy: 0.5955 - val_loss: 0.4419 - val_accuracy: 0.5618\n",
      "Epoch 132/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1634 - accuracy: 0.5981 - val_loss: 0.4945 - val_accuracy: 0.5676\n",
      "Epoch 133/1600\n",
      "680/680 [==============================] - 1s 799us/step - loss: 0.1701 - accuracy: 0.5977 - val_loss: 0.4527 - val_accuracy: 0.5529\n",
      "Epoch 134/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1658 - accuracy: 0.5974 - val_loss: 0.4421 - val_accuracy: 0.5471\n",
      "Epoch 135/1600\n",
      "680/680 [==============================] - 1s 738us/step - loss: 0.1804 - accuracy: 0.5922 - val_loss: 0.4382 - val_accuracy: 0.5588\n",
      "Epoch 136/1600\n",
      "680/680 [==============================] - 0s 717us/step - loss: 0.1652 - accuracy: 0.5970 - val_loss: 0.4134 - val_accuracy: 0.5529\n",
      "Epoch 137/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1574 - accuracy: 0.5996 - val_loss: 0.4137 - val_accuracy: 0.5588\n",
      "Epoch 138/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1643 - accuracy: 0.5974 - val_loss: 0.4306 - val_accuracy: 0.5559\n",
      "Epoch 139/1600\n",
      "680/680 [==============================] - 1s 782us/step - loss: 0.1501 - accuracy: 0.6014 - val_loss: 0.4237 - val_accuracy: 0.5647\n",
      "Epoch 140/1600\n",
      "680/680 [==============================] - 1s 756us/step - loss: 0.1705 - accuracy: 0.5977 - val_loss: 0.4437 - val_accuracy: 0.5500\n",
      "Epoch 141/1600\n",
      "680/680 [==============================] - 1s 772us/step - loss: 0.1760 - accuracy: 0.5944 - val_loss: 0.4237 - val_accuracy: 0.5588\n",
      "Epoch 142/1600\n",
      "680/680 [==============================] - 1s 761us/step - loss: 0.1653 - accuracy: 0.5948 - val_loss: 0.4473 - val_accuracy: 0.5500\n",
      "Epoch 143/1600\n",
      "680/680 [==============================] - 1s 755us/step - loss: 0.1554 - accuracy: 0.5985 - val_loss: 0.4022 - val_accuracy: 0.5588\n",
      "Epoch 144/1600\n",
      "680/680 [==============================] - 0s 712us/step - loss: 0.1663 - accuracy: 0.5977 - val_loss: 0.4354 - val_accuracy: 0.5618\n",
      "Epoch 145/1600\n",
      "680/680 [==============================] - 0s 694us/step - loss: 0.1793 - accuracy: 0.5940 - val_loss: 0.4303 - val_accuracy: 0.5500\n",
      "Epoch 146/1600\n",
      "680/680 [==============================] - 0s 713us/step - loss: 0.1596 - accuracy: 0.5977 - val_loss: 0.4402 - val_accuracy: 0.5500\n",
      "Epoch 147/1600\n",
      "680/680 [==============================] - 1s 757us/step - loss: 0.1583 - accuracy: 0.5981 - val_loss: 0.4366 - val_accuracy: 0.5529\n",
      "Epoch 148/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1577 - accuracy: 0.5999 - val_loss: 0.4151 - val_accuracy: 0.5529\n",
      "Epoch 149/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1736 - accuracy: 0.5962 - val_loss: 0.4360 - val_accuracy: 0.5618\n",
      "Epoch 150/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1562 - accuracy: 0.5988 - val_loss: 0.4250 - val_accuracy: 0.5588\n",
      "Epoch 151/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1554 - accuracy: 0.5985 - val_loss: 0.4236 - val_accuracy: 0.5618\n",
      "Epoch 152/1600\n",
      "680/680 [==============================] - 0s 717us/step - loss: 0.1568 - accuracy: 0.5999 - val_loss: 0.4236 - val_accuracy: 0.5500\n",
      "Epoch 153/1600\n",
      "680/680 [==============================] - 0s 710us/step - loss: 0.1595 - accuracy: 0.5955 - val_loss: 0.4544 - val_accuracy: 0.5559\n",
      "Epoch 154/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1616 - accuracy: 0.5966 - val_loss: 0.4403 - val_accuracy: 0.5618\n",
      "Epoch 155/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1590 - accuracy: 0.6007 - val_loss: 0.4552 - val_accuracy: 0.5618\n",
      "Epoch 156/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1710 - accuracy: 0.5955 - val_loss: 0.4444 - val_accuracy: 0.5559\n",
      "Epoch 157/1600\n",
      "680/680 [==============================] - 0s 732us/step - loss: 0.1491 - accuracy: 0.5996 - val_loss: 0.4504 - val_accuracy: 0.5559\n",
      "Epoch 158/1600\n",
      "680/680 [==============================] - 0s 699us/step - loss: 0.1527 - accuracy: 0.5992 - val_loss: 0.4443 - val_accuracy: 0.5588\n",
      "Epoch 159/1600\n",
      "680/680 [==============================] - 0s 709us/step - loss: 0.1561 - accuracy: 0.5996 - val_loss: 0.4492 - val_accuracy: 0.5559\n",
      "Epoch 160/1600\n",
      "680/680 [==============================] - 1s 751us/step - loss: 0.1628 - accuracy: 0.5962 - val_loss: 0.4428 - val_accuracy: 0.5676\n",
      "Epoch 161/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1677 - accuracy: 0.5966 - val_loss: 0.4334 - val_accuracy: 0.5559\n",
      "Epoch 162/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1564 - accuracy: 0.5977 - val_loss: 0.4484 - val_accuracy: 0.5441\n",
      "Epoch 163/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1834 - accuracy: 0.5937 - val_loss: 0.4638 - val_accuracy: 0.5559\n",
      "Epoch 164/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1687 - accuracy: 0.5970 - val_loss: 0.4591 - val_accuracy: 0.5588\n",
      "Epoch 165/1600\n",
      "680/680 [==============================] - 1s 780us/step - loss: 0.1578 - accuracy: 0.6003 - val_loss: 0.4451 - val_accuracy: 0.5529\n",
      "Epoch 166/1600\n",
      "680/680 [==============================] - 1s 751us/step - loss: 0.1590 - accuracy: 0.5937 - val_loss: 0.4512 - val_accuracy: 0.5618\n",
      "Epoch 167/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 749us/step - loss: 0.1677 - accuracy: 0.5962 - val_loss: 0.4339 - val_accuracy: 0.5441\n",
      "Epoch 168/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1590 - accuracy: 0.5977 - val_loss: 0.4204 - val_accuracy: 0.5588\n",
      "Epoch 169/1600\n",
      "680/680 [==============================] - 1s 832us/step - loss: 0.1579 - accuracy: 0.5970 - val_loss: 0.4658 - val_accuracy: 0.5559\n",
      "Epoch 170/1600\n",
      "680/680 [==============================] - 1s 826us/step - loss: 0.1580 - accuracy: 0.5992 - val_loss: 0.4415 - val_accuracy: 0.5676\n",
      "Epoch 171/1600\n",
      "680/680 [==============================] - 1s 738us/step - loss: 0.1594 - accuracy: 0.5977 - val_loss: 0.4379 - val_accuracy: 0.5529\n",
      "Epoch 172/1600\n",
      "680/680 [==============================] - 0s 669us/step - loss: 0.1743 - accuracy: 0.5926 - val_loss: 0.4349 - val_accuracy: 0.5588\n",
      "Epoch 173/1600\n",
      "680/680 [==============================] - 0s 722us/step - loss: 0.1588 - accuracy: 0.5981 - val_loss: 0.4297 - val_accuracy: 0.5618\n",
      "Epoch 174/1600\n",
      "680/680 [==============================] - 1s 737us/step - loss: 0.1479 - accuracy: 0.6003 - val_loss: 0.4918 - val_accuracy: 0.5647\n",
      "Epoch 175/1600\n",
      "680/680 [==============================] - 0s 707us/step - loss: 0.1484 - accuracy: 0.6007 - val_loss: 0.4692 - val_accuracy: 0.5500\n",
      "Epoch 176/1600\n",
      "680/680 [==============================] - 0s 718us/step - loss: 0.1577 - accuracy: 0.5992 - val_loss: 0.4349 - val_accuracy: 0.5500\n",
      "Epoch 177/1600\n",
      "680/680 [==============================] - 0s 716us/step - loss: 0.1594 - accuracy: 0.5988 - val_loss: 0.4494 - val_accuracy: 0.5588\n",
      "Epoch 178/1600\n",
      "680/680 [==============================] - 0s 711us/step - loss: 0.1659 - accuracy: 0.6014 - val_loss: 0.4199 - val_accuracy: 0.5588\n",
      "Epoch 179/1600\n",
      "680/680 [==============================] - 0s 708us/step - loss: 0.1597 - accuracy: 0.5988 - val_loss: 0.4484 - val_accuracy: 0.5676\n",
      "Epoch 180/1600\n",
      "680/680 [==============================] - 0s 713us/step - loss: 0.1618 - accuracy: 0.5966 - val_loss: 0.4162 - val_accuracy: 0.5647\n",
      "Epoch 181/1600\n",
      "680/680 [==============================] - 0s 711us/step - loss: 0.1519 - accuracy: 0.6007 - val_loss: 0.4211 - val_accuracy: 0.5529\n",
      "Epoch 182/1600\n",
      "680/680 [==============================] - 0s 715us/step - loss: 0.1601 - accuracy: 0.5985 - val_loss: 0.4344 - val_accuracy: 0.5559\n",
      "Epoch 183/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1588 - accuracy: 0.5962 - val_loss: 0.4680 - val_accuracy: 0.5471\n",
      "Epoch 184/1600\n",
      "680/680 [==============================] - 1s 751us/step - loss: 0.1577 - accuracy: 0.5962 - val_loss: 0.4413 - val_accuracy: 0.5559\n",
      "Epoch 185/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1658 - accuracy: 0.5951 - val_loss: 0.4243 - val_accuracy: 0.5618\n",
      "Epoch 186/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1592 - accuracy: 0.5974 - val_loss: 0.4323 - val_accuracy: 0.5588\n",
      "Epoch 187/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1460 - accuracy: 0.6021 - val_loss: 0.3986 - val_accuracy: 0.5500\n",
      "Epoch 188/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1567 - accuracy: 0.5985 - val_loss: 0.4953 - val_accuracy: 0.5647\n",
      "Epoch 189/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1650 - accuracy: 0.5974 - val_loss: 0.4384 - val_accuracy: 0.5529\n",
      "Epoch 190/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1692 - accuracy: 0.5966 - val_loss: 0.4131 - val_accuracy: 0.5588\n",
      "Epoch 191/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1659 - accuracy: 0.5962 - val_loss: 0.4479 - val_accuracy: 0.5588\n",
      "Epoch 192/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1608 - accuracy: 0.5981 - val_loss: 0.4420 - val_accuracy: 0.5412\n",
      "Epoch 193/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1640 - accuracy: 0.5929 - val_loss: 0.4852 - val_accuracy: 0.5618\n",
      "Epoch 194/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1604 - accuracy: 0.5999 - val_loss: 0.4361 - val_accuracy: 0.5647\n",
      "Epoch 195/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1452 - accuracy: 0.6014 - val_loss: 0.4889 - val_accuracy: 0.5588\n",
      "Epoch 196/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1507 - accuracy: 0.6010 - val_loss: 0.4437 - val_accuracy: 0.5559\n",
      "Epoch 197/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1607 - accuracy: 0.5985 - val_loss: 0.4463 - val_accuracy: 0.5559\n",
      "Epoch 198/1600\n",
      "680/680 [==============================] - 1s 798us/step - loss: 0.1564 - accuracy: 0.5988 - val_loss: 0.4215 - val_accuracy: 0.5559\n",
      "Epoch 199/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1557 - accuracy: 0.6007 - val_loss: 0.4276 - val_accuracy: 0.5676\n",
      "Epoch 200/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1614 - accuracy: 0.5996 - val_loss: 0.4511 - val_accuracy: 0.5559\n",
      "Epoch 201/1600\n",
      "680/680 [==============================] - 1s 757us/step - loss: 0.1662 - accuracy: 0.5974 - val_loss: 0.4344 - val_accuracy: 0.5588\n",
      "Epoch 202/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1560 - accuracy: 0.5992 - val_loss: 0.4285 - val_accuracy: 0.5618\n",
      "Epoch 203/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1595 - accuracy: 0.5988 - val_loss: 0.4360 - val_accuracy: 0.5529\n",
      "Epoch 204/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1594 - accuracy: 0.5988 - val_loss: 0.4631 - val_accuracy: 0.5618\n",
      "Epoch 205/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1695 - accuracy: 0.5996 - val_loss: 0.4068 - val_accuracy: 0.5588\n",
      "Epoch 206/1600\n",
      "680/680 [==============================] - 1s 797us/step - loss: 0.1568 - accuracy: 0.5992 - val_loss: 0.4153 - val_accuracy: 0.5529\n",
      "Epoch 207/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1498 - accuracy: 0.6010 - val_loss: 0.4089 - val_accuracy: 0.5529\n",
      "Epoch 208/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1551 - accuracy: 0.5985 - val_loss: 0.4438 - val_accuracy: 0.5529\n",
      "Epoch 209/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1578 - accuracy: 0.5981 - val_loss: 0.4366 - val_accuracy: 0.5618\n",
      "Epoch 210/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1695 - accuracy: 0.5977 - val_loss: 0.4529 - val_accuracy: 0.5588\n",
      "Epoch 211/1600\n",
      "680/680 [==============================] - 1s 782us/step - loss: 0.1455 - accuracy: 0.6025 - val_loss: 0.4452 - val_accuracy: 0.5618\n",
      "Epoch 212/1600\n",
      "680/680 [==============================] - 1s 823us/step - loss: 0.1557 - accuracy: 0.5981 - val_loss: 0.4529 - val_accuracy: 0.5441\n",
      "Epoch 213/1600\n",
      "680/680 [==============================] - 1s 800us/step - loss: 0.1524 - accuracy: 0.6014 - val_loss: 0.4214 - val_accuracy: 0.5647\n",
      "Epoch 214/1600\n",
      "680/680 [==============================] - 1s 823us/step - loss: 0.1754 - accuracy: 0.5974 - val_loss: 0.4298 - val_accuracy: 0.5559\n",
      "Epoch 215/1600\n",
      "680/680 [==============================] - 0s 727us/step - loss: 0.1693 - accuracy: 0.5977 - val_loss: 0.4310 - val_accuracy: 0.5559\n",
      "Epoch 216/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1672 - accuracy: 0.5981 - val_loss: 0.4004 - val_accuracy: 0.5500\n",
      "Epoch 217/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1665 - accuracy: 0.5959 - val_loss: 0.4272 - val_accuracy: 0.5588\n",
      "Epoch 218/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1497 - accuracy: 0.5999 - val_loss: 0.4368 - val_accuracy: 0.5588\n",
      "Epoch 219/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1516 - accuracy: 0.5999 - val_loss: 0.4217 - val_accuracy: 0.5647\n",
      "Epoch 220/1600\n",
      "680/680 [==============================] - 1s 797us/step - loss: 0.1573 - accuracy: 0.6007 - val_loss: 0.4091 - val_accuracy: 0.5559\n",
      "Epoch 221/1600\n",
      "680/680 [==============================] - 1s 756us/step - loss: 0.1572 - accuracy: 0.5992 - val_loss: 0.4423 - val_accuracy: 0.5500\n",
      "Epoch 222/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 754us/step - loss: 0.1604 - accuracy: 0.5974 - val_loss: 0.4215 - val_accuracy: 0.5588\n",
      "Epoch 223/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1610 - accuracy: 0.5981 - val_loss: 0.4605 - val_accuracy: 0.5559\n",
      "Epoch 224/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1495 - accuracy: 0.5985 - val_loss: 0.5001 - val_accuracy: 0.5618\n",
      "Epoch 225/1600\n",
      "680/680 [==============================] - 1s 797us/step - loss: 0.1667 - accuracy: 0.5951 - val_loss: 0.4341 - val_accuracy: 0.5588\n",
      "Epoch 226/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1503 - accuracy: 0.6021 - val_loss: 0.4409 - val_accuracy: 0.5559\n",
      "Epoch 227/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1619 - accuracy: 0.6003 - val_loss: 0.4255 - val_accuracy: 0.5559\n",
      "Epoch 228/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1564 - accuracy: 0.6007 - val_loss: 0.4466 - val_accuracy: 0.5441\n",
      "Epoch 229/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1547 - accuracy: 0.6010 - val_loss: 0.4223 - val_accuracy: 0.5676\n",
      "Epoch 230/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1521 - accuracy: 0.6010 - val_loss: 0.4080 - val_accuracy: 0.5500\n",
      "Epoch 231/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1620 - accuracy: 0.6010 - val_loss: 0.4227 - val_accuracy: 0.5500\n",
      "Epoch 232/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1518 - accuracy: 0.6021 - val_loss: 0.4268 - val_accuracy: 0.5618\n",
      "Epoch 233/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1671 - accuracy: 0.5948 - val_loss: 0.3871 - val_accuracy: 0.5559\n",
      "Epoch 234/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1536 - accuracy: 0.5999 - val_loss: 0.4285 - val_accuracy: 0.5559\n",
      "Epoch 235/1600\n",
      "680/680 [==============================] - 1s 781us/step - loss: 0.1642 - accuracy: 0.5974 - val_loss: 0.4244 - val_accuracy: 0.5676\n",
      "Epoch 236/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1505 - accuracy: 0.6018 - val_loss: 0.4183 - val_accuracy: 0.5618\n",
      "Epoch 237/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1700 - accuracy: 0.5970 - val_loss: 0.3864 - val_accuracy: 0.5618\n",
      "Epoch 238/1600\n",
      "680/680 [==============================] - 1s 781us/step - loss: 0.1488 - accuracy: 0.6018 - val_loss: 0.4580 - val_accuracy: 0.5588\n",
      "Epoch 239/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1559 - accuracy: 0.5999 - val_loss: 0.4522 - val_accuracy: 0.5559\n",
      "Epoch 240/1600\n",
      "680/680 [==============================] - 1s 751us/step - loss: 0.1474 - accuracy: 0.6021 - val_loss: 0.4536 - val_accuracy: 0.5559\n",
      "Epoch 241/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1586 - accuracy: 0.5992 - val_loss: 0.4333 - val_accuracy: 0.5647\n",
      "Epoch 242/1600\n",
      "680/680 [==============================] - 1s 756us/step - loss: 0.1704 - accuracy: 0.5940 - val_loss: 0.4201 - val_accuracy: 0.5529\n",
      "Epoch 243/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1552 - accuracy: 0.6007 - val_loss: 0.4096 - val_accuracy: 0.5618\n",
      "Epoch 244/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1624 - accuracy: 0.5977 - val_loss: 0.4513 - val_accuracy: 0.5529\n",
      "Epoch 245/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1597 - accuracy: 0.5955 - val_loss: 0.4528 - val_accuracy: 0.5559\n",
      "Epoch 246/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1596 - accuracy: 0.5988 - val_loss: 0.4178 - val_accuracy: 0.5588\n",
      "Epoch 247/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1570 - accuracy: 0.5966 - val_loss: 0.4507 - val_accuracy: 0.5647\n",
      "Epoch 248/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1580 - accuracy: 0.5999 - val_loss: 0.4273 - val_accuracy: 0.5618\n",
      "Epoch 249/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1448 - accuracy: 0.6007 - val_loss: 0.4496 - val_accuracy: 0.5529\n",
      "Epoch 250/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1702 - accuracy: 0.5926 - val_loss: 0.4451 - val_accuracy: 0.5647\n",
      "Epoch 251/1600\n",
      "680/680 [==============================] - 1s 801us/step - loss: 0.1512 - accuracy: 0.6029 - val_loss: 0.4509 - val_accuracy: 0.5529\n",
      "Epoch 252/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1459 - accuracy: 0.6010 - val_loss: 0.4263 - val_accuracy: 0.5529\n",
      "Epoch 253/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1463 - accuracy: 0.6018 - val_loss: 0.4453 - val_accuracy: 0.5559\n",
      "Epoch 254/1600\n",
      "680/680 [==============================] - 1s 802us/step - loss: 0.1651 - accuracy: 0.5981 - val_loss: 0.4609 - val_accuracy: 0.5618\n",
      "Epoch 255/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1567 - accuracy: 0.5996 - val_loss: 0.4647 - val_accuracy: 0.5588\n",
      "Epoch 256/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1673 - accuracy: 0.5992 - val_loss: 0.4580 - val_accuracy: 0.5529\n",
      "Epoch 257/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1487 - accuracy: 0.5999 - val_loss: 0.4232 - val_accuracy: 0.5500\n",
      "Epoch 258/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1619 - accuracy: 0.5955 - val_loss: 0.4568 - val_accuracy: 0.5618\n",
      "Epoch 259/1600\n",
      "680/680 [==============================] - 1s 782us/step - loss: 0.1522 - accuracy: 0.6007 - val_loss: 0.4287 - val_accuracy: 0.5559\n",
      "Epoch 260/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1595 - accuracy: 0.5974 - val_loss: 0.4610 - val_accuracy: 0.5559\n",
      "Epoch 261/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1481 - accuracy: 0.6014 - val_loss: 0.4453 - val_accuracy: 0.5618\n",
      "Epoch 262/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1490 - accuracy: 0.6003 - val_loss: 0.4384 - val_accuracy: 0.5588\n",
      "Epoch 263/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1568 - accuracy: 0.5985 - val_loss: 0.4839 - val_accuracy: 0.5529\n",
      "Epoch 264/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1786 - accuracy: 0.5937 - val_loss: 0.4740 - val_accuracy: 0.5618\n",
      "Epoch 265/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1729 - accuracy: 0.5977 - val_loss: 0.4593 - val_accuracy: 0.5382\n",
      "Epoch 266/1600\n",
      "680/680 [==============================] - 1s 752us/step - loss: 0.1611 - accuracy: 0.5948 - val_loss: 0.4388 - val_accuracy: 0.5471\n",
      "Epoch 267/1600\n",
      "680/680 [==============================] - 1s 761us/step - loss: 0.1630 - accuracy: 0.5981 - val_loss: 0.4419 - val_accuracy: 0.5559\n",
      "Epoch 268/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1373 - accuracy: 0.6032 - val_loss: 0.4677 - val_accuracy: 0.5618\n",
      "Epoch 269/1600\n",
      "680/680 [==============================] - 1s 757us/step - loss: 0.1518 - accuracy: 0.5996 - val_loss: 0.4654 - val_accuracy: 0.5559\n",
      "Epoch 270/1600\n",
      "680/680 [==============================] - 1s 740us/step - loss: 0.1586 - accuracy: 0.5988 - val_loss: 0.4343 - val_accuracy: 0.5529\n",
      "Epoch 271/1600\n",
      "680/680 [==============================] - 1s 752us/step - loss: 0.1506 - accuracy: 0.5996 - val_loss: 0.4474 - val_accuracy: 0.5647\n",
      "Epoch 272/1600\n",
      "680/680 [==============================] - 0s 704us/step - loss: 0.1490 - accuracy: 0.5988 - val_loss: 0.4621 - val_accuracy: 0.5618\n",
      "Epoch 273/1600\n",
      "680/680 [==============================] - 0s 706us/step - loss: 0.1563 - accuracy: 0.5985 - val_loss: 0.4284 - val_accuracy: 0.5500\n",
      "Epoch 274/1600\n",
      "680/680 [==============================] - 0s 713us/step - loss: 0.1606 - accuracy: 0.5981 - val_loss: 0.4337 - val_accuracy: 0.5588\n",
      "Epoch 275/1600\n",
      "680/680 [==============================] - 0s 714us/step - loss: 0.1561 - accuracy: 0.5981 - val_loss: 0.4367 - val_accuracy: 0.5618\n",
      "Epoch 276/1600\n",
      "680/680 [==============================] - 0s 713us/step - loss: 0.1509 - accuracy: 0.5985 - val_loss: 0.4541 - val_accuracy: 0.5559\n",
      "Epoch 277/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 0s 714us/step - loss: 0.1526 - accuracy: 0.6018 - val_loss: 0.4660 - val_accuracy: 0.5588\n",
      "Epoch 278/1600\n",
      "680/680 [==============================] - 0s 716us/step - loss: 0.1585 - accuracy: 0.5988 - val_loss: 0.4777 - val_accuracy: 0.5500\n",
      "Epoch 279/1600\n",
      "680/680 [==============================] - 0s 715us/step - loss: 0.1698 - accuracy: 0.5981 - val_loss: 0.4201 - val_accuracy: 0.5588\n",
      "Epoch 280/1600\n",
      "680/680 [==============================] - 0s 710us/step - loss: 0.1489 - accuracy: 0.6003 - val_loss: 0.4413 - val_accuracy: 0.5618\n",
      "Epoch 281/1600\n",
      "680/680 [==============================] - 0s 714us/step - loss: 0.1598 - accuracy: 0.5999 - val_loss: 0.4934 - val_accuracy: 0.5559\n",
      "Epoch 282/1600\n",
      "680/680 [==============================] - 1s 755us/step - loss: 0.1612 - accuracy: 0.5981 - val_loss: 0.4824 - val_accuracy: 0.5588\n",
      "Epoch 283/1600\n",
      "680/680 [==============================] - 1s 758us/step - loss: 0.1669 - accuracy: 0.5962 - val_loss: 0.4566 - val_accuracy: 0.5588\n",
      "Epoch 284/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1546 - accuracy: 0.6003 - val_loss: 0.4475 - val_accuracy: 0.5529\n",
      "Epoch 285/1600\n",
      "680/680 [==============================] - 1s 755us/step - loss: 0.1511 - accuracy: 0.5996 - val_loss: 0.4687 - val_accuracy: 0.5588\n",
      "Epoch 286/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1549 - accuracy: 0.6014 - val_loss: 0.4065 - val_accuracy: 0.5618\n",
      "Epoch 287/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1550 - accuracy: 0.5974 - val_loss: 0.4107 - val_accuracy: 0.5559\n",
      "Epoch 288/1600\n",
      "680/680 [==============================] - 1s 740us/step - loss: 0.1600 - accuracy: 0.5959 - val_loss: 0.4529 - val_accuracy: 0.5647\n",
      "Epoch 289/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1422 - accuracy: 0.6021 - val_loss: 0.4387 - val_accuracy: 0.5676\n",
      "Epoch 290/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1583 - accuracy: 0.5985 - val_loss: 0.4602 - val_accuracy: 0.5559\n",
      "Epoch 291/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1633 - accuracy: 0.5981 - val_loss: 0.4410 - val_accuracy: 0.5529\n",
      "Epoch 292/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1609 - accuracy: 0.5944 - val_loss: 0.4569 - val_accuracy: 0.5559\n",
      "Epoch 293/1600\n",
      "680/680 [==============================] - 1s 752us/step - loss: 0.1428 - accuracy: 0.6029 - val_loss: 0.4660 - val_accuracy: 0.5559\n",
      "Epoch 294/1600\n",
      "680/680 [==============================] - 1s 798us/step - loss: 0.1464 - accuracy: 0.6010 - val_loss: 0.4477 - val_accuracy: 0.5618\n",
      "Epoch 295/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1536 - accuracy: 0.5996 - val_loss: 0.4508 - val_accuracy: 0.5588\n",
      "Epoch 296/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1583 - accuracy: 0.5996 - val_loss: 0.4387 - val_accuracy: 0.5559\n",
      "Epoch 297/1600\n",
      "680/680 [==============================] - 1s 826us/step - loss: 0.1598 - accuracy: 0.5981 - val_loss: 0.4818 - val_accuracy: 0.5618\n",
      "Epoch 298/1600\n",
      "680/680 [==============================] - 1s 837us/step - loss: 0.1550 - accuracy: 0.5988 - val_loss: 0.4343 - val_accuracy: 0.5588\n",
      "Epoch 299/1600\n",
      "680/680 [==============================] - 1s 835us/step - loss: 0.1601 - accuracy: 0.5988 - val_loss: 0.4852 - val_accuracy: 0.5588\n",
      "Epoch 300/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1682 - accuracy: 0.5962 - val_loss: 0.4415 - val_accuracy: 0.5559\n",
      "Epoch 301/1600\n",
      "680/680 [==============================] - 1s 800us/step - loss: 0.1476 - accuracy: 0.6007 - val_loss: 0.4078 - val_accuracy: 0.5676\n",
      "Epoch 302/1600\n",
      "680/680 [==============================] - 1s 836us/step - loss: 0.1547 - accuracy: 0.5992 - val_loss: 0.5137 - val_accuracy: 0.5529\n",
      "Epoch 303/1600\n",
      "680/680 [==============================] - 1s 843us/step - loss: 0.1508 - accuracy: 0.5999 - val_loss: 0.4788 - val_accuracy: 0.5618\n",
      "Epoch 304/1600\n",
      "680/680 [==============================] - 1s 835us/step - loss: 0.1606 - accuracy: 0.5981 - val_loss: 0.4575 - val_accuracy: 0.5559\n",
      "Epoch 305/1600\n",
      "680/680 [==============================] - 1s 803us/step - loss: 0.1572 - accuracy: 0.5992 - val_loss: 0.4294 - val_accuracy: 0.5647\n",
      "Epoch 306/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1736 - accuracy: 0.5944 - val_loss: 0.4736 - val_accuracy: 0.5412\n",
      "Epoch 307/1600\n",
      "680/680 [==============================] - 1s 753us/step - loss: 0.1671 - accuracy: 0.5985 - val_loss: 0.4340 - val_accuracy: 0.5529\n",
      "Epoch 308/1600\n",
      "680/680 [==============================] - 1s 763us/step - loss: 0.1442 - accuracy: 0.6007 - val_loss: 0.4310 - val_accuracy: 0.5588\n",
      "Epoch 309/1600\n",
      "680/680 [==============================] - 1s 762us/step - loss: 0.1526 - accuracy: 0.6010 - val_loss: 0.4393 - val_accuracy: 0.5559\n",
      "Epoch 310/1600\n",
      "680/680 [==============================] - 1s 753us/step - loss: 0.1662 - accuracy: 0.5962 - val_loss: 0.4181 - val_accuracy: 0.5588\n",
      "Epoch 311/1600\n",
      "680/680 [==============================] - 1s 772us/step - loss: 0.1535 - accuracy: 0.5985 - val_loss: 0.4647 - val_accuracy: 0.5471\n",
      "Epoch 312/1600\n",
      "680/680 [==============================] - 1s 753us/step - loss: 0.1594 - accuracy: 0.5974 - val_loss: 0.4248 - val_accuracy: 0.5529\n",
      "Epoch 313/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1637 - accuracy: 0.5977 - val_loss: 0.4436 - val_accuracy: 0.5529\n",
      "Epoch 314/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1474 - accuracy: 0.5999 - val_loss: 0.4699 - val_accuracy: 0.5559\n",
      "Epoch 315/1600\n",
      "680/680 [==============================] - 1s 756us/step - loss: 0.1520 - accuracy: 0.6010 - val_loss: 0.4271 - val_accuracy: 0.5588\n",
      "Epoch 316/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1485 - accuracy: 0.6010 - val_loss: 0.4433 - val_accuracy: 0.5588\n",
      "Epoch 317/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1505 - accuracy: 0.5985 - val_loss: 0.4442 - val_accuracy: 0.5559\n",
      "Epoch 318/1600\n",
      "680/680 [==============================] - 1s 758us/step - loss: 0.1640 - accuracy: 0.5962 - val_loss: 0.4388 - val_accuracy: 0.5559\n",
      "Epoch 319/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1504 - accuracy: 0.6010 - val_loss: 0.4195 - val_accuracy: 0.5529\n",
      "Epoch 320/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1544 - accuracy: 0.5985 - val_loss: 0.4694 - val_accuracy: 0.5676\n",
      "Epoch 321/1600\n",
      "680/680 [==============================] - 1s 751us/step - loss: 0.1662 - accuracy: 0.5977 - val_loss: 0.4542 - val_accuracy: 0.5500\n",
      "Epoch 322/1600\n",
      "680/680 [==============================] - 1s 827us/step - loss: 0.1562 - accuracy: 0.5999 - val_loss: 0.5044 - val_accuracy: 0.5500\n",
      "Epoch 323/1600\n",
      "680/680 [==============================] - 1s 831us/step - loss: 0.1778 - accuracy: 0.5970 - val_loss: 0.4300 - val_accuracy: 0.5559\n",
      "Epoch 324/1600\n",
      "680/680 [==============================] - 1s 830us/step - loss: 0.1436 - accuracy: 0.6029 - val_loss: 0.4453 - val_accuracy: 0.5471\n",
      "Epoch 325/1600\n",
      "680/680 [==============================] - 1s 810us/step - loss: 0.1482 - accuracy: 0.6025 - val_loss: 0.4650 - val_accuracy: 0.5471\n",
      "Epoch 326/1600\n",
      "680/680 [==============================] - 0s 734us/step - loss: 0.1622 - accuracy: 0.5981 - val_loss: 0.4820 - val_accuracy: 0.5588\n",
      "Epoch 327/1600\n",
      "680/680 [==============================] - 0s 705us/step - loss: 0.1436 - accuracy: 0.6018 - val_loss: 0.4428 - val_accuracy: 0.5618\n",
      "Epoch 328/1600\n",
      "680/680 [==============================] - 0s 711us/step - loss: 0.1685 - accuracy: 0.5966 - val_loss: 0.4077 - val_accuracy: 0.5559\n",
      "Epoch 329/1600\n",
      "680/680 [==============================] - 1s 765us/step - loss: 0.1697 - accuracy: 0.5988 - val_loss: 0.4710 - val_accuracy: 0.5559\n",
      "Epoch 330/1600\n",
      "680/680 [==============================] - 0s 693us/step - loss: 0.1664 - accuracy: 0.5974 - val_loss: 0.4634 - val_accuracy: 0.5588\n",
      "Epoch 331/1600\n",
      "680/680 [==============================] - 0s 708us/step - loss: 0.1430 - accuracy: 0.6025 - val_loss: 0.4480 - val_accuracy: 0.5559\n",
      "Epoch 332/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 0s 710us/step - loss: 0.1509 - accuracy: 0.6007 - val_loss: 0.4434 - val_accuracy: 0.5529\n",
      "Epoch 333/1600\n",
      "680/680 [==============================] - 0s 709us/step - loss: 0.1588 - accuracy: 0.5981 - val_loss: 0.4519 - val_accuracy: 0.5471\n",
      "Epoch 334/1600\n",
      "680/680 [==============================] - 0s 712us/step - loss: 0.1571 - accuracy: 0.6010 - val_loss: 0.4404 - val_accuracy: 0.5588\n",
      "Epoch 335/1600\n",
      "680/680 [==============================] - 0s 710us/step - loss: 0.1568 - accuracy: 0.5977 - val_loss: 0.4786 - val_accuracy: 0.5529\n",
      "Epoch 336/1600\n",
      "680/680 [==============================] - 1s 829us/step - loss: 0.1520 - accuracy: 0.6014 - val_loss: 0.4649 - val_accuracy: 0.5559\n",
      "Epoch 337/1600\n",
      "680/680 [==============================] - 1s 806us/step - loss: 0.1635 - accuracy: 0.5974 - val_loss: 0.4399 - val_accuracy: 0.5588\n",
      "Epoch 338/1600\n",
      "680/680 [==============================] - 1s 773us/step - loss: 0.1595 - accuracy: 0.5996 - val_loss: 0.4431 - val_accuracy: 0.5618\n",
      "Epoch 339/1600\n",
      "680/680 [==============================] - 1s 752us/step - loss: 0.1544 - accuracy: 0.5977 - val_loss: 0.4494 - val_accuracy: 0.5529\n",
      "Epoch 340/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1750 - accuracy: 0.5940 - val_loss: 0.4836 - val_accuracy: 0.5559\n",
      "Epoch 341/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1571 - accuracy: 0.6010 - val_loss: 0.4611 - val_accuracy: 0.5559\n",
      "Epoch 342/1600\n",
      "680/680 [==============================] - 1s 831us/step - loss: 0.1499 - accuracy: 0.6007 - val_loss: 0.4250 - val_accuracy: 0.5618\n",
      "Epoch 343/1600\n",
      "680/680 [==============================] - 1s 832us/step - loss: 0.1467 - accuracy: 0.6025 - val_loss: 0.4272 - val_accuracy: 0.5588\n",
      "Epoch 344/1600\n",
      "680/680 [==============================] - 1s 834us/step - loss: 0.1550 - accuracy: 0.5988 - val_loss: 0.4598 - val_accuracy: 0.5618\n",
      "Epoch 345/1600\n",
      "680/680 [==============================] - 1s 832us/step - loss: 0.1596 - accuracy: 0.5970 - val_loss: 0.4416 - val_accuracy: 0.5559\n",
      "Epoch 346/1600\n",
      "680/680 [==============================] - 1s 825us/step - loss: 0.1532 - accuracy: 0.5988 - val_loss: 0.4433 - val_accuracy: 0.5588\n",
      "Epoch 347/1600\n",
      "680/680 [==============================] - 1s 798us/step - loss: 0.1585 - accuracy: 0.5985 - val_loss: 0.4355 - val_accuracy: 0.5559\n",
      "Epoch 348/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1523 - accuracy: 0.5992 - val_loss: 0.4834 - val_accuracy: 0.5559\n",
      "Epoch 349/1600\n",
      "680/680 [==============================] - 1s 779us/step - loss: 0.1628 - accuracy: 0.5940 - val_loss: 0.4523 - val_accuracy: 0.5529\n",
      "Epoch 350/1600\n",
      "680/680 [==============================] - 1s 849us/step - loss: 0.1582 - accuracy: 0.5977 - val_loss: 0.4366 - val_accuracy: 0.5559\n",
      "Epoch 351/1600\n",
      "680/680 [==============================] - 1s 760us/step - loss: 0.1496 - accuracy: 0.6021 - val_loss: 0.4415 - val_accuracy: 0.5588\n",
      "Epoch 352/1600\n",
      "680/680 [==============================] - 0s 683us/step - loss: 0.1633 - accuracy: 0.5951 - val_loss: 0.4618 - val_accuracy: 0.5588\n",
      "Epoch 353/1600\n",
      "680/680 [==============================] - 0s 702us/step - loss: 0.1493 - accuracy: 0.6014 - val_loss: 0.4640 - val_accuracy: 0.5559\n",
      "Epoch 354/1600\n",
      "680/680 [==============================] - 0s 712us/step - loss: 0.1590 - accuracy: 0.5988 - val_loss: 0.4495 - val_accuracy: 0.5618\n",
      "Epoch 355/1600\n",
      "680/680 [==============================] - 1s 781us/step - loss: 0.1530 - accuracy: 0.6010 - val_loss: 0.4560 - val_accuracy: 0.5382\n",
      "Epoch 356/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1624 - accuracy: 0.5977 - val_loss: 0.3989 - val_accuracy: 0.5529\n",
      "Epoch 357/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1501 - accuracy: 0.6007 - val_loss: 0.4140 - val_accuracy: 0.5529\n",
      "Epoch 358/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1524 - accuracy: 0.5992 - val_loss: 0.4165 - val_accuracy: 0.5588\n",
      "Epoch 359/1600\n",
      "680/680 [==============================] - 0s 733us/step - loss: 0.1528 - accuracy: 0.5996 - val_loss: 0.4285 - val_accuracy: 0.5588\n",
      "Epoch 360/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1445 - accuracy: 0.6036 - val_loss: 0.4238 - val_accuracy: 0.5588\n",
      "Epoch 361/1600\n",
      "680/680 [==============================] - 0s 730us/step - loss: 0.1751 - accuracy: 0.5951 - val_loss: 0.4392 - val_accuracy: 0.5529\n",
      "Epoch 362/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1579 - accuracy: 0.6003 - val_loss: 0.4626 - val_accuracy: 0.5529\n",
      "Epoch 363/1600\n",
      "680/680 [==============================] - 1s 752us/step - loss: 0.1426 - accuracy: 0.6029 - val_loss: 0.4473 - val_accuracy: 0.5647\n",
      "Epoch 364/1600\n",
      "680/680 [==============================] - 1s 771us/step - loss: 0.1672 - accuracy: 0.5970 - val_loss: 0.4300 - val_accuracy: 0.5588\n",
      "Epoch 365/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1557 - accuracy: 0.5981 - val_loss: 0.4399 - val_accuracy: 0.5588\n",
      "Epoch 366/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1568 - accuracy: 0.5988 - val_loss: 0.4623 - val_accuracy: 0.5588\n",
      "Epoch 367/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1556 - accuracy: 0.5992 - val_loss: 0.4642 - val_accuracy: 0.5559\n",
      "Epoch 368/1600\n",
      "680/680 [==============================] - 1s 739us/step - loss: 0.1477 - accuracy: 0.6025 - val_loss: 0.4482 - val_accuracy: 0.5471\n",
      "Epoch 369/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1637 - accuracy: 0.5955 - val_loss: 0.4731 - val_accuracy: 0.5500\n",
      "Epoch 370/1600\n",
      "680/680 [==============================] - 1s 771us/step - loss: 0.1514 - accuracy: 0.6003 - val_loss: 0.4340 - val_accuracy: 0.5529\n",
      "Epoch 371/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1455 - accuracy: 0.6021 - val_loss: 0.4213 - val_accuracy: 0.5500\n",
      "Epoch 372/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1538 - accuracy: 0.5996 - val_loss: 0.4321 - val_accuracy: 0.5588\n",
      "Epoch 373/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1611 - accuracy: 0.5974 - val_loss: 0.4311 - val_accuracy: 0.5529\n",
      "Epoch 374/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1653 - accuracy: 0.5951 - val_loss: 0.4182 - val_accuracy: 0.5559\n",
      "Epoch 375/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1562 - accuracy: 0.5981 - val_loss: 0.4664 - val_accuracy: 0.5529\n",
      "Epoch 376/1600\n",
      "680/680 [==============================] - 1s 773us/step - loss: 0.1607 - accuracy: 0.5959 - val_loss: 0.4169 - val_accuracy: 0.5559\n",
      "Epoch 377/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1537 - accuracy: 0.5999 - val_loss: 0.4294 - val_accuracy: 0.5588\n",
      "Epoch 378/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1649 - accuracy: 0.5966 - val_loss: 0.4472 - val_accuracy: 0.5529\n",
      "Epoch 379/1600\n",
      "680/680 [==============================] - 1s 766us/step - loss: 0.1424 - accuracy: 0.6029 - val_loss: 0.4517 - val_accuracy: 0.5618\n",
      "Epoch 380/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1736 - accuracy: 0.5944 - val_loss: 0.4429 - val_accuracy: 0.5529\n",
      "Epoch 381/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1580 - accuracy: 0.5999 - val_loss: 0.4327 - val_accuracy: 0.5500\n",
      "Epoch 382/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1575 - accuracy: 0.5966 - val_loss: 0.4314 - val_accuracy: 0.5618\n",
      "Epoch 383/1600\n",
      "680/680 [==============================] - 1s 739us/step - loss: 0.1521 - accuracy: 0.5999 - val_loss: 0.4245 - val_accuracy: 0.5559\n",
      "Epoch 384/1600\n",
      "680/680 [==============================] - 1s 738us/step - loss: 0.1611 - accuracy: 0.5985 - val_loss: 0.4515 - val_accuracy: 0.5559\n",
      "Epoch 385/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1462 - accuracy: 0.6025 - val_loss: 0.4409 - val_accuracy: 0.5588\n",
      "Epoch 386/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1629 - accuracy: 0.5977 - val_loss: 0.4223 - val_accuracy: 0.5588\n",
      "Epoch 387/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 750us/step - loss: 0.1517 - accuracy: 0.6003 - val_loss: 0.4474 - val_accuracy: 0.5618\n",
      "Epoch 388/1600\n",
      "680/680 [==============================] - 0s 703us/step - loss: 0.1644 - accuracy: 0.5955 - val_loss: 0.4369 - val_accuracy: 0.5676\n",
      "Epoch 389/1600\n",
      "680/680 [==============================] - 0s 714us/step - loss: 0.1509 - accuracy: 0.5999 - val_loss: 0.4313 - val_accuracy: 0.5529\n",
      "Epoch 390/1600\n",
      "680/680 [==============================] - 0s 711us/step - loss: 0.1586 - accuracy: 0.5974 - val_loss: 0.4449 - val_accuracy: 0.5588\n",
      "Epoch 391/1600\n",
      "680/680 [==============================] - 0s 708us/step - loss: 0.1662 - accuracy: 0.5981 - val_loss: 0.4592 - val_accuracy: 0.5588\n",
      "Epoch 392/1600\n",
      "680/680 [==============================] - 0s 711us/step - loss: 0.1721 - accuracy: 0.5970 - val_loss: 0.4622 - val_accuracy: 0.5529\n",
      "Epoch 393/1600\n",
      "680/680 [==============================] - 0s 713us/step - loss: 0.1442 - accuracy: 0.6021 - val_loss: 0.4565 - val_accuracy: 0.5529\n",
      "Epoch 394/1600\n",
      "680/680 [==============================] - 0s 710us/step - loss: 0.1455 - accuracy: 0.6047 - val_loss: 0.4205 - val_accuracy: 0.5471\n",
      "Epoch 395/1600\n",
      "680/680 [==============================] - 0s 708us/step - loss: 0.1517 - accuracy: 0.5996 - val_loss: 0.4226 - val_accuracy: 0.5559\n",
      "Epoch 396/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1549 - accuracy: 0.5988 - val_loss: 0.4655 - val_accuracy: 0.5559\n",
      "Epoch 397/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1684 - accuracy: 0.5962 - val_loss: 0.4439 - val_accuracy: 0.5559\n",
      "Epoch 398/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1551 - accuracy: 0.6014 - val_loss: 0.4177 - val_accuracy: 0.5500\n",
      "Epoch 399/1600\n",
      "680/680 [==============================] - 1s 751us/step - loss: 0.1464 - accuracy: 0.6007 - val_loss: 0.4399 - val_accuracy: 0.5529\n",
      "Epoch 400/1600\n",
      "680/680 [==============================] - 1s 751us/step - loss: 0.1536 - accuracy: 0.6014 - val_loss: 0.4558 - val_accuracy: 0.5500\n",
      "Epoch 401/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1601 - accuracy: 0.5988 - val_loss: 0.4516 - val_accuracy: 0.5618\n",
      "Epoch 402/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1604 - accuracy: 0.5974 - val_loss: 0.4394 - val_accuracy: 0.5559\n",
      "Epoch 403/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1519 - accuracy: 0.5996 - val_loss: 0.4236 - val_accuracy: 0.5529\n",
      "Epoch 404/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1579 - accuracy: 0.5974 - val_loss: 0.4181 - val_accuracy: 0.5588\n",
      "Epoch 405/1600\n",
      "680/680 [==============================] - 1s 753us/step - loss: 0.1559 - accuracy: 0.6010 - val_loss: 0.4423 - val_accuracy: 0.5618\n",
      "Epoch 406/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1459 - accuracy: 0.6018 - val_loss: 0.4292 - val_accuracy: 0.5588\n",
      "Epoch 407/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1490 - accuracy: 0.6007 - val_loss: 0.4069 - val_accuracy: 0.5588\n",
      "Epoch 408/1600\n",
      "680/680 [==============================] - 1s 751us/step - loss: 0.1537 - accuracy: 0.5974 - val_loss: 0.4525 - val_accuracy: 0.5618\n",
      "Epoch 409/1600\n",
      "680/680 [==============================] - 1s 753us/step - loss: 0.1647 - accuracy: 0.5970 - val_loss: 0.4388 - val_accuracy: 0.5588\n",
      "Epoch 410/1600\n",
      "680/680 [==============================] - 1s 753us/step - loss: 0.1508 - accuracy: 0.5996 - val_loss: 0.4230 - val_accuracy: 0.5588\n",
      "Epoch 411/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1523 - accuracy: 0.5985 - val_loss: 0.4425 - val_accuracy: 0.5706\n",
      "Epoch 412/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1594 - accuracy: 0.5988 - val_loss: 0.4211 - val_accuracy: 0.5529\n",
      "Epoch 413/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1495 - accuracy: 0.6003 - val_loss: 0.4561 - val_accuracy: 0.5647\n",
      "Epoch 414/1600\n",
      "680/680 [==============================] - 1s 800us/step - loss: 0.1506 - accuracy: 0.6021 - val_loss: 0.4225 - val_accuracy: 0.5529\n",
      "Epoch 415/1600\n",
      "680/680 [==============================] - 1s 770us/step - loss: 0.1572 - accuracy: 0.5996 - val_loss: 0.4722 - val_accuracy: 0.5471\n",
      "Epoch 416/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1671 - accuracy: 0.5966 - val_loss: 0.4399 - val_accuracy: 0.5529\n",
      "Epoch 417/1600\n",
      "680/680 [==============================] - 1s 776us/step - loss: 0.1611 - accuracy: 0.5981 - val_loss: 0.4575 - val_accuracy: 0.5500\n",
      "Epoch 418/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1527 - accuracy: 0.6014 - val_loss: 0.4389 - val_accuracy: 0.5588\n",
      "Epoch 419/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1498 - accuracy: 0.6014 - val_loss: 0.4525 - val_accuracy: 0.5588\n",
      "Epoch 420/1600\n",
      "680/680 [==============================] - 1s 752us/step - loss: 0.1513 - accuracy: 0.6018 - val_loss: 0.4742 - val_accuracy: 0.5559\n",
      "Epoch 421/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1588 - accuracy: 0.5988 - val_loss: 0.4531 - val_accuracy: 0.5588\n",
      "Epoch 422/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1505 - accuracy: 0.6018 - val_loss: 0.4202 - val_accuracy: 0.5500\n",
      "Epoch 423/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1591 - accuracy: 0.5996 - val_loss: 0.4673 - val_accuracy: 0.5471\n",
      "Epoch 424/1600\n",
      "680/680 [==============================] - 1s 769us/step - loss: 0.1622 - accuracy: 0.5985 - val_loss: 0.4466 - val_accuracy: 0.5559\n",
      "Epoch 425/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1623 - accuracy: 0.5962 - val_loss: 0.4641 - val_accuracy: 0.5588\n",
      "Epoch 426/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1520 - accuracy: 0.5992 - val_loss: 0.4402 - val_accuracy: 0.5559\n",
      "Epoch 427/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1471 - accuracy: 0.6021 - val_loss: 0.4611 - val_accuracy: 0.5529\n",
      "Epoch 428/1600\n",
      "680/680 [==============================] - 1s 753us/step - loss: 0.1401 - accuracy: 0.6051 - val_loss: 0.4459 - val_accuracy: 0.5529\n",
      "Epoch 429/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1527 - accuracy: 0.6014 - val_loss: 0.4374 - val_accuracy: 0.5559\n",
      "Epoch 430/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1622 - accuracy: 0.5966 - val_loss: 0.4715 - val_accuracy: 0.5588\n",
      "Epoch 431/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1564 - accuracy: 0.6003 - val_loss: 0.4515 - val_accuracy: 0.5529\n",
      "Epoch 432/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1586 - accuracy: 0.5988 - val_loss: 0.4622 - val_accuracy: 0.5647\n",
      "Epoch 433/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1647 - accuracy: 0.5996 - val_loss: 0.4489 - val_accuracy: 0.5559\n",
      "Epoch 434/1600\n",
      "680/680 [==============================] - 1s 833us/step - loss: 0.1428 - accuracy: 0.6036 - val_loss: 0.4291 - val_accuracy: 0.5529\n",
      "Epoch 435/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1492 - accuracy: 0.6021 - val_loss: 0.4388 - val_accuracy: 0.5559\n",
      "Epoch 436/1600\n",
      "680/680 [==============================] - 1s 801us/step - loss: 0.1722 - accuracy: 0.5985 - val_loss: 0.4300 - val_accuracy: 0.5471\n",
      "Epoch 437/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1533 - accuracy: 0.5992 - val_loss: 0.4119 - val_accuracy: 0.5588\n",
      "Epoch 438/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1586 - accuracy: 0.5981 - val_loss: 0.4423 - val_accuracy: 0.5500\n",
      "Epoch 439/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1537 - accuracy: 0.6003 - val_loss: 0.4819 - val_accuracy: 0.5500\n",
      "Epoch 440/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1514 - accuracy: 0.5992 - val_loss: 0.4619 - val_accuracy: 0.5559\n",
      "Epoch 441/1600\n",
      "680/680 [==============================] - 1s 781us/step - loss: 0.1516 - accuracy: 0.6014 - val_loss: 0.4299 - val_accuracy: 0.5647\n",
      "Epoch 442/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 782us/step - loss: 0.1582 - accuracy: 0.6014 - val_loss: 0.4434 - val_accuracy: 0.5559\n",
      "Epoch 443/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1594 - accuracy: 0.5977 - val_loss: 0.4681 - val_accuracy: 0.5529\n",
      "Epoch 444/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1616 - accuracy: 0.5970 - val_loss: 0.4219 - val_accuracy: 0.5471\n",
      "Epoch 445/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1539 - accuracy: 0.5999 - val_loss: 0.4468 - val_accuracy: 0.5529\n",
      "Epoch 446/1600\n",
      "680/680 [==============================] - 0s 712us/step - loss: 0.1540 - accuracy: 0.6010 - val_loss: 0.4640 - val_accuracy: 0.5559\n",
      "Epoch 447/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1598 - accuracy: 0.5996 - val_loss: 0.4369 - val_accuracy: 0.5471\n",
      "Epoch 448/1600\n",
      "680/680 [==============================] - 1s 797us/step - loss: 0.1426 - accuracy: 0.6058 - val_loss: 0.4566 - val_accuracy: 0.5529\n",
      "Epoch 449/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1581 - accuracy: 0.5977 - val_loss: 0.4501 - val_accuracy: 0.5441\n",
      "Epoch 450/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1578 - accuracy: 0.5985 - val_loss: 0.4388 - val_accuracy: 0.5529\n",
      "Epoch 451/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1611 - accuracy: 0.5988 - val_loss: 0.4632 - val_accuracy: 0.5588\n",
      "Epoch 452/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1450 - accuracy: 0.6021 - val_loss: 0.4769 - val_accuracy: 0.5618\n",
      "Epoch 453/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1484 - accuracy: 0.6025 - val_loss: 0.4785 - val_accuracy: 0.5529\n",
      "Epoch 454/1600\n",
      "680/680 [==============================] - 1s 753us/step - loss: 0.1693 - accuracy: 0.5962 - val_loss: 0.4408 - val_accuracy: 0.5529\n",
      "Epoch 455/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1594 - accuracy: 0.5985 - val_loss: 0.4677 - val_accuracy: 0.5529\n",
      "Epoch 456/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1717 - accuracy: 0.5940 - val_loss: 0.4589 - val_accuracy: 0.5559\n",
      "Epoch 457/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1512 - accuracy: 0.6007 - val_loss: 0.4579 - val_accuracy: 0.5529\n",
      "Epoch 458/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1520 - accuracy: 0.6007 - val_loss: 0.4395 - val_accuracy: 0.5529\n",
      "Epoch 459/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1598 - accuracy: 0.5974 - val_loss: 0.4464 - val_accuracy: 0.5559\n",
      "Epoch 460/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1718 - accuracy: 0.5959 - val_loss: 0.4237 - val_accuracy: 0.5618\n",
      "Epoch 461/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1501 - accuracy: 0.6018 - val_loss: 0.4190 - val_accuracy: 0.5647\n",
      "Epoch 462/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1450 - accuracy: 0.6003 - val_loss: 0.4769 - val_accuracy: 0.5559\n",
      "Epoch 463/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1432 - accuracy: 0.6043 - val_loss: 0.4508 - val_accuracy: 0.5588\n",
      "Epoch 464/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1563 - accuracy: 0.5992 - val_loss: 0.4882 - val_accuracy: 0.5618\n",
      "Epoch 465/1600\n",
      "680/680 [==============================] - 1s 837us/step - loss: 0.1648 - accuracy: 0.5985 - val_loss: 0.4271 - val_accuracy: 0.5559\n",
      "Epoch 466/1600\n",
      "680/680 [==============================] - 1s 834us/step - loss: 0.1637 - accuracy: 0.5981 - val_loss: 0.4530 - val_accuracy: 0.5618\n",
      "Epoch 467/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1498 - accuracy: 0.6003 - val_loss: 0.4237 - val_accuracy: 0.5529\n",
      "Epoch 468/1600\n",
      "680/680 [==============================] - 1s 782us/step - loss: 0.1654 - accuracy: 0.5966 - val_loss: 0.4420 - val_accuracy: 0.5529\n",
      "Epoch 469/1600\n",
      "680/680 [==============================] - 1s 773us/step - loss: 0.1550 - accuracy: 0.5988 - val_loss: 0.4478 - val_accuracy: 0.5588\n",
      "Epoch 470/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1407 - accuracy: 0.6043 - val_loss: 0.4270 - val_accuracy: 0.5647\n",
      "Epoch 471/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1602 - accuracy: 0.6007 - val_loss: 0.4571 - val_accuracy: 0.5500\n",
      "Epoch 472/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1615 - accuracy: 0.5996 - val_loss: 0.4427 - val_accuracy: 0.5588\n",
      "Epoch 473/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1642 - accuracy: 0.5955 - val_loss: 0.4451 - val_accuracy: 0.5500\n",
      "Epoch 474/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1537 - accuracy: 0.5996 - val_loss: 0.4168 - val_accuracy: 0.5588\n",
      "Epoch 475/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1510 - accuracy: 0.5977 - val_loss: 0.4218 - val_accuracy: 0.5559\n",
      "Epoch 476/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1516 - accuracy: 0.6025 - val_loss: 0.4399 - val_accuracy: 0.5529\n",
      "Epoch 477/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1569 - accuracy: 0.5992 - val_loss: 0.4559 - val_accuracy: 0.5471\n",
      "Epoch 478/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1520 - accuracy: 0.5985 - val_loss: 0.4295 - val_accuracy: 0.5529\n",
      "Epoch 479/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1542 - accuracy: 0.5985 - val_loss: 0.4930 - val_accuracy: 0.5529\n",
      "Epoch 480/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1635 - accuracy: 0.5970 - val_loss: 0.4367 - val_accuracy: 0.5529\n",
      "Epoch 481/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1582 - accuracy: 0.5985 - val_loss: 0.4391 - val_accuracy: 0.5559\n",
      "Epoch 482/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1505 - accuracy: 0.6032 - val_loss: 0.4538 - val_accuracy: 0.5529\n",
      "Epoch 483/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1553 - accuracy: 0.5992 - val_loss: 0.4533 - val_accuracy: 0.5471\n",
      "Epoch 484/1600\n",
      "680/680 [==============================] - 1s 830us/step - loss: 0.1498 - accuracy: 0.6021 - val_loss: 0.4427 - val_accuracy: 0.5588\n",
      "Epoch 485/1600\n",
      "680/680 [==============================] - 1s 828us/step - loss: 0.1547 - accuracy: 0.5988 - val_loss: 0.4322 - val_accuracy: 0.5618\n",
      "Epoch 486/1600\n",
      "680/680 [==============================] - 1s 832us/step - loss: 0.1619 - accuracy: 0.5981 - val_loss: 0.4129 - val_accuracy: 0.5471\n",
      "Epoch 487/1600\n",
      "680/680 [==============================] - 1s 826us/step - loss: 0.1459 - accuracy: 0.6021 - val_loss: 0.4355 - val_accuracy: 0.5529\n",
      "Epoch 488/1600\n",
      "680/680 [==============================] - 1s 838us/step - loss: 0.1541 - accuracy: 0.5985 - val_loss: 0.4508 - val_accuracy: 0.5588\n",
      "Epoch 489/1600\n",
      "680/680 [==============================] - 1s 828us/step - loss: 0.1517 - accuracy: 0.5992 - val_loss: 0.4494 - val_accuracy: 0.5529\n",
      "Epoch 490/1600\n",
      "680/680 [==============================] - 1s 801us/step - loss: 0.1737 - accuracy: 0.5933 - val_loss: 0.4625 - val_accuracy: 0.5588\n",
      "Epoch 491/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1552 - accuracy: 0.5999 - val_loss: 0.4376 - val_accuracy: 0.5588\n",
      "Epoch 492/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1560 - accuracy: 0.6010 - val_loss: 0.4384 - val_accuracy: 0.5529\n",
      "Epoch 493/1600\n",
      "680/680 [==============================] - 1s 756us/step - loss: 0.1548 - accuracy: 0.5977 - val_loss: 0.4631 - val_accuracy: 0.5559\n",
      "Epoch 494/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1591 - accuracy: 0.5974 - val_loss: 0.4604 - val_accuracy: 0.5559\n",
      "Epoch 495/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1662 - accuracy: 0.5974 - val_loss: 0.4457 - val_accuracy: 0.5471\n",
      "Epoch 496/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1769 - accuracy: 0.5940 - val_loss: 0.4577 - val_accuracy: 0.5500\n",
      "Epoch 497/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 749us/step - loss: 0.1475 - accuracy: 0.6025 - val_loss: 0.4295 - val_accuracy: 0.5618\n",
      "Epoch 498/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1393 - accuracy: 0.6036 - val_loss: 0.4512 - val_accuracy: 0.5529\n",
      "Epoch 499/1600\n",
      "680/680 [==============================] - 1s 752us/step - loss: 0.1561 - accuracy: 0.5992 - val_loss: 0.4293 - val_accuracy: 0.5618\n",
      "Epoch 500/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1701 - accuracy: 0.5970 - val_loss: 0.4637 - val_accuracy: 0.5529\n",
      "Epoch 501/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1719 - accuracy: 0.5962 - val_loss: 0.4390 - val_accuracy: 0.5559\n",
      "Epoch 502/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1472 - accuracy: 0.6003 - val_loss: 0.4422 - val_accuracy: 0.5618\n",
      "Epoch 503/1600\n",
      "680/680 [==============================] - 0s 731us/step - loss: 0.1677 - accuracy: 0.5970 - val_loss: 0.4406 - val_accuracy: 0.5441\n",
      "Epoch 504/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1554 - accuracy: 0.5992 - val_loss: 0.4721 - val_accuracy: 0.5529\n",
      "Epoch 505/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1483 - accuracy: 0.6036 - val_loss: 0.4546 - val_accuracy: 0.5618\n",
      "Epoch 506/1600\n",
      "680/680 [==============================] - 1s 738us/step - loss: 0.1625 - accuracy: 0.5981 - val_loss: 0.4727 - val_accuracy: 0.5500\n",
      "Epoch 507/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1468 - accuracy: 0.6014 - val_loss: 0.4699 - val_accuracy: 0.5559\n",
      "Epoch 508/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1491 - accuracy: 0.5999 - val_loss: 0.4585 - val_accuracy: 0.5559\n",
      "Epoch 509/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1495 - accuracy: 0.6010 - val_loss: 0.4662 - val_accuracy: 0.5529\n",
      "Epoch 510/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1596 - accuracy: 0.6007 - val_loss: 0.4768 - val_accuracy: 0.5529\n",
      "Epoch 511/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1625 - accuracy: 0.5974 - val_loss: 0.4614 - val_accuracy: 0.5500\n",
      "Epoch 512/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1581 - accuracy: 0.5974 - val_loss: 0.4607 - val_accuracy: 0.5500\n",
      "Epoch 513/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1443 - accuracy: 0.6014 - val_loss: 0.4502 - val_accuracy: 0.5647\n",
      "Epoch 514/1600\n",
      "680/680 [==============================] - 1s 735us/step - loss: 0.1566 - accuracy: 0.5985 - val_loss: 0.4742 - val_accuracy: 0.5588\n",
      "Epoch 515/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1504 - accuracy: 0.6014 - val_loss: 0.4673 - val_accuracy: 0.5529\n",
      "Epoch 516/1600\n",
      "680/680 [==============================] - 1s 738us/step - loss: 0.1555 - accuracy: 0.6010 - val_loss: 0.4293 - val_accuracy: 0.5588\n",
      "Epoch 517/1600\n",
      "680/680 [==============================] - 1s 738us/step - loss: 0.1494 - accuracy: 0.5977 - val_loss: 0.4406 - val_accuracy: 0.5559\n",
      "Epoch 518/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1592 - accuracy: 0.5985 - val_loss: 0.4640 - val_accuracy: 0.5529\n",
      "Epoch 519/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1592 - accuracy: 0.5974 - val_loss: 0.4535 - val_accuracy: 0.5618\n",
      "Epoch 520/1600\n",
      "680/680 [==============================] - 1s 736us/step - loss: 0.1580 - accuracy: 0.5974 - val_loss: 0.4370 - val_accuracy: 0.5588\n",
      "Epoch 521/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1621 - accuracy: 0.6003 - val_loss: 0.4421 - val_accuracy: 0.5559\n",
      "Epoch 522/1600\n",
      "680/680 [==============================] - 0s 733us/step - loss: 0.1504 - accuracy: 0.5996 - val_loss: 0.4492 - val_accuracy: 0.5588\n",
      "Epoch 523/1600\n",
      "680/680 [==============================] - 0s 733us/step - loss: 0.1699 - accuracy: 0.5977 - val_loss: 0.4621 - val_accuracy: 0.5529\n",
      "Epoch 524/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1597 - accuracy: 0.5974 - val_loss: 0.4315 - val_accuracy: 0.5559\n",
      "Epoch 525/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1525 - accuracy: 0.5962 - val_loss: 0.4619 - val_accuracy: 0.5529\n",
      "Epoch 526/1600\n",
      "680/680 [==============================] - 0s 730us/step - loss: 0.1583 - accuracy: 0.5985 - val_loss: 0.4489 - val_accuracy: 0.5500\n",
      "Epoch 527/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1507 - accuracy: 0.5999 - val_loss: 0.4373 - val_accuracy: 0.5500\n",
      "Epoch 528/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1506 - accuracy: 0.5992 - val_loss: 0.4583 - val_accuracy: 0.5500\n",
      "Epoch 529/1600\n",
      "680/680 [==============================] - 0s 733us/step - loss: 0.1528 - accuracy: 0.5992 - val_loss: 0.4517 - val_accuracy: 0.5471\n",
      "Epoch 530/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1592 - accuracy: 0.5970 - val_loss: 0.4565 - val_accuracy: 0.5559\n",
      "Epoch 531/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1567 - accuracy: 0.5988 - val_loss: 0.4478 - val_accuracy: 0.5618\n",
      "Epoch 532/1600\n",
      "680/680 [==============================] - 1s 740us/step - loss: 0.1573 - accuracy: 0.5977 - val_loss: 0.4337 - val_accuracy: 0.5618\n",
      "Epoch 533/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1458 - accuracy: 0.6021 - val_loss: 0.4561 - val_accuracy: 0.5588\n",
      "Epoch 534/1600\n",
      "680/680 [==============================] - 1s 834us/step - loss: 0.1526 - accuracy: 0.6003 - val_loss: 0.4035 - val_accuracy: 0.5529\n",
      "Epoch 535/1600\n",
      "680/680 [==============================] - 1s 828us/step - loss: 0.1539 - accuracy: 0.5996 - val_loss: 0.4273 - val_accuracy: 0.5559\n",
      "Epoch 536/1600\n",
      "680/680 [==============================] - 0s 690us/step - loss: 0.1597 - accuracy: 0.5974 - val_loss: 0.4390 - val_accuracy: 0.5618\n",
      "Epoch 537/1600\n",
      "680/680 [==============================] - 0s 713us/step - loss: 0.1598 - accuracy: 0.5966 - val_loss: 0.4285 - val_accuracy: 0.5529\n",
      "Epoch 538/1600\n",
      "680/680 [==============================] - 0s 702us/step - loss: 0.1613 - accuracy: 0.5966 - val_loss: 0.4571 - val_accuracy: 0.5529\n",
      "Epoch 539/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1457 - accuracy: 0.6010 - val_loss: 0.4561 - val_accuracy: 0.5529\n",
      "Epoch 540/1600\n",
      "680/680 [==============================] - 1s 762us/step - loss: 0.1453 - accuracy: 0.6018 - val_loss: 0.4451 - val_accuracy: 0.5559\n",
      "Epoch 541/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1476 - accuracy: 0.6007 - val_loss: 0.4539 - val_accuracy: 0.5412\n",
      "Epoch 542/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1677 - accuracy: 0.5970 - val_loss: 0.4219 - val_accuracy: 0.5559\n",
      "Epoch 543/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1539 - accuracy: 0.6007 - val_loss: 0.4474 - val_accuracy: 0.5588\n",
      "Epoch 544/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1397 - accuracy: 0.6025 - val_loss: 0.4549 - val_accuracy: 0.5559\n",
      "Epoch 545/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1618 - accuracy: 0.5955 - val_loss: 0.4158 - val_accuracy: 0.5588\n",
      "Epoch 546/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1602 - accuracy: 0.5992 - val_loss: 0.4727 - val_accuracy: 0.5647\n",
      "Epoch 547/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1579 - accuracy: 0.5977 - val_loss: 0.4379 - val_accuracy: 0.5500\n",
      "Epoch 548/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1591 - accuracy: 0.5955 - val_loss: 0.4539 - val_accuracy: 0.5647\n",
      "Epoch 549/1600\n",
      "680/680 [==============================] - 1s 832us/step - loss: 0.1475 - accuracy: 0.5999 - val_loss: 0.4708 - val_accuracy: 0.5471\n",
      "Epoch 550/1600\n",
      "680/680 [==============================] - 0s 709us/step - loss: 0.1580 - accuracy: 0.5966 - val_loss: 0.5059 - val_accuracy: 0.5588\n",
      "Epoch 551/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1607 - accuracy: 0.5970 - val_loss: 0.4753 - val_accuracy: 0.5500\n",
      "Epoch 552/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 0s 731us/step - loss: 0.1545 - accuracy: 0.6007 - val_loss: 0.4863 - val_accuracy: 0.5647\n",
      "Epoch 553/1600\n",
      "680/680 [==============================] - 0s 702us/step - loss: 0.1671 - accuracy: 0.5955 - val_loss: 0.4429 - val_accuracy: 0.5618\n",
      "Epoch 554/1600\n",
      "680/680 [==============================] - 0s 715us/step - loss: 0.1509 - accuracy: 0.6007 - val_loss: 0.5211 - val_accuracy: 0.5588\n",
      "Epoch 555/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1595 - accuracy: 0.5977 - val_loss: 0.4605 - val_accuracy: 0.5529\n",
      "Epoch 556/1600\n",
      "680/680 [==============================] - 0s 716us/step - loss: 0.1606 - accuracy: 0.5988 - val_loss: 0.4449 - val_accuracy: 0.5559\n",
      "Epoch 557/1600\n",
      "680/680 [==============================] - 1s 780us/step - loss: 0.1585 - accuracy: 0.5974 - val_loss: 0.4338 - val_accuracy: 0.5529\n",
      "Epoch 558/1600\n",
      "680/680 [==============================] - 0s 715us/step - loss: 0.1552 - accuracy: 0.5981 - val_loss: 0.4668 - val_accuracy: 0.5588\n",
      "Epoch 559/1600\n",
      "680/680 [==============================] - 0s 735us/step - loss: 0.1477 - accuracy: 0.6003 - val_loss: 0.4641 - val_accuracy: 0.5529\n",
      "Epoch 560/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1457 - accuracy: 0.6003 - val_loss: 0.4283 - val_accuracy: 0.5529\n",
      "Epoch 561/1600\n",
      "680/680 [==============================] - 1s 756us/step - loss: 0.1438 - accuracy: 0.6029 - val_loss: 0.4800 - val_accuracy: 0.5588\n",
      "Epoch 562/1600\n",
      "680/680 [==============================] - 0s 705us/step - loss: 0.1596 - accuracy: 0.5959 - val_loss: 0.4297 - val_accuracy: 0.5500\n",
      "Epoch 563/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1491 - accuracy: 0.6003 - val_loss: 0.4756 - val_accuracy: 0.5471\n",
      "Epoch 564/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1551 - accuracy: 0.6007 - val_loss: 0.4304 - val_accuracy: 0.5618\n",
      "Epoch 565/1600\n",
      "680/680 [==============================] - 0s 694us/step - loss: 0.1664 - accuracy: 0.5948 - val_loss: 0.4269 - val_accuracy: 0.5529\n",
      "Epoch 566/1600\n",
      "680/680 [==============================] - 0s 682us/step - loss: 0.1584 - accuracy: 0.5985 - val_loss: 0.4456 - val_accuracy: 0.5559\n",
      "Epoch 567/1600\n",
      "680/680 [==============================] - 0s 708us/step - loss: 0.1520 - accuracy: 0.6007 - val_loss: 0.4146 - val_accuracy: 0.5647\n",
      "Epoch 568/1600\n",
      "680/680 [==============================] - 0s 709us/step - loss: 0.1545 - accuracy: 0.6007 - val_loss: 0.4442 - val_accuracy: 0.5588\n",
      "Epoch 569/1600\n",
      "680/680 [==============================] - 0s 711us/step - loss: 0.1560 - accuracy: 0.5992 - val_loss: 0.4494 - val_accuracy: 0.5529\n",
      "Epoch 570/1600\n",
      "680/680 [==============================] - 0s 711us/step - loss: 0.1699 - accuracy: 0.5948 - val_loss: 0.4330 - val_accuracy: 0.5559\n",
      "Epoch 571/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1420 - accuracy: 0.6032 - val_loss: 0.4471 - val_accuracy: 0.5588\n",
      "Epoch 572/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1464 - accuracy: 0.6021 - val_loss: 0.4559 - val_accuracy: 0.5500\n",
      "Epoch 573/1600\n",
      "680/680 [==============================] - 1s 827us/step - loss: 0.1516 - accuracy: 0.6003 - val_loss: 0.4481 - val_accuracy: 0.5618\n",
      "Epoch 574/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1621 - accuracy: 0.5951 - val_loss: 0.4421 - val_accuracy: 0.5559\n",
      "Epoch 575/1600\n",
      "680/680 [==============================] - 1s 821us/step - loss: 0.1502 - accuracy: 0.5999 - val_loss: 0.4325 - val_accuracy: 0.5588\n",
      "Epoch 576/1600\n",
      "680/680 [==============================] - 1s 831us/step - loss: 0.1526 - accuracy: 0.6003 - val_loss: 0.4671 - val_accuracy: 0.5588\n",
      "Epoch 577/1600\n",
      "680/680 [==============================] - 1s 825us/step - loss: 0.1530 - accuracy: 0.5999 - val_loss: 0.4366 - val_accuracy: 0.5529\n",
      "Epoch 578/1600\n",
      "680/680 [==============================] - 1s 820us/step - loss: 0.1582 - accuracy: 0.5985 - val_loss: 0.4384 - val_accuracy: 0.5559\n",
      "Epoch 579/1600\n",
      "680/680 [==============================] - 1s 798us/step - loss: 0.1636 - accuracy: 0.5955 - val_loss: 0.4376 - val_accuracy: 0.5529\n",
      "Epoch 580/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1521 - accuracy: 0.6014 - val_loss: 0.4498 - val_accuracy: 0.5529\n",
      "Epoch 581/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1624 - accuracy: 0.5985 - val_loss: 0.4320 - val_accuracy: 0.5559\n",
      "Epoch 582/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1552 - accuracy: 0.5996 - val_loss: 0.4469 - val_accuracy: 0.5618\n",
      "Epoch 583/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1526 - accuracy: 0.5999 - val_loss: 0.4677 - val_accuracy: 0.5529\n",
      "Epoch 584/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1479 - accuracy: 0.6007 - val_loss: 0.4391 - val_accuracy: 0.5588\n",
      "Epoch 585/1600\n",
      "680/680 [==============================] - 1s 781us/step - loss: 0.1452 - accuracy: 0.6010 - val_loss: 0.4513 - val_accuracy: 0.5500\n",
      "Epoch 586/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1430 - accuracy: 0.6018 - val_loss: 0.4176 - val_accuracy: 0.5500\n",
      "Epoch 587/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1411 - accuracy: 0.6032 - val_loss: 0.4563 - val_accuracy: 0.5529\n",
      "Epoch 588/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1518 - accuracy: 0.6010 - val_loss: 0.4561 - val_accuracy: 0.5500\n",
      "Epoch 589/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1793 - accuracy: 0.5926 - val_loss: 0.4284 - val_accuracy: 0.5559\n",
      "Epoch 590/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1554 - accuracy: 0.5999 - val_loss: 0.4675 - val_accuracy: 0.5647\n",
      "Epoch 591/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1656 - accuracy: 0.5940 - val_loss: 0.4427 - val_accuracy: 0.5588\n",
      "Epoch 592/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1485 - accuracy: 0.6018 - val_loss: 0.4505 - val_accuracy: 0.5500\n",
      "Epoch 593/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1482 - accuracy: 0.6007 - val_loss: 0.4452 - val_accuracy: 0.5588\n",
      "Epoch 594/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1596 - accuracy: 0.5981 - val_loss: 0.4420 - val_accuracy: 0.5529\n",
      "Epoch 595/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1588 - accuracy: 0.5966 - val_loss: 0.4376 - val_accuracy: 0.5588\n",
      "Epoch 596/1600\n",
      "680/680 [==============================] - 1s 782us/step - loss: 0.1428 - accuracy: 0.6025 - val_loss: 0.4579 - val_accuracy: 0.5500\n",
      "Epoch 597/1600\n",
      "680/680 [==============================] - 0s 695us/step - loss: 0.1581 - accuracy: 0.5981 - val_loss: 0.4517 - val_accuracy: 0.5588\n",
      "Epoch 598/1600\n",
      "680/680 [==============================] - 1s 752us/step - loss: 0.1470 - accuracy: 0.6003 - val_loss: 0.4400 - val_accuracy: 0.5618\n",
      "Epoch 599/1600\n",
      "680/680 [==============================] - 1s 753us/step - loss: 0.1500 - accuracy: 0.6007 - val_loss: 0.4372 - val_accuracy: 0.5529\n",
      "Epoch 600/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1635 - accuracy: 0.6003 - val_loss: 0.4555 - val_accuracy: 0.5353\n",
      "Epoch 601/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1549 - accuracy: 0.5999 - val_loss: 0.4257 - val_accuracy: 0.5500\n",
      "Epoch 602/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1564 - accuracy: 0.5999 - val_loss: 0.4701 - val_accuracy: 0.5559\n",
      "Epoch 603/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1470 - accuracy: 0.6010 - val_loss: 0.4519 - val_accuracy: 0.5559\n",
      "Epoch 604/1600\n",
      "680/680 [==============================] - 1s 756us/step - loss: 0.1589 - accuracy: 0.5974 - val_loss: 0.4481 - val_accuracy: 0.5559\n",
      "Epoch 605/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1535 - accuracy: 0.5988 - val_loss: 0.4373 - val_accuracy: 0.5529\n",
      "Epoch 606/1600\n",
      "680/680 [==============================] - 1s 759us/step - loss: 0.1520 - accuracy: 0.5992 - val_loss: 0.4434 - val_accuracy: 0.5588\n",
      "Epoch 607/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 748us/step - loss: 0.1578 - accuracy: 0.5981 - val_loss: 0.4477 - val_accuracy: 0.5471\n",
      "Epoch 608/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1526 - accuracy: 0.5996 - val_loss: 0.4722 - val_accuracy: 0.5529\n",
      "Epoch 609/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1504 - accuracy: 0.6007 - val_loss: 0.4837 - val_accuracy: 0.5412\n",
      "Epoch 610/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1575 - accuracy: 0.5981 - val_loss: 0.4465 - val_accuracy: 0.5500\n",
      "Epoch 611/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1454 - accuracy: 0.6014 - val_loss: 0.4633 - val_accuracy: 0.5588\n",
      "Epoch 612/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1557 - accuracy: 0.5977 - val_loss: 0.4376 - val_accuracy: 0.5618\n",
      "Epoch 613/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1442 - accuracy: 0.6021 - val_loss: 0.4731 - val_accuracy: 0.5647\n",
      "Epoch 614/1600\n",
      "680/680 [==============================] - 1s 832us/step - loss: 0.1707 - accuracy: 0.5962 - val_loss: 0.4331 - val_accuracy: 0.5471\n",
      "Epoch 615/1600\n",
      "680/680 [==============================] - 1s 826us/step - loss: 0.1504 - accuracy: 0.6003 - val_loss: 0.4511 - val_accuracy: 0.5588\n",
      "Epoch 616/1600\n",
      "680/680 [==============================] - 1s 838us/step - loss: 0.1491 - accuracy: 0.5996 - val_loss: 0.4290 - val_accuracy: 0.5559\n",
      "Epoch 617/1600\n",
      "680/680 [==============================] - 1s 821us/step - loss: 0.1643 - accuracy: 0.5962 - val_loss: 0.4341 - val_accuracy: 0.5647\n",
      "Epoch 618/1600\n",
      "680/680 [==============================] - 1s 846us/step - loss: 0.1515 - accuracy: 0.5985 - val_loss: 0.4701 - val_accuracy: 0.5618\n",
      "Epoch 619/1600\n",
      "680/680 [==============================] - 1s 770us/step - loss: 0.1708 - accuracy: 0.5981 - val_loss: 0.4001 - val_accuracy: 0.5588\n",
      "Epoch 620/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1563 - accuracy: 0.5977 - val_loss: 0.4358 - val_accuracy: 0.5529\n",
      "Epoch 621/1600\n",
      "680/680 [==============================] - 1s 753us/step - loss: 0.1457 - accuracy: 0.6014 - val_loss: 0.4422 - val_accuracy: 0.5529\n",
      "Epoch 622/1600\n",
      "680/680 [==============================] - 1s 739us/step - loss: 0.1402 - accuracy: 0.6025 - val_loss: 0.4386 - val_accuracy: 0.5676\n",
      "Epoch 623/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1543 - accuracy: 0.6003 - val_loss: 0.4411 - val_accuracy: 0.5618\n",
      "Epoch 624/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1557 - accuracy: 0.5985 - val_loss: 0.4319 - val_accuracy: 0.5529\n",
      "Epoch 625/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1658 - accuracy: 0.5981 - val_loss: 0.4659 - val_accuracy: 0.5618\n",
      "Epoch 626/1600\n",
      "680/680 [==============================] - 0s 734us/step - loss: 0.1729 - accuracy: 0.5951 - val_loss: 0.4758 - val_accuracy: 0.5618\n",
      "Epoch 627/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1562 - accuracy: 0.6010 - val_loss: 0.4153 - val_accuracy: 0.5618\n",
      "Epoch 628/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1573 - accuracy: 0.5977 - val_loss: 0.4541 - val_accuracy: 0.5529\n",
      "Epoch 629/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1598 - accuracy: 0.5962 - val_loss: 0.4737 - val_accuracy: 0.5500\n",
      "Epoch 630/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1513 - accuracy: 0.6003 - val_loss: 0.4522 - val_accuracy: 0.5588\n",
      "Epoch 631/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1526 - accuracy: 0.5996 - val_loss: 0.4630 - val_accuracy: 0.5559\n",
      "Epoch 632/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1615 - accuracy: 0.5959 - val_loss: 0.4735 - val_accuracy: 0.5559\n",
      "Epoch 633/1600\n",
      "680/680 [==============================] - 1s 759us/step - loss: 0.1681 - accuracy: 0.5959 - val_loss: 0.4577 - val_accuracy: 0.5618\n",
      "Epoch 634/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1499 - accuracy: 0.6018 - val_loss: 0.4703 - val_accuracy: 0.5529\n",
      "Epoch 635/1600\n",
      "680/680 [==============================] - 0s 732us/step - loss: 0.1507 - accuracy: 0.5999 - val_loss: 0.4247 - val_accuracy: 0.5618\n",
      "Epoch 636/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1423 - accuracy: 0.6025 - val_loss: 0.4556 - val_accuracy: 0.5588\n",
      "Epoch 637/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1653 - accuracy: 0.5948 - val_loss: 0.4356 - val_accuracy: 0.5588\n",
      "Epoch 638/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1597 - accuracy: 0.5948 - val_loss: 0.4366 - val_accuracy: 0.5559\n",
      "Epoch 639/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1452 - accuracy: 0.6021 - val_loss: 0.4427 - val_accuracy: 0.5559\n",
      "Epoch 640/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1603 - accuracy: 0.5988 - val_loss: 0.4482 - val_accuracy: 0.5529\n",
      "Epoch 641/1600\n",
      "680/680 [==============================] - 1s 752us/step - loss: 0.1579 - accuracy: 0.5985 - val_loss: 0.4515 - val_accuracy: 0.5471\n",
      "Epoch 642/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1504 - accuracy: 0.5996 - val_loss: 0.4511 - val_accuracy: 0.5559\n",
      "Epoch 643/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1625 - accuracy: 0.5981 - val_loss: 0.4142 - val_accuracy: 0.5618\n",
      "Epoch 644/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1404 - accuracy: 0.6036 - val_loss: 0.4583 - val_accuracy: 0.5647\n",
      "Epoch 645/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1506 - accuracy: 0.6025 - val_loss: 0.5051 - val_accuracy: 0.5529\n",
      "Epoch 646/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1532 - accuracy: 0.6018 - val_loss: 0.4429 - val_accuracy: 0.5529\n",
      "Epoch 647/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1699 - accuracy: 0.5966 - val_loss: 0.4544 - val_accuracy: 0.5441\n",
      "Epoch 648/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1499 - accuracy: 0.5992 - val_loss: 0.4542 - val_accuracy: 0.5647\n",
      "Epoch 649/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1518 - accuracy: 0.6003 - val_loss: 0.4173 - val_accuracy: 0.5559\n",
      "Epoch 650/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1523 - accuracy: 0.5988 - val_loss: 0.4706 - val_accuracy: 0.5529\n",
      "Epoch 651/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1732 - accuracy: 0.5959 - val_loss: 0.4631 - val_accuracy: 0.5559\n",
      "Epoch 652/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1444 - accuracy: 0.6032 - val_loss: 0.4710 - val_accuracy: 0.5500\n",
      "Epoch 653/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1449 - accuracy: 0.6018 - val_loss: 0.4449 - val_accuracy: 0.5529\n",
      "Epoch 654/1600\n",
      "680/680 [==============================] - 1s 804us/step - loss: 0.1540 - accuracy: 0.5988 - val_loss: 0.4433 - val_accuracy: 0.5500\n",
      "Epoch 655/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1554 - accuracy: 0.5974 - val_loss: 0.4463 - val_accuracy: 0.5588\n",
      "Epoch 656/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1517 - accuracy: 0.6003 - val_loss: 0.4222 - val_accuracy: 0.5529\n",
      "Epoch 657/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1500 - accuracy: 0.5999 - val_loss: 0.4419 - val_accuracy: 0.5588\n",
      "Epoch 658/1600\n",
      "680/680 [==============================] - 1s 782us/step - loss: 0.1445 - accuracy: 0.6018 - val_loss: 0.4257 - val_accuracy: 0.5647\n",
      "Epoch 659/1600\n",
      "680/680 [==============================] - 1s 781us/step - loss: 0.1628 - accuracy: 0.5985 - val_loss: 0.4258 - val_accuracy: 0.5559\n",
      "Epoch 660/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1451 - accuracy: 0.6036 - val_loss: 0.4444 - val_accuracy: 0.5559\n",
      "Epoch 661/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1569 - accuracy: 0.5988 - val_loss: 0.4484 - val_accuracy: 0.5588\n",
      "Epoch 662/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 791us/step - loss: 0.1663 - accuracy: 0.5977 - val_loss: 0.4801 - val_accuracy: 0.5559\n",
      "Epoch 663/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1553 - accuracy: 0.5981 - val_loss: 0.4349 - val_accuracy: 0.5618\n",
      "Epoch 664/1600\n",
      "680/680 [==============================] - 1s 778us/step - loss: 0.1453 - accuracy: 0.6025 - val_loss: 0.4419 - val_accuracy: 0.5588\n",
      "Epoch 665/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1462 - accuracy: 0.6018 - val_loss: 0.4303 - val_accuracy: 0.5529\n",
      "Epoch 666/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1517 - accuracy: 0.6014 - val_loss: 0.4318 - val_accuracy: 0.5559\n",
      "Epoch 667/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1495 - accuracy: 0.6021 - val_loss: 0.4397 - val_accuracy: 0.5500\n",
      "Epoch 668/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1746 - accuracy: 0.5929 - val_loss: 0.4552 - val_accuracy: 0.5588\n",
      "Epoch 669/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1535 - accuracy: 0.6003 - val_loss: 0.4493 - val_accuracy: 0.5559\n",
      "Epoch 670/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1552 - accuracy: 0.5992 - val_loss: 0.4301 - val_accuracy: 0.5588\n",
      "Epoch 671/1600\n",
      "680/680 [==============================] - 0s 679us/step - loss: 0.1401 - accuracy: 0.6029 - val_loss: 0.4399 - val_accuracy: 0.5588\n",
      "Epoch 672/1600\n",
      "680/680 [==============================] - 0s 704us/step - loss: 0.1471 - accuracy: 0.5999 - val_loss: 0.5073 - val_accuracy: 0.5618\n",
      "Epoch 673/1600\n",
      "680/680 [==============================] - 0s 707us/step - loss: 0.1561 - accuracy: 0.5985 - val_loss: 0.4347 - val_accuracy: 0.5559\n",
      "Epoch 674/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1468 - accuracy: 0.6003 - val_loss: 0.4472 - val_accuracy: 0.5529\n",
      "Epoch 675/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1462 - accuracy: 0.6003 - val_loss: 0.4580 - val_accuracy: 0.5500\n",
      "Epoch 676/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1623 - accuracy: 0.5962 - val_loss: 0.4558 - val_accuracy: 0.5559\n",
      "Epoch 677/1600\n",
      "680/680 [==============================] - 0s 680us/step - loss: 0.1580 - accuracy: 0.5974 - val_loss: 0.4518 - val_accuracy: 0.5618\n",
      "Epoch 678/1600\n",
      "680/680 [==============================] - 0s 698us/step - loss: 0.1528 - accuracy: 0.5985 - val_loss: 0.4303 - val_accuracy: 0.5529\n",
      "Epoch 679/1600\n",
      "680/680 [==============================] - 1s 740us/step - loss: 0.1515 - accuracy: 0.5996 - val_loss: 0.4624 - val_accuracy: 0.5588\n",
      "Epoch 680/1600\n",
      "680/680 [==============================] - 1s 738us/step - loss: 0.1386 - accuracy: 0.6032 - val_loss: 0.4941 - val_accuracy: 0.5588\n",
      "Epoch 681/1600\n",
      "680/680 [==============================] - 0s 680us/step - loss: 0.1694 - accuracy: 0.5970 - val_loss: 0.4642 - val_accuracy: 0.5618\n",
      "Epoch 682/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1598 - accuracy: 0.5992 - val_loss: 0.4399 - val_accuracy: 0.5559\n",
      "Epoch 683/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1493 - accuracy: 0.6007 - val_loss: 0.4233 - val_accuracy: 0.5500\n",
      "Epoch 684/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1449 - accuracy: 0.6032 - val_loss: 0.4394 - val_accuracy: 0.5618\n",
      "Epoch 685/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1521 - accuracy: 0.6007 - val_loss: 0.4270 - val_accuracy: 0.5529\n",
      "Epoch 686/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1565 - accuracy: 0.5992 - val_loss: 0.4181 - val_accuracy: 0.5588\n",
      "Epoch 687/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1411 - accuracy: 0.6021 - val_loss: 0.4566 - val_accuracy: 0.5559\n",
      "Epoch 688/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1689 - accuracy: 0.5985 - val_loss: 0.4370 - val_accuracy: 0.5618\n",
      "Epoch 689/1600\n",
      "680/680 [==============================] - 0s 724us/step - loss: 0.1523 - accuracy: 0.5999 - val_loss: 0.4351 - val_accuracy: 0.5588\n",
      "Epoch 690/1600\n",
      "680/680 [==============================] - 1s 739us/step - loss: 0.1547 - accuracy: 0.5996 - val_loss: 0.4517 - val_accuracy: 0.5559\n",
      "Epoch 691/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1530 - accuracy: 0.5999 - val_loss: 0.4428 - val_accuracy: 0.5559\n",
      "Epoch 692/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1610 - accuracy: 0.5970 - val_loss: 0.4718 - val_accuracy: 0.5500\n",
      "Epoch 693/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1619 - accuracy: 0.5977 - val_loss: 0.4448 - val_accuracy: 0.5588\n",
      "Epoch 694/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1489 - accuracy: 0.6007 - val_loss: 0.4576 - val_accuracy: 0.5647\n",
      "Epoch 695/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1593 - accuracy: 0.5985 - val_loss: 0.4610 - val_accuracy: 0.5529\n",
      "Epoch 696/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1459 - accuracy: 0.6010 - val_loss: 0.4447 - val_accuracy: 0.5471\n",
      "Epoch 697/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1527 - accuracy: 0.6007 - val_loss: 0.4736 - val_accuracy: 0.5529\n",
      "Epoch 698/1600\n",
      "680/680 [==============================] - 1s 739us/step - loss: 0.1613 - accuracy: 0.5970 - val_loss: 0.4642 - val_accuracy: 0.5588\n",
      "Epoch 699/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1545 - accuracy: 0.5996 - val_loss: 0.4715 - val_accuracy: 0.5618\n",
      "Epoch 700/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1413 - accuracy: 0.6040 - val_loss: 0.4568 - val_accuracy: 0.5647\n",
      "Epoch 701/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1519 - accuracy: 0.6018 - val_loss: 0.4382 - val_accuracy: 0.5471\n",
      "Epoch 702/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1686 - accuracy: 0.5962 - val_loss: 0.4136 - val_accuracy: 0.5559\n",
      "Epoch 703/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1552 - accuracy: 0.5981 - val_loss: 0.4711 - val_accuracy: 0.5559\n",
      "Epoch 704/1600\n",
      "680/680 [==============================] - 1s 740us/step - loss: 0.1508 - accuracy: 0.6010 - val_loss: 0.4783 - val_accuracy: 0.5412\n",
      "Epoch 705/1600\n",
      "680/680 [==============================] - 1s 738us/step - loss: 0.1563 - accuracy: 0.6003 - val_loss: 0.4513 - val_accuracy: 0.5588\n",
      "Epoch 706/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1490 - accuracy: 0.5999 - val_loss: 0.4271 - val_accuracy: 0.5529\n",
      "Epoch 707/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1494 - accuracy: 0.5985 - val_loss: 0.4518 - val_accuracy: 0.5588\n",
      "Epoch 708/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1640 - accuracy: 0.5966 - val_loss: 0.4595 - val_accuracy: 0.5588\n",
      "Epoch 709/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1541 - accuracy: 0.5988 - val_loss: 0.4274 - val_accuracy: 0.5500\n",
      "Epoch 710/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1397 - accuracy: 0.6025 - val_loss: 0.4390 - val_accuracy: 0.5500\n",
      "Epoch 711/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1353 - accuracy: 0.6054 - val_loss: 0.4501 - val_accuracy: 0.5588\n",
      "Epoch 712/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1569 - accuracy: 0.6003 - val_loss: 0.4768 - val_accuracy: 0.5588\n",
      "Epoch 713/1600\n",
      "680/680 [==============================] - 0s 689us/step - loss: 0.1659 - accuracy: 0.5940 - val_loss: 0.4291 - val_accuracy: 0.5647\n",
      "Epoch 714/1600\n",
      "680/680 [==============================] - 1s 752us/step - loss: 0.1443 - accuracy: 0.6003 - val_loss: 0.4579 - val_accuracy: 0.5588\n",
      "Epoch 715/1600\n",
      "680/680 [==============================] - 1s 758us/step - loss: 0.1448 - accuracy: 0.6010 - val_loss: 0.4269 - val_accuracy: 0.5618\n",
      "Epoch 716/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1593 - accuracy: 0.5974 - val_loss: 0.4399 - val_accuracy: 0.5529\n",
      "Epoch 717/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 749us/step - loss: 0.1506 - accuracy: 0.5992 - val_loss: 0.4685 - val_accuracy: 0.5588\n",
      "Epoch 718/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1561 - accuracy: 0.5992 - val_loss: 0.4790 - val_accuracy: 0.5618\n",
      "Epoch 719/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1497 - accuracy: 0.5996 - val_loss: 0.4723 - val_accuracy: 0.5559\n",
      "Epoch 720/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1583 - accuracy: 0.5992 - val_loss: 0.4198 - val_accuracy: 0.5500\n",
      "Epoch 721/1600\n",
      "680/680 [==============================] - 1s 757us/step - loss: 0.1454 - accuracy: 0.6007 - val_loss: 0.4625 - val_accuracy: 0.5559\n",
      "Epoch 722/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1505 - accuracy: 0.5992 - val_loss: 0.4721 - val_accuracy: 0.5471\n",
      "Epoch 723/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1431 - accuracy: 0.6025 - val_loss: 0.4308 - val_accuracy: 0.5559\n",
      "Epoch 724/1600\n",
      "680/680 [==============================] - 1s 799us/step - loss: 0.1574 - accuracy: 0.5970 - val_loss: 0.4476 - val_accuracy: 0.5618\n",
      "Epoch 725/1600\n",
      "680/680 [==============================] - 1s 779us/step - loss: 0.1444 - accuracy: 0.6018 - val_loss: 0.4560 - val_accuracy: 0.5618\n",
      "Epoch 726/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1612 - accuracy: 0.5970 - val_loss: 0.4330 - val_accuracy: 0.5500\n",
      "Epoch 727/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1529 - accuracy: 0.5999 - val_loss: 0.4427 - val_accuracy: 0.5500\n",
      "Epoch 728/1600\n",
      "680/680 [==============================] - 1s 830us/step - loss: 0.1497 - accuracy: 0.5992 - val_loss: 0.4496 - val_accuracy: 0.5618\n",
      "Epoch 729/1600\n",
      "680/680 [==============================] - 1s 828us/step - loss: 0.1475 - accuracy: 0.6021 - val_loss: 0.4165 - val_accuracy: 0.5529\n",
      "Epoch 730/1600\n",
      "680/680 [==============================] - 1s 834us/step - loss: 0.1519 - accuracy: 0.5992 - val_loss: 0.4361 - val_accuracy: 0.5588\n",
      "Epoch 731/1600\n",
      "680/680 [==============================] - 1s 822us/step - loss: 0.1518 - accuracy: 0.6010 - val_loss: 0.4708 - val_accuracy: 0.5559\n",
      "Epoch 732/1600\n",
      "680/680 [==============================] - 1s 830us/step - loss: 0.1497 - accuracy: 0.6014 - val_loss: 0.4167 - val_accuracy: 0.5588\n",
      "Epoch 733/1600\n",
      "680/680 [==============================] - 1s 832us/step - loss: 0.1644 - accuracy: 0.5951 - val_loss: 0.4159 - val_accuracy: 0.5529\n",
      "Epoch 734/1600\n",
      "680/680 [==============================] - 1s 827us/step - loss: 0.1478 - accuracy: 0.5999 - val_loss: 0.4538 - val_accuracy: 0.5559\n",
      "Epoch 735/1600\n",
      "680/680 [==============================] - 0s 709us/step - loss: 0.1434 - accuracy: 0.5999 - val_loss: 0.4668 - val_accuracy: 0.5588\n",
      "Epoch 736/1600\n",
      "680/680 [==============================] - 0s 705us/step - loss: 0.1536 - accuracy: 0.5985 - val_loss: 0.4482 - val_accuracy: 0.5618\n",
      "Epoch 737/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1385 - accuracy: 0.6036 - val_loss: 0.4473 - val_accuracy: 0.5618\n",
      "Epoch 738/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1567 - accuracy: 0.5988 - val_loss: 0.4337 - val_accuracy: 0.5471\n",
      "Epoch 739/1600\n",
      "680/680 [==============================] - 1s 737us/step - loss: 0.1487 - accuracy: 0.6025 - val_loss: 0.4370 - val_accuracy: 0.5588\n",
      "Epoch 740/1600\n",
      "680/680 [==============================] - 0s 730us/step - loss: 0.1471 - accuracy: 0.5992 - val_loss: 0.4103 - val_accuracy: 0.5647\n",
      "Epoch 741/1600\n",
      "680/680 [==============================] - 1s 769us/step - loss: 0.1495 - accuracy: 0.5999 - val_loss: 0.4423 - val_accuracy: 0.5471\n",
      "Epoch 742/1600\n",
      "680/680 [==============================] - 0s 708us/step - loss: 0.1682 - accuracy: 0.5974 - val_loss: 0.4589 - val_accuracy: 0.5559\n",
      "Epoch 743/1600\n",
      "680/680 [==============================] - 0s 714us/step - loss: 0.1530 - accuracy: 0.5985 - val_loss: 0.4174 - val_accuracy: 0.5588\n",
      "Epoch 744/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1569 - accuracy: 0.5974 - val_loss: 0.4675 - val_accuracy: 0.5559\n",
      "Epoch 745/1600\n",
      "680/680 [==============================] - 1s 740us/step - loss: 0.1541 - accuracy: 0.5996 - val_loss: 0.4318 - val_accuracy: 0.5588\n",
      "Epoch 746/1600\n",
      "680/680 [==============================] - 0s 735us/step - loss: 0.1488 - accuracy: 0.5999 - val_loss: 0.4099 - val_accuracy: 0.5559\n",
      "Epoch 747/1600\n",
      "680/680 [==============================] - 1s 751us/step - loss: 0.1412 - accuracy: 0.6025 - val_loss: 0.4529 - val_accuracy: 0.5588\n",
      "Epoch 748/1600\n",
      "680/680 [==============================] - 1s 782us/step - loss: 0.1402 - accuracy: 0.6014 - val_loss: 0.4830 - val_accuracy: 0.5265\n",
      "Epoch 749/1600\n",
      "680/680 [==============================] - 1s 840us/step - loss: 0.1862 - accuracy: 0.5940 - val_loss: 0.4160 - val_accuracy: 0.5618\n",
      "Epoch 750/1600\n",
      "680/680 [==============================] - 1s 823us/step - loss: 0.1509 - accuracy: 0.6003 - val_loss: 0.4445 - val_accuracy: 0.5529\n",
      "Epoch 751/1600\n",
      "680/680 [==============================] - 1s 832us/step - loss: 0.1526 - accuracy: 0.6010 - val_loss: 0.4810 - val_accuracy: 0.5559\n",
      "Epoch 752/1600\n",
      "680/680 [==============================] - 1s 826us/step - loss: 0.1400 - accuracy: 0.6032 - val_loss: 0.4287 - val_accuracy: 0.5529\n",
      "Epoch 753/1600\n",
      "680/680 [==============================] - 1s 797us/step - loss: 0.1435 - accuracy: 0.6014 - val_loss: 0.4344 - val_accuracy: 0.5647\n",
      "Epoch 754/1600\n",
      "680/680 [==============================] - 1s 782us/step - loss: 0.1635 - accuracy: 0.5996 - val_loss: 0.4840 - val_accuracy: 0.5618\n",
      "Epoch 755/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1600 - accuracy: 0.5985 - val_loss: 0.4606 - val_accuracy: 0.5588\n",
      "Epoch 756/1600\n",
      "680/680 [==============================] - 1s 782us/step - loss: 0.1525 - accuracy: 0.5985 - val_loss: 0.4245 - val_accuracy: 0.5559\n",
      "Epoch 757/1600\n",
      "680/680 [==============================] - 1s 780us/step - loss: 0.1486 - accuracy: 0.6003 - val_loss: 0.4167 - val_accuracy: 0.5529\n",
      "Epoch 758/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1411 - accuracy: 0.6003 - val_loss: 0.4306 - val_accuracy: 0.5559\n",
      "Epoch 759/1600\n",
      "680/680 [==============================] - 1s 778us/step - loss: 0.1550 - accuracy: 0.5992 - val_loss: 0.4416 - val_accuracy: 0.5618\n",
      "Epoch 760/1600\n",
      "680/680 [==============================] - 1s 781us/step - loss: 0.1550 - accuracy: 0.5996 - val_loss: 0.4404 - val_accuracy: 0.5500\n",
      "Epoch 761/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1586 - accuracy: 0.5977 - val_loss: 0.4596 - val_accuracy: 0.5500\n",
      "Epoch 762/1600\n",
      "680/680 [==============================] - 1s 782us/step - loss: 0.1374 - accuracy: 0.6043 - val_loss: 0.4512 - val_accuracy: 0.5559\n",
      "Epoch 763/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1454 - accuracy: 0.6021 - val_loss: 0.4385 - val_accuracy: 0.5559\n",
      "Epoch 764/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1385 - accuracy: 0.6010 - val_loss: 0.4436 - val_accuracy: 0.5618\n",
      "Epoch 765/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1763 - accuracy: 0.5948 - val_loss: 0.4438 - val_accuracy: 0.5559\n",
      "Epoch 766/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1575 - accuracy: 0.5985 - val_loss: 0.4571 - val_accuracy: 0.5588\n",
      "Epoch 767/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1420 - accuracy: 0.6021 - val_loss: 0.4319 - val_accuracy: 0.5529\n",
      "Epoch 768/1600\n",
      "680/680 [==============================] - 1s 737us/step - loss: 0.1366 - accuracy: 0.6040 - val_loss: 0.4682 - val_accuracy: 0.5618\n",
      "Epoch 769/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1314 - accuracy: 0.6047 - val_loss: 0.4444 - val_accuracy: 0.5529\n",
      "Epoch 770/1600\n",
      "680/680 [==============================] - 1s 740us/step - loss: 0.1555 - accuracy: 0.5977 - val_loss: 0.4562 - val_accuracy: 0.5500\n",
      "Epoch 771/1600\n",
      "680/680 [==============================] - 0s 732us/step - loss: 0.1556 - accuracy: 0.6003 - val_loss: 0.4492 - val_accuracy: 0.5559\n",
      "Epoch 772/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 745us/step - loss: 0.1471 - accuracy: 0.6010 - val_loss: 0.4313 - val_accuracy: 0.5471\n",
      "Epoch 773/1600\n",
      "680/680 [==============================] - 0s 648us/step - loss: 0.1592 - accuracy: 0.5992 - val_loss: 0.4335 - val_accuracy: 0.5529\n",
      "Epoch 774/1600\n",
      "680/680 [==============================] - 0s 664us/step - loss: 0.1381 - accuracy: 0.6032 - val_loss: 0.4693 - val_accuracy: 0.5500\n",
      "Epoch 775/1600\n",
      "680/680 [==============================] - 0s 657us/step - loss: 0.1584 - accuracy: 0.5996 - val_loss: 0.4705 - val_accuracy: 0.5206\n",
      "Epoch 776/1600\n",
      "680/680 [==============================] - 0s 650us/step - loss: 0.1596 - accuracy: 0.5955 - val_loss: 0.4509 - val_accuracy: 0.5588\n",
      "Epoch 777/1600\n",
      "680/680 [==============================] - 0s 696us/step - loss: 0.1539 - accuracy: 0.5981 - val_loss: 0.4245 - val_accuracy: 0.5500\n",
      "Epoch 778/1600\n",
      "680/680 [==============================] - 0s 676us/step - loss: 0.1388 - accuracy: 0.6010 - val_loss: 0.4475 - val_accuracy: 0.5471\n",
      "Epoch 779/1600\n",
      "680/680 [==============================] - 0s 716us/step - loss: 0.1370 - accuracy: 0.6036 - val_loss: 0.4242 - val_accuracy: 0.5559\n",
      "Epoch 780/1600\n",
      "680/680 [==============================] - 0s 707us/step - loss: 0.1550 - accuracy: 0.6003 - val_loss: 0.4530 - val_accuracy: 0.5588\n",
      "Epoch 781/1600\n",
      "680/680 [==============================] - 1s 755us/step - loss: 0.1503 - accuracy: 0.5985 - val_loss: 0.4200 - val_accuracy: 0.5559\n",
      "Epoch 782/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1519 - accuracy: 0.6003 - val_loss: 0.4254 - val_accuracy: 0.5500\n",
      "Epoch 783/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1612 - accuracy: 0.5974 - val_loss: 0.4465 - val_accuracy: 0.5559\n",
      "Epoch 784/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1447 - accuracy: 0.6018 - val_loss: 0.4581 - val_accuracy: 0.5647\n",
      "Epoch 785/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1504 - accuracy: 0.6010 - val_loss: 0.4350 - val_accuracy: 0.5618\n",
      "Epoch 786/1600\n",
      "680/680 [==============================] - 0s 732us/step - loss: 0.1472 - accuracy: 0.5999 - val_loss: 0.4500 - val_accuracy: 0.5618\n",
      "Epoch 787/1600\n",
      "680/680 [==============================] - 0s 713us/step - loss: 0.1484 - accuracy: 0.6003 - val_loss: 0.4449 - val_accuracy: 0.5559\n",
      "Epoch 788/1600\n",
      "680/680 [==============================] - 0s 701us/step - loss: 0.1529 - accuracy: 0.5974 - val_loss: 0.4565 - val_accuracy: 0.5559\n",
      "Epoch 789/1600\n",
      "680/680 [==============================] - 0s 710us/step - loss: 0.1564 - accuracy: 0.5974 - val_loss: 0.4639 - val_accuracy: 0.5529\n",
      "Epoch 790/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1667 - accuracy: 0.5981 - val_loss: 0.4617 - val_accuracy: 0.5529\n",
      "Epoch 791/1600\n",
      "680/680 [==============================] - 1s 736us/step - loss: 0.1472 - accuracy: 0.6010 - val_loss: 0.4460 - val_accuracy: 0.5618\n",
      "Epoch 792/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1599 - accuracy: 0.5981 - val_loss: 0.4333 - val_accuracy: 0.5529\n",
      "Epoch 793/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1536 - accuracy: 0.5981 - val_loss: 0.4271 - val_accuracy: 0.5559\n",
      "Epoch 794/1600\n",
      "680/680 [==============================] - 0s 679us/step - loss: 0.1425 - accuracy: 0.6040 - val_loss: 0.4629 - val_accuracy: 0.5471\n",
      "Epoch 795/1600\n",
      "680/680 [==============================] - 0s 714us/step - loss: 0.1635 - accuracy: 0.5974 - val_loss: 0.4526 - val_accuracy: 0.5559\n",
      "Epoch 796/1600\n",
      "680/680 [==============================] - 0s 711us/step - loss: 0.1447 - accuracy: 0.6032 - val_loss: 0.4404 - val_accuracy: 0.5412\n",
      "Epoch 797/1600\n",
      "680/680 [==============================] - 0s 733us/step - loss: 0.1567 - accuracy: 0.6003 - val_loss: 0.4241 - val_accuracy: 0.5471\n",
      "Epoch 798/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1467 - accuracy: 0.5974 - val_loss: 0.4439 - val_accuracy: 0.5588\n",
      "Epoch 799/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1465 - accuracy: 0.6018 - val_loss: 0.5151 - val_accuracy: 0.5382\n",
      "Epoch 800/1600\n",
      "680/680 [==============================] - 1s 740us/step - loss: 0.1544 - accuracy: 0.5985 - val_loss: 0.4174 - val_accuracy: 0.5529\n",
      "Epoch 801/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1666 - accuracy: 0.5948 - val_loss: 0.4207 - val_accuracy: 0.5500\n",
      "Epoch 802/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1412 - accuracy: 0.6040 - val_loss: 0.4485 - val_accuracy: 0.5559\n",
      "Epoch 803/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1456 - accuracy: 0.6021 - val_loss: 0.4651 - val_accuracy: 0.5559\n",
      "Epoch 804/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1542 - accuracy: 0.5992 - val_loss: 0.4352 - val_accuracy: 0.5647\n",
      "Epoch 805/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1459 - accuracy: 0.6032 - val_loss: 0.4086 - val_accuracy: 0.5500\n",
      "Epoch 806/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1630 - accuracy: 0.5966 - val_loss: 0.4428 - val_accuracy: 0.5588\n",
      "Epoch 807/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1476 - accuracy: 0.6010 - val_loss: 0.4737 - val_accuracy: 0.5559\n",
      "Epoch 808/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1598 - accuracy: 0.5996 - val_loss: 0.4027 - val_accuracy: 0.5500\n",
      "Epoch 809/1600\n",
      "680/680 [==============================] - 1s 738us/step - loss: 0.1452 - accuracy: 0.6010 - val_loss: 0.4263 - val_accuracy: 0.5529\n",
      "Epoch 810/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1440 - accuracy: 0.6010 - val_loss: 0.4382 - val_accuracy: 0.5588\n",
      "Epoch 811/1600\n",
      "680/680 [==============================] - 1s 739us/step - loss: 0.1592 - accuracy: 0.6007 - val_loss: 0.4463 - val_accuracy: 0.5647\n",
      "Epoch 812/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1576 - accuracy: 0.5966 - val_loss: 0.4569 - val_accuracy: 0.5559\n",
      "Epoch 813/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1394 - accuracy: 0.6021 - val_loss: 0.4384 - val_accuracy: 0.5529\n",
      "Epoch 814/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1624 - accuracy: 0.5981 - val_loss: 0.4354 - val_accuracy: 0.5500\n",
      "Epoch 815/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1568 - accuracy: 0.5981 - val_loss: 0.4678 - val_accuracy: 0.5529\n",
      "Epoch 816/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1491 - accuracy: 0.6018 - val_loss: 0.4638 - val_accuracy: 0.5647\n",
      "Epoch 817/1600\n",
      "680/680 [==============================] - 0s 670us/step - loss: 0.1538 - accuracy: 0.5981 - val_loss: 0.4311 - val_accuracy: 0.5559\n",
      "Epoch 818/1600\n",
      "680/680 [==============================] - 0s 696us/step - loss: 0.1403 - accuracy: 0.6032 - val_loss: 0.4517 - val_accuracy: 0.5559\n",
      "Epoch 819/1600\n",
      "680/680 [==============================] - 0s 710us/step - loss: 0.1470 - accuracy: 0.6003 - val_loss: 0.4876 - val_accuracy: 0.5441\n",
      "Epoch 820/1600\n",
      "680/680 [==============================] - 0s 711us/step - loss: 0.1538 - accuracy: 0.6010 - val_loss: 0.4804 - val_accuracy: 0.5588\n",
      "Epoch 821/1600\n",
      "680/680 [==============================] - 0s 713us/step - loss: 0.1542 - accuracy: 0.6010 - val_loss: 0.4481 - val_accuracy: 0.5559\n",
      "Epoch 822/1600\n",
      "680/680 [==============================] - 0s 712us/step - loss: 0.1388 - accuracy: 0.6021 - val_loss: 0.4339 - val_accuracy: 0.5529\n",
      "Epoch 823/1600\n",
      "680/680 [==============================] - 0s 710us/step - loss: 0.1484 - accuracy: 0.6007 - val_loss: 0.4483 - val_accuracy: 0.5529\n",
      "Epoch 824/1600\n",
      "680/680 [==============================] - 0s 712us/step - loss: 0.1488 - accuracy: 0.5992 - val_loss: 0.4577 - val_accuracy: 0.5529\n",
      "Epoch 825/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1461 - accuracy: 0.6007 - val_loss: 0.4229 - val_accuracy: 0.5559\n",
      "Epoch 826/1600\n",
      "680/680 [==============================] - 1s 756us/step - loss: 0.1523 - accuracy: 0.5996 - val_loss: 0.4816 - val_accuracy: 0.5588\n",
      "Epoch 827/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 764us/step - loss: 0.1611 - accuracy: 0.5999 - val_loss: 0.4442 - val_accuracy: 0.5588\n",
      "Epoch 828/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1546 - accuracy: 0.5981 - val_loss: 0.4431 - val_accuracy: 0.5588\n",
      "Epoch 829/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1450 - accuracy: 0.6021 - val_loss: 0.4512 - val_accuracy: 0.5500\n",
      "Epoch 830/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1551 - accuracy: 0.5999 - val_loss: 0.4627 - val_accuracy: 0.5588\n",
      "Epoch 831/1600\n",
      "680/680 [==============================] - 1s 736us/step - loss: 0.1580 - accuracy: 0.5992 - val_loss: 0.4383 - val_accuracy: 0.5588\n",
      "Epoch 832/1600\n",
      "680/680 [==============================] - 1s 735us/step - loss: 0.1535 - accuracy: 0.5996 - val_loss: 0.5004 - val_accuracy: 0.5706\n",
      "Epoch 833/1600\n",
      "680/680 [==============================] - 0s 671us/step - loss: 0.1564 - accuracy: 0.6010 - val_loss: 0.4291 - val_accuracy: 0.5588\n",
      "Epoch 834/1600\n",
      "680/680 [==============================] - 0s 718us/step - loss: 0.1461 - accuracy: 0.6007 - val_loss: 0.4554 - val_accuracy: 0.5500\n",
      "Epoch 835/1600\n",
      "680/680 [==============================] - 0s 712us/step - loss: 0.1489 - accuracy: 0.6014 - val_loss: 0.4182 - val_accuracy: 0.5559\n",
      "Epoch 836/1600\n",
      "680/680 [==============================] - 0s 715us/step - loss: 0.1600 - accuracy: 0.5981 - val_loss: 0.4343 - val_accuracy: 0.5588\n",
      "Epoch 837/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1382 - accuracy: 0.6036 - val_loss: 0.4691 - val_accuracy: 0.5588\n",
      "Epoch 838/1600\n",
      "680/680 [==============================] - 1s 752us/step - loss: 0.1432 - accuracy: 0.6040 - val_loss: 0.4644 - val_accuracy: 0.5588\n",
      "Epoch 839/1600\n",
      "680/680 [==============================] - 0s 717us/step - loss: 0.1512 - accuracy: 0.5999 - val_loss: 0.4283 - val_accuracy: 0.5618\n",
      "Epoch 840/1600\n",
      "680/680 [==============================] - 0s 714us/step - loss: 0.1450 - accuracy: 0.6014 - val_loss: 0.4933 - val_accuracy: 0.5588\n",
      "Epoch 841/1600\n",
      "680/680 [==============================] - 0s 714us/step - loss: 0.1531 - accuracy: 0.5996 - val_loss: 0.4205 - val_accuracy: 0.5529\n",
      "Epoch 842/1600\n",
      "680/680 [==============================] - 0s 713us/step - loss: 0.1519 - accuracy: 0.6003 - val_loss: 0.4477 - val_accuracy: 0.5647\n",
      "Epoch 843/1600\n",
      "680/680 [==============================] - 0s 711us/step - loss: 0.1524 - accuracy: 0.5999 - val_loss: 0.4148 - val_accuracy: 0.5500\n",
      "Epoch 844/1600\n",
      "680/680 [==============================] - 0s 711us/step - loss: 0.1534 - accuracy: 0.5988 - val_loss: 0.4094 - val_accuracy: 0.5588\n",
      "Epoch 845/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1475 - accuracy: 0.6014 - val_loss: 0.4489 - val_accuracy: 0.5706\n",
      "Epoch 846/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1551 - accuracy: 0.5992 - val_loss: 0.4471 - val_accuracy: 0.5471\n",
      "Epoch 847/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1374 - accuracy: 0.6036 - val_loss: 0.4267 - val_accuracy: 0.5529\n",
      "Epoch 848/1600\n",
      "680/680 [==============================] - 0s 734us/step - loss: 0.1646 - accuracy: 0.5974 - val_loss: 0.4656 - val_accuracy: 0.5412\n",
      "Epoch 849/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1556 - accuracy: 0.5988 - val_loss: 0.4185 - val_accuracy: 0.5559\n",
      "Epoch 850/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1537 - accuracy: 0.5992 - val_loss: 0.4612 - val_accuracy: 0.5529\n",
      "Epoch 851/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1464 - accuracy: 0.6018 - val_loss: 0.4303 - val_accuracy: 0.5647\n",
      "Epoch 852/1600\n",
      "680/680 [==============================] - 1s 763us/step - loss: 0.1613 - accuracy: 0.5977 - val_loss: 0.4290 - val_accuracy: 0.5559\n",
      "Epoch 853/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1443 - accuracy: 0.6018 - val_loss: 0.4587 - val_accuracy: 0.5588\n",
      "Epoch 854/1600\n",
      "680/680 [==============================] - 0s 689us/step - loss: 0.1430 - accuracy: 0.6014 - val_loss: 0.4664 - val_accuracy: 0.5618\n",
      "Epoch 855/1600\n",
      "680/680 [==============================] - 0s 716us/step - loss: 0.1440 - accuracy: 0.6010 - val_loss: 0.4503 - val_accuracy: 0.5647\n",
      "Epoch 856/1600\n",
      "680/680 [==============================] - 0s 713us/step - loss: 0.1569 - accuracy: 0.5992 - val_loss: 0.4260 - val_accuracy: 0.5559\n",
      "Epoch 857/1600\n",
      "680/680 [==============================] - 0s 712us/step - loss: 0.1607 - accuracy: 0.5977 - val_loss: 0.4575 - val_accuracy: 0.5500\n",
      "Epoch 858/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1511 - accuracy: 0.5988 - val_loss: 0.4526 - val_accuracy: 0.5559\n",
      "Epoch 859/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1429 - accuracy: 0.6014 - val_loss: 0.4660 - val_accuracy: 0.5529\n",
      "Epoch 860/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1536 - accuracy: 0.6007 - val_loss: 0.4052 - val_accuracy: 0.5529\n",
      "Epoch 861/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1603 - accuracy: 0.5977 - val_loss: 0.4398 - val_accuracy: 0.5529\n",
      "Epoch 862/1600\n",
      "680/680 [==============================] - 1s 799us/step - loss: 0.1395 - accuracy: 0.6043 - val_loss: 0.4346 - val_accuracy: 0.5471\n",
      "Epoch 863/1600\n",
      "680/680 [==============================] - 1s 822us/step - loss: 0.1492 - accuracy: 0.5985 - val_loss: 0.4809 - val_accuracy: 0.5618\n",
      "Epoch 864/1600\n",
      "680/680 [==============================] - 1s 830us/step - loss: 0.1362 - accuracy: 0.6029 - val_loss: 0.4404 - val_accuracy: 0.5588\n",
      "Epoch 865/1600\n",
      "680/680 [==============================] - 1s 808us/step - loss: 0.1686 - accuracy: 0.5959 - val_loss: 0.4445 - val_accuracy: 0.5500\n",
      "Epoch 866/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1429 - accuracy: 0.6007 - val_loss: 0.4424 - val_accuracy: 0.5471\n",
      "Epoch 867/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1675 - accuracy: 0.5940 - val_loss: 0.4909 - val_accuracy: 0.5559\n",
      "Epoch 868/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1515 - accuracy: 0.6003 - val_loss: 0.4993 - val_accuracy: 0.5559\n",
      "Epoch 869/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1465 - accuracy: 0.5996 - val_loss: 0.4577 - val_accuracy: 0.5618\n",
      "Epoch 870/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1531 - accuracy: 0.6007 - val_loss: 0.4486 - val_accuracy: 0.5441\n",
      "Epoch 871/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1557 - accuracy: 0.5999 - val_loss: 0.4825 - val_accuracy: 0.5588\n",
      "Epoch 872/1600\n",
      "680/680 [==============================] - 1s 827us/step - loss: 0.1502 - accuracy: 0.6010 - val_loss: 0.4598 - val_accuracy: 0.5529\n",
      "Epoch 873/1600\n",
      "680/680 [==============================] - 1s 832us/step - loss: 0.1518 - accuracy: 0.6010 - val_loss: 0.4712 - val_accuracy: 0.5588\n",
      "Epoch 874/1600\n",
      "680/680 [==============================] - 1s 833us/step - loss: 0.1485 - accuracy: 0.5992 - val_loss: 0.4995 - val_accuracy: 0.5559\n",
      "Epoch 875/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1497 - accuracy: 0.6010 - val_loss: 0.4348 - val_accuracy: 0.5500\n",
      "Epoch 876/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1602 - accuracy: 0.5999 - val_loss: 0.4467 - val_accuracy: 0.5618\n",
      "Epoch 877/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1559 - accuracy: 0.5966 - val_loss: 0.4938 - val_accuracy: 0.5588\n",
      "Epoch 878/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1530 - accuracy: 0.6003 - val_loss: 0.3905 - val_accuracy: 0.5618\n",
      "Epoch 879/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1435 - accuracy: 0.6025 - val_loss: 0.4375 - val_accuracy: 0.5559\n",
      "Epoch 880/1600\n",
      "680/680 [==============================] - 1s 824us/step - loss: 0.1472 - accuracy: 0.5992 - val_loss: 0.4444 - val_accuracy: 0.5588\n",
      "Epoch 881/1600\n",
      "680/680 [==============================] - 1s 829us/step - loss: 0.1416 - accuracy: 0.6040 - val_loss: 0.4512 - val_accuracy: 0.5618\n",
      "Epoch 882/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 790us/step - loss: 0.1742 - accuracy: 0.5977 - val_loss: 0.4325 - val_accuracy: 0.5618\n",
      "Epoch 883/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1601 - accuracy: 0.5996 - val_loss: 0.4614 - val_accuracy: 0.5471\n",
      "Epoch 884/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1429 - accuracy: 0.6018 - val_loss: 0.4152 - val_accuracy: 0.5559\n",
      "Epoch 885/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1535 - accuracy: 0.6003 - val_loss: 0.4852 - val_accuracy: 0.5618\n",
      "Epoch 886/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1560 - accuracy: 0.6003 - val_loss: 0.4540 - val_accuracy: 0.5529\n",
      "Epoch 887/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1589 - accuracy: 0.5974 - val_loss: 0.4540 - val_accuracy: 0.5588\n",
      "Epoch 888/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1355 - accuracy: 0.6043 - val_loss: 0.4571 - val_accuracy: 0.5588\n",
      "Epoch 889/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1482 - accuracy: 0.6018 - val_loss: 0.4615 - val_accuracy: 0.5500\n",
      "Epoch 890/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1589 - accuracy: 0.5988 - val_loss: 0.4501 - val_accuracy: 0.5500\n",
      "Epoch 891/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1489 - accuracy: 0.6010 - val_loss: 0.4494 - val_accuracy: 0.5559\n",
      "Epoch 892/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1542 - accuracy: 0.6003 - val_loss: 0.4560 - val_accuracy: 0.5559\n",
      "Epoch 893/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1484 - accuracy: 0.6018 - val_loss: 0.4573 - val_accuracy: 0.5500\n",
      "Epoch 894/1600\n",
      "680/680 [==============================] - 1s 782us/step - loss: 0.1553 - accuracy: 0.5988 - val_loss: 0.4540 - val_accuracy: 0.5618\n",
      "Epoch 895/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1528 - accuracy: 0.6003 - val_loss: 0.4251 - val_accuracy: 0.5500\n",
      "Epoch 896/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1499 - accuracy: 0.5996 - val_loss: 0.4818 - val_accuracy: 0.5559\n",
      "Epoch 897/1600\n",
      "680/680 [==============================] - 1s 834us/step - loss: 0.1538 - accuracy: 0.5988 - val_loss: 0.5053 - val_accuracy: 0.5588\n",
      "Epoch 898/1600\n",
      "680/680 [==============================] - 1s 828us/step - loss: 0.1700 - accuracy: 0.5992 - val_loss: 0.4281 - val_accuracy: 0.5559\n",
      "Epoch 899/1600\n",
      "680/680 [==============================] - 1s 806us/step - loss: 0.1451 - accuracy: 0.6010 - val_loss: 0.4592 - val_accuracy: 0.5588\n",
      "Epoch 900/1600\n",
      "680/680 [==============================] - 1s 782us/step - loss: 0.1474 - accuracy: 0.6021 - val_loss: 0.4784 - val_accuracy: 0.5559\n",
      "Epoch 901/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1605 - accuracy: 0.5985 - val_loss: 0.4587 - val_accuracy: 0.5471\n",
      "Epoch 902/1600\n",
      "680/680 [==============================] - 1s 832us/step - loss: 0.1590 - accuracy: 0.5977 - val_loss: 0.4490 - val_accuracy: 0.5412\n",
      "Epoch 903/1600\n",
      "680/680 [==============================] - 1s 824us/step - loss: 0.1466 - accuracy: 0.5988 - val_loss: 0.4386 - val_accuracy: 0.5588\n",
      "Epoch 904/1600\n",
      "680/680 [==============================] - 1s 837us/step - loss: 0.1427 - accuracy: 0.6014 - val_loss: 0.4312 - val_accuracy: 0.5559\n",
      "Epoch 905/1600\n",
      "680/680 [==============================] - 1s 836us/step - loss: 0.1413 - accuracy: 0.6021 - val_loss: 0.4396 - val_accuracy: 0.5500\n",
      "Epoch 906/1600\n",
      "680/680 [==============================] - 1s 828us/step - loss: 0.1634 - accuracy: 0.5974 - val_loss: 0.4215 - val_accuracy: 0.5529\n",
      "Epoch 907/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1511 - accuracy: 0.5992 - val_loss: 0.4567 - val_accuracy: 0.5588\n",
      "Epoch 908/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1493 - accuracy: 0.6010 - val_loss: 0.4370 - val_accuracy: 0.5618\n",
      "Epoch 909/1600\n",
      "680/680 [==============================] - 1s 777us/step - loss: 0.1378 - accuracy: 0.6040 - val_loss: 0.4502 - val_accuracy: 0.5559\n",
      "Epoch 910/1600\n",
      "680/680 [==============================] - 0s 696us/step - loss: 0.1478 - accuracy: 0.6021 - val_loss: 0.4592 - val_accuracy: 0.5529\n",
      "Epoch 911/1600\n",
      "680/680 [==============================] - 0s 712us/step - loss: 0.1343 - accuracy: 0.6051 - val_loss: 0.4472 - val_accuracy: 0.5500\n",
      "Epoch 912/1600\n",
      "680/680 [==============================] - 1s 759us/step - loss: 0.1521 - accuracy: 0.5992 - val_loss: 0.4547 - val_accuracy: 0.5529\n",
      "Epoch 913/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1660 - accuracy: 0.5937 - val_loss: 0.4695 - val_accuracy: 0.5588\n",
      "Epoch 914/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1510 - accuracy: 0.6007 - val_loss: 0.4254 - val_accuracy: 0.5500\n",
      "Epoch 915/1600\n",
      "680/680 [==============================] - 1s 773us/step - loss: 0.1453 - accuracy: 0.6014 - val_loss: 0.4325 - val_accuracy: 0.5559\n",
      "Epoch 916/1600\n",
      "680/680 [==============================] - 1s 756us/step - loss: 0.1536 - accuracy: 0.5974 - val_loss: 0.4631 - val_accuracy: 0.5676\n",
      "Epoch 917/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1508 - accuracy: 0.6003 - val_loss: 0.4108 - val_accuracy: 0.5588\n",
      "Epoch 918/1600\n",
      "680/680 [==============================] - 1s 756us/step - loss: 0.1438 - accuracy: 0.6007 - val_loss: 0.4165 - val_accuracy: 0.5647\n",
      "Epoch 919/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1448 - accuracy: 0.6010 - val_loss: 0.4561 - val_accuracy: 0.5529\n",
      "Epoch 920/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1552 - accuracy: 0.5985 - val_loss: 0.4523 - val_accuracy: 0.5500\n",
      "Epoch 921/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1595 - accuracy: 0.5981 - val_loss: 0.4166 - val_accuracy: 0.5618\n",
      "Epoch 922/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1505 - accuracy: 0.5999 - val_loss: 0.4707 - val_accuracy: 0.5471\n",
      "Epoch 923/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1524 - accuracy: 0.5992 - val_loss: 0.4548 - val_accuracy: 0.5500\n",
      "Epoch 924/1600\n",
      "680/680 [==============================] - 1s 804us/step - loss: 0.1692 - accuracy: 0.5962 - val_loss: 0.4423 - val_accuracy: 0.5559\n",
      "Epoch 925/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1561 - accuracy: 0.5951 - val_loss: 0.4563 - val_accuracy: 0.5647\n",
      "Epoch 926/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1631 - accuracy: 0.5966 - val_loss: 0.4577 - val_accuracy: 0.5529\n",
      "Epoch 927/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1462 - accuracy: 0.5999 - val_loss: 0.4349 - val_accuracy: 0.5588\n",
      "Epoch 928/1600\n",
      "680/680 [==============================] - 1s 833us/step - loss: 0.1547 - accuracy: 0.5959 - val_loss: 0.4191 - val_accuracy: 0.5529\n",
      "Epoch 929/1600\n",
      "680/680 [==============================] - 1s 832us/step - loss: 0.1557 - accuracy: 0.5992 - val_loss: 0.4365 - val_accuracy: 0.5559\n",
      "Epoch 930/1600\n",
      "680/680 [==============================] - 1s 837us/step - loss: 0.1516 - accuracy: 0.6003 - val_loss: 0.4570 - val_accuracy: 0.5500\n",
      "Epoch 931/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1514 - accuracy: 0.5981 - val_loss: 0.5145 - val_accuracy: 0.5618\n",
      "Epoch 932/1600\n",
      "680/680 [==============================] - 1s 779us/step - loss: 0.1518 - accuracy: 0.5996 - val_loss: 0.4994 - val_accuracy: 0.5559\n",
      "Epoch 933/1600\n",
      "680/680 [==============================] - 1s 782us/step - loss: 0.1542 - accuracy: 0.5988 - val_loss: 0.4679 - val_accuracy: 0.5588\n",
      "Epoch 934/1600\n",
      "680/680 [==============================] - 1s 781us/step - loss: 0.1498 - accuracy: 0.6007 - val_loss: 0.4901 - val_accuracy: 0.5471\n",
      "Epoch 935/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1520 - accuracy: 0.6007 - val_loss: 0.4550 - val_accuracy: 0.5588\n",
      "Epoch 936/1600\n",
      "680/680 [==============================] - 1s 780us/step - loss: 0.1535 - accuracy: 0.5996 - val_loss: 0.4695 - val_accuracy: 0.5618\n",
      "Epoch 937/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 783us/step - loss: 0.1539 - accuracy: 0.6021 - val_loss: 0.4725 - val_accuracy: 0.5618\n",
      "Epoch 938/1600\n",
      "680/680 [==============================] - 1s 824us/step - loss: 0.1577 - accuracy: 0.5985 - val_loss: 0.4515 - val_accuracy: 0.5588\n",
      "Epoch 939/1600\n",
      "680/680 [==============================] - 1s 803us/step - loss: 0.1532 - accuracy: 0.5996 - val_loss: 0.4232 - val_accuracy: 0.5676\n",
      "Epoch 940/1600\n",
      "680/680 [==============================] - 1s 834us/step - loss: 0.1568 - accuracy: 0.5996 - val_loss: 0.4929 - val_accuracy: 0.5588\n",
      "Epoch 941/1600\n",
      "680/680 [==============================] - 1s 830us/step - loss: 0.1612 - accuracy: 0.5970 - val_loss: 0.5333 - val_accuracy: 0.5471\n",
      "Epoch 942/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1588 - accuracy: 0.5988 - val_loss: 0.4397 - val_accuracy: 0.5529\n",
      "Epoch 943/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1561 - accuracy: 0.6007 - val_loss: 0.4739 - val_accuracy: 0.5529\n",
      "Epoch 944/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1391 - accuracy: 0.6051 - val_loss: 0.4467 - val_accuracy: 0.5588\n",
      "Epoch 945/1600\n",
      "680/680 [==============================] - 1s 837us/step - loss: 0.1727 - accuracy: 0.5970 - val_loss: 0.4290 - val_accuracy: 0.5529\n",
      "Epoch 946/1600\n",
      "680/680 [==============================] - 1s 825us/step - loss: 0.1626 - accuracy: 0.5966 - val_loss: 0.4587 - val_accuracy: 0.5588\n",
      "Epoch 947/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1508 - accuracy: 0.6014 - val_loss: 0.4375 - val_accuracy: 0.5441\n",
      "Epoch 948/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1357 - accuracy: 0.6043 - val_loss: 0.4721 - val_accuracy: 0.5441\n",
      "Epoch 949/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1621 - accuracy: 0.5992 - val_loss: 0.4353 - val_accuracy: 0.5647\n",
      "Epoch 950/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1417 - accuracy: 0.6036 - val_loss: 0.4366 - val_accuracy: 0.5441\n",
      "Epoch 951/1600\n",
      "680/680 [==============================] - 1s 738us/step - loss: 0.1452 - accuracy: 0.5996 - val_loss: 0.4527 - val_accuracy: 0.5500\n",
      "Epoch 952/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1610 - accuracy: 0.5970 - val_loss: 0.4747 - val_accuracy: 0.5529\n",
      "Epoch 953/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1622 - accuracy: 0.5974 - val_loss: 0.4447 - val_accuracy: 0.5559\n",
      "Epoch 954/1600\n",
      "680/680 [==============================] - 1s 740us/step - loss: 0.1471 - accuracy: 0.6014 - val_loss: 0.4983 - val_accuracy: 0.5500\n",
      "Epoch 955/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1432 - accuracy: 0.6021 - val_loss: 0.4580 - val_accuracy: 0.5529\n",
      "Epoch 956/1600\n",
      "680/680 [==============================] - 1s 740us/step - loss: 0.1385 - accuracy: 0.6014 - val_loss: 0.4655 - val_accuracy: 0.5588\n",
      "Epoch 957/1600\n",
      "680/680 [==============================] - 1s 800us/step - loss: 0.1520 - accuracy: 0.6003 - val_loss: 0.4285 - val_accuracy: 0.5588\n",
      "Epoch 958/1600\n",
      "680/680 [==============================] - 1s 782us/step - loss: 0.1605 - accuracy: 0.5974 - val_loss: 0.4356 - val_accuracy: 0.5588\n",
      "Epoch 959/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1464 - accuracy: 0.5999 - val_loss: 0.4605 - val_accuracy: 0.5647\n",
      "Epoch 960/1600\n",
      "680/680 [==============================] - 1s 831us/step - loss: 0.1356 - accuracy: 0.6051 - val_loss: 0.4509 - val_accuracy: 0.5588\n",
      "Epoch 961/1600\n",
      "680/680 [==============================] - 1s 830us/step - loss: 0.1370 - accuracy: 0.6029 - val_loss: 0.4577 - val_accuracy: 0.5588\n",
      "Epoch 962/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1624 - accuracy: 0.5985 - val_loss: 0.4505 - val_accuracy: 0.5588\n",
      "Epoch 963/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1664 - accuracy: 0.5985 - val_loss: 0.4372 - val_accuracy: 0.5618\n",
      "Epoch 964/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1404 - accuracy: 0.6025 - val_loss: 0.4275 - val_accuracy: 0.5618\n",
      "Epoch 965/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1434 - accuracy: 0.6014 - val_loss: 0.4536 - val_accuracy: 0.5676\n",
      "Epoch 966/1600\n",
      "680/680 [==============================] - 0s 703us/step - loss: 0.1475 - accuracy: 0.6021 - val_loss: 0.4702 - val_accuracy: 0.5529\n",
      "Epoch 967/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1642 - accuracy: 0.5966 - val_loss: 0.4560 - val_accuracy: 0.5618\n",
      "Epoch 968/1600\n",
      "680/680 [==============================] - 1s 740us/step - loss: 0.1513 - accuracy: 0.6018 - val_loss: 0.4263 - val_accuracy: 0.5559\n",
      "Epoch 969/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1470 - accuracy: 0.6014 - val_loss: 0.4557 - val_accuracy: 0.5500\n",
      "Epoch 970/1600\n",
      "680/680 [==============================] - 1s 738us/step - loss: 0.1505 - accuracy: 0.6003 - val_loss: 0.4827 - val_accuracy: 0.5559\n",
      "Epoch 971/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1436 - accuracy: 0.6025 - val_loss: 0.4263 - val_accuracy: 0.5588\n",
      "Epoch 972/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1487 - accuracy: 0.6010 - val_loss: 0.4459 - val_accuracy: 0.5529\n",
      "Epoch 973/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1548 - accuracy: 0.5999 - val_loss: 0.4485 - val_accuracy: 0.5559\n",
      "Epoch 974/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1427 - accuracy: 0.6029 - val_loss: 0.4578 - val_accuracy: 0.5618\n",
      "Epoch 975/1600\n",
      "680/680 [==============================] - 1s 832us/step - loss: 0.1397 - accuracy: 0.6003 - val_loss: 0.4965 - val_accuracy: 0.5676\n",
      "Epoch 976/1600\n",
      "680/680 [==============================] - 1s 833us/step - loss: 0.1540 - accuracy: 0.5985 - val_loss: 0.4820 - val_accuracy: 0.5529\n",
      "Epoch 977/1600\n",
      "680/680 [==============================] - 1s 779us/step - loss: 0.1521 - accuracy: 0.5992 - val_loss: 0.4341 - val_accuracy: 0.5529\n",
      "Epoch 978/1600\n",
      "680/680 [==============================] - 0s 698us/step - loss: 0.1393 - accuracy: 0.6025 - val_loss: 0.4353 - val_accuracy: 0.5471\n",
      "Epoch 979/1600\n",
      "680/680 [==============================] - 0s 720us/step - loss: 0.1598 - accuracy: 0.5988 - val_loss: 0.4244 - val_accuracy: 0.5618\n",
      "Epoch 980/1600\n",
      "680/680 [==============================] - 1s 753us/step - loss: 0.1453 - accuracy: 0.6007 - val_loss: 0.4596 - val_accuracy: 0.5588\n",
      "Epoch 981/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1640 - accuracy: 0.5962 - val_loss: 0.4851 - val_accuracy: 0.5471\n",
      "Epoch 982/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1422 - accuracy: 0.6032 - val_loss: 0.4108 - val_accuracy: 0.5529\n",
      "Epoch 983/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1495 - accuracy: 0.6003 - val_loss: 0.4321 - val_accuracy: 0.5529\n",
      "Epoch 984/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1570 - accuracy: 0.5977 - val_loss: 0.4799 - val_accuracy: 0.5441\n",
      "Epoch 985/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1434 - accuracy: 0.6014 - val_loss: 0.4819 - val_accuracy: 0.5529\n",
      "Epoch 986/1600\n",
      "680/680 [==============================] - 1s 801us/step - loss: 0.1413 - accuracy: 0.6036 - val_loss: 0.4894 - val_accuracy: 0.5529\n",
      "Epoch 987/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1516 - accuracy: 0.5974 - val_loss: 0.4943 - val_accuracy: 0.5588\n",
      "Epoch 988/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1580 - accuracy: 0.5970 - val_loss: 0.4618 - val_accuracy: 0.5500\n",
      "Epoch 989/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1623 - accuracy: 0.5981 - val_loss: 0.4315 - val_accuracy: 0.5529\n",
      "Epoch 990/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1335 - accuracy: 0.6047 - val_loss: 0.4473 - val_accuracy: 0.5559\n",
      "Epoch 991/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1560 - accuracy: 0.5974 - val_loss: 0.4550 - val_accuracy: 0.5559\n",
      "Epoch 992/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 788us/step - loss: 0.1434 - accuracy: 0.6040 - val_loss: 0.4314 - val_accuracy: 0.5471\n",
      "Epoch 993/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1688 - accuracy: 0.5977 - val_loss: 0.4271 - val_accuracy: 0.5559\n",
      "Epoch 994/1600\n",
      "680/680 [==============================] - 1s 798us/step - loss: 0.1496 - accuracy: 0.6007 - val_loss: 0.4557 - val_accuracy: 0.5706\n",
      "Epoch 995/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1466 - accuracy: 0.6025 - val_loss: 0.4313 - val_accuracy: 0.5588\n",
      "Epoch 996/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1427 - accuracy: 0.6021 - val_loss: 0.4274 - val_accuracy: 0.5500\n",
      "Epoch 997/1600\n",
      "680/680 [==============================] - 1s 813us/step - loss: 0.1563 - accuracy: 0.5996 - val_loss: 0.4153 - val_accuracy: 0.5500\n",
      "Epoch 998/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1460 - accuracy: 0.6010 - val_loss: 0.4145 - val_accuracy: 0.5676\n",
      "Epoch 999/1600\n",
      "680/680 [==============================] - 1s 751us/step - loss: 0.1498 - accuracy: 0.6018 - val_loss: 0.4217 - val_accuracy: 0.5529\n",
      "Epoch 1000/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1434 - accuracy: 0.6021 - val_loss: 0.4107 - val_accuracy: 0.5529\n",
      "Epoch 1001/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1394 - accuracy: 0.6014 - val_loss: 0.4338 - val_accuracy: 0.5588\n",
      "Epoch 1002/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1608 - accuracy: 0.5970 - val_loss: 0.4402 - val_accuracy: 0.5559\n",
      "Epoch 1003/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1653 - accuracy: 0.5988 - val_loss: 0.4436 - val_accuracy: 0.5529\n",
      "Epoch 1004/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1540 - accuracy: 0.6014 - val_loss: 0.4122 - val_accuracy: 0.5471\n",
      "Epoch 1005/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1551 - accuracy: 0.5974 - val_loss: 0.5175 - val_accuracy: 0.5559\n",
      "Epoch 1006/1600\n",
      "680/680 [==============================] - 1s 782us/step - loss: 0.1481 - accuracy: 0.5985 - val_loss: 0.4487 - val_accuracy: 0.5529\n",
      "Epoch 1007/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1544 - accuracy: 0.5992 - val_loss: 0.4420 - val_accuracy: 0.5618\n",
      "Epoch 1008/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1321 - accuracy: 0.6043 - val_loss: 0.4743 - val_accuracy: 0.5588\n",
      "Epoch 1009/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1568 - accuracy: 0.5996 - val_loss: 0.4458 - val_accuracy: 0.5588\n",
      "Epoch 1010/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1506 - accuracy: 0.5996 - val_loss: 0.4728 - val_accuracy: 0.5471\n",
      "Epoch 1011/1600\n",
      "680/680 [==============================] - 1s 814us/step - loss: 0.1451 - accuracy: 0.6007 - val_loss: 0.4327 - val_accuracy: 0.5588\n",
      "Epoch 1012/1600\n",
      "680/680 [==============================] - 1s 780us/step - loss: 0.1460 - accuracy: 0.6021 - val_loss: 0.4669 - val_accuracy: 0.5559\n",
      "Epoch 1013/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1511 - accuracy: 0.6003 - val_loss: 0.4579 - val_accuracy: 0.5559\n",
      "Epoch 1014/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1725 - accuracy: 0.5951 - val_loss: 0.5008 - val_accuracy: 0.5382\n",
      "Epoch 1015/1600\n",
      "680/680 [==============================] - 1s 797us/step - loss: 0.1546 - accuracy: 0.5996 - val_loss: 0.4452 - val_accuracy: 0.5500\n",
      "Epoch 1016/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1498 - accuracy: 0.5992 - val_loss: 0.5165 - val_accuracy: 0.5676\n",
      "Epoch 1017/1600\n",
      "680/680 [==============================] - 1s 756us/step - loss: 0.1474 - accuracy: 0.6010 - val_loss: 0.4251 - val_accuracy: 0.5529\n",
      "Epoch 1018/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1569 - accuracy: 0.5970 - val_loss: 0.4533 - val_accuracy: 0.5529\n",
      "Epoch 1019/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1560 - accuracy: 0.5996 - val_loss: 0.4413 - val_accuracy: 0.5588\n",
      "Epoch 1020/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1513 - accuracy: 0.6003 - val_loss: 0.4338 - val_accuracy: 0.5529\n",
      "Epoch 1021/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1492 - accuracy: 0.6003 - val_loss: 0.4592 - val_accuracy: 0.5500\n",
      "Epoch 1022/1600\n",
      "680/680 [==============================] - 1s 770us/step - loss: 0.1514 - accuracy: 0.5996 - val_loss: 0.4338 - val_accuracy: 0.5588\n",
      "Epoch 1023/1600\n",
      "680/680 [==============================] - 0s 687us/step - loss: 0.1530 - accuracy: 0.5970 - val_loss: 0.4410 - val_accuracy: 0.5529\n",
      "Epoch 1024/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1534 - accuracy: 0.5985 - val_loss: 0.5045 - val_accuracy: 0.5471\n",
      "Epoch 1025/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1465 - accuracy: 0.6018 - val_loss: 0.4426 - val_accuracy: 0.5588\n",
      "Epoch 1026/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1409 - accuracy: 0.6003 - val_loss: 0.4731 - val_accuracy: 0.5618\n",
      "Epoch 1027/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1682 - accuracy: 0.5981 - val_loss: 0.5062 - val_accuracy: 0.5529\n",
      "Epoch 1028/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1483 - accuracy: 0.6040 - val_loss: 0.4729 - val_accuracy: 0.5559\n",
      "Epoch 1029/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1395 - accuracy: 0.6025 - val_loss: 0.4642 - val_accuracy: 0.5588\n",
      "Epoch 1030/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1514 - accuracy: 0.6014 - val_loss: 0.4815 - val_accuracy: 0.5441\n",
      "Epoch 1031/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1686 - accuracy: 0.5977 - val_loss: 0.4387 - val_accuracy: 0.5588\n",
      "Epoch 1032/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1434 - accuracy: 0.6043 - val_loss: 0.4547 - val_accuracy: 0.5588\n",
      "Epoch 1033/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1462 - accuracy: 0.6036 - val_loss: 0.4497 - val_accuracy: 0.5588\n",
      "Epoch 1034/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1529 - accuracy: 0.5992 - val_loss: 0.4577 - val_accuracy: 0.5618\n",
      "Epoch 1035/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1485 - accuracy: 0.5996 - val_loss: 0.4571 - val_accuracy: 0.5529\n",
      "Epoch 1036/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1598 - accuracy: 0.5981 - val_loss: 0.4581 - val_accuracy: 0.5618\n",
      "Epoch 1037/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1489 - accuracy: 0.6010 - val_loss: 0.4978 - val_accuracy: 0.5647\n",
      "Epoch 1038/1600\n",
      "680/680 [==============================] - 1s 736us/step - loss: 0.1627 - accuracy: 0.5970 - val_loss: 0.4459 - val_accuracy: 0.5500\n",
      "Epoch 1039/1600\n",
      "680/680 [==============================] - 0s 716us/step - loss: 0.1532 - accuracy: 0.5981 - val_loss: 0.4635 - val_accuracy: 0.5500\n",
      "Epoch 1040/1600\n",
      "680/680 [==============================] - 0s 711us/step - loss: 0.1509 - accuracy: 0.6010 - val_loss: 0.4211 - val_accuracy: 0.5559\n",
      "Epoch 1041/1600\n",
      "680/680 [==============================] - 1s 757us/step - loss: 0.1314 - accuracy: 0.6040 - val_loss: 0.4744 - val_accuracy: 0.5500\n",
      "Epoch 1042/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1543 - accuracy: 0.5974 - val_loss: 0.4797 - val_accuracy: 0.5588\n",
      "Epoch 1043/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1561 - accuracy: 0.6007 - val_loss: 0.4444 - val_accuracy: 0.5529\n",
      "Epoch 1044/1600\n",
      "680/680 [==============================] - 1s 797us/step - loss: 0.1592 - accuracy: 0.5988 - val_loss: 0.4764 - val_accuracy: 0.5588\n",
      "Epoch 1045/1600\n",
      "680/680 [==============================] - 1s 797us/step - loss: 0.1520 - accuracy: 0.5996 - val_loss: 0.4498 - val_accuracy: 0.5618\n",
      "Epoch 1046/1600\n",
      "680/680 [==============================] - 1s 764us/step - loss: 0.1450 - accuracy: 0.6014 - val_loss: 0.4869 - val_accuracy: 0.5706\n",
      "Epoch 1047/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 756us/step - loss: 0.1357 - accuracy: 0.6054 - val_loss: 0.4675 - val_accuracy: 0.5618\n",
      "Epoch 1048/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1433 - accuracy: 0.5996 - val_loss: 0.4460 - val_accuracy: 0.5618\n",
      "Epoch 1049/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1437 - accuracy: 0.6018 - val_loss: 0.4094 - val_accuracy: 0.5588\n",
      "Epoch 1050/1600\n",
      "680/680 [==============================] - 1s 752us/step - loss: 0.1647 - accuracy: 0.5977 - val_loss: 0.4390 - val_accuracy: 0.5471\n",
      "Epoch 1051/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1484 - accuracy: 0.6014 - val_loss: 0.4326 - val_accuracy: 0.5559\n",
      "Epoch 1052/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1532 - accuracy: 0.5977 - val_loss: 0.4670 - val_accuracy: 0.5412\n",
      "Epoch 1053/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1438 - accuracy: 0.6007 - val_loss: 0.4690 - val_accuracy: 0.5559\n",
      "Epoch 1054/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1393 - accuracy: 0.6029 - val_loss: 0.4355 - val_accuracy: 0.5588\n",
      "Epoch 1055/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1483 - accuracy: 0.5999 - val_loss: 0.4362 - val_accuracy: 0.5529\n",
      "Epoch 1056/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1397 - accuracy: 0.6003 - val_loss: 0.4509 - val_accuracy: 0.5647\n",
      "Epoch 1057/1600\n",
      "680/680 [==============================] - 1s 799us/step - loss: 0.1512 - accuracy: 0.5999 - val_loss: 0.4484 - val_accuracy: 0.5647\n",
      "Epoch 1058/1600\n",
      "680/680 [==============================] - 1s 751us/step - loss: 0.1452 - accuracy: 0.6010 - val_loss: 0.4689 - val_accuracy: 0.5618\n",
      "Epoch 1059/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1386 - accuracy: 0.6021 - val_loss: 0.4870 - val_accuracy: 0.5529\n",
      "Epoch 1060/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1410 - accuracy: 0.6040 - val_loss: 0.4783 - val_accuracy: 0.5588\n",
      "Epoch 1061/1600\n",
      "680/680 [==============================] - 1s 799us/step - loss: 0.1426 - accuracy: 0.6036 - val_loss: 0.4683 - val_accuracy: 0.5588\n",
      "Epoch 1062/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1417 - accuracy: 0.6029 - val_loss: 0.4862 - val_accuracy: 0.5588\n",
      "Epoch 1063/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1496 - accuracy: 0.5992 - val_loss: 0.4531 - val_accuracy: 0.5500\n",
      "Epoch 1064/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1451 - accuracy: 0.6029 - val_loss: 0.4684 - val_accuracy: 0.5588\n",
      "Epoch 1065/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1475 - accuracy: 0.6003 - val_loss: 0.4813 - val_accuracy: 0.5441\n",
      "Epoch 1066/1600\n",
      "680/680 [==============================] - 0s 717us/step - loss: 0.1378 - accuracy: 0.6036 - val_loss: 0.4391 - val_accuracy: 0.5588\n",
      "Epoch 1067/1600\n",
      "680/680 [==============================] - 0s 675us/step - loss: 0.1450 - accuracy: 0.6007 - val_loss: 0.4251 - val_accuracy: 0.5353\n",
      "Epoch 1068/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1465 - accuracy: 0.6003 - val_loss: 0.4304 - val_accuracy: 0.5588\n",
      "Epoch 1069/1600\n",
      "680/680 [==============================] - 1s 740us/step - loss: 0.1290 - accuracy: 0.6062 - val_loss: 0.4259 - val_accuracy: 0.5618\n",
      "Epoch 1070/1600\n",
      "680/680 [==============================] - 0s 733us/step - loss: 0.1400 - accuracy: 0.6043 - val_loss: 0.4553 - val_accuracy: 0.5529\n",
      "Epoch 1071/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1499 - accuracy: 0.5985 - val_loss: 0.4387 - val_accuracy: 0.5500\n",
      "Epoch 1072/1600\n",
      "680/680 [==============================] - 1s 738us/step - loss: 0.1526 - accuracy: 0.5962 - val_loss: 0.4389 - val_accuracy: 0.5676\n",
      "Epoch 1073/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1424 - accuracy: 0.6014 - val_loss: 0.4499 - val_accuracy: 0.5559\n",
      "Epoch 1074/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1436 - accuracy: 0.6032 - val_loss: 0.4293 - val_accuracy: 0.5471\n",
      "Epoch 1075/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1442 - accuracy: 0.6025 - val_loss: 0.4594 - val_accuracy: 0.5500\n",
      "Epoch 1076/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1313 - accuracy: 0.6051 - val_loss: 0.4662 - val_accuracy: 0.5529\n",
      "Epoch 1077/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1524 - accuracy: 0.5996 - val_loss: 0.4622 - val_accuracy: 0.5471\n",
      "Epoch 1078/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1495 - accuracy: 0.5992 - val_loss: 0.4480 - val_accuracy: 0.5588\n",
      "Epoch 1079/1600\n",
      "680/680 [==============================] - 0s 730us/step - loss: 0.1475 - accuracy: 0.6003 - val_loss: 0.4536 - val_accuracy: 0.5618\n",
      "Epoch 1080/1600\n",
      "680/680 [==============================] - 0s 713us/step - loss: 0.1529 - accuracy: 0.5970 - val_loss: 0.4573 - val_accuracy: 0.5559\n",
      "Epoch 1081/1600\n",
      "680/680 [==============================] - 0s 707us/step - loss: 0.1424 - accuracy: 0.6021 - val_loss: 0.4485 - val_accuracy: 0.5529\n",
      "Epoch 1082/1600\n",
      "680/680 [==============================] - 0s 711us/step - loss: 0.1408 - accuracy: 0.6021 - val_loss: 0.4608 - val_accuracy: 0.5559\n",
      "Epoch 1083/1600\n",
      "680/680 [==============================] - 0s 715us/step - loss: 0.1320 - accuracy: 0.6043 - val_loss: 0.4929 - val_accuracy: 0.5559\n",
      "Epoch 1084/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1397 - accuracy: 0.6007 - val_loss: 0.4364 - val_accuracy: 0.5529\n",
      "Epoch 1085/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1548 - accuracy: 0.6003 - val_loss: 0.4810 - val_accuracy: 0.5529\n",
      "Epoch 1086/1600\n",
      "680/680 [==============================] - 1s 757us/step - loss: 0.1620 - accuracy: 0.5977 - val_loss: 0.4734 - val_accuracy: 0.5618\n",
      "Epoch 1087/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1550 - accuracy: 0.6007 - val_loss: 0.4362 - val_accuracy: 0.5647\n",
      "Epoch 1088/1600\n",
      "680/680 [==============================] - 0s 735us/step - loss: 0.1431 - accuracy: 0.6025 - val_loss: 0.4467 - val_accuracy: 0.5500\n",
      "Epoch 1089/1600\n",
      "680/680 [==============================] - 1s 739us/step - loss: 0.1487 - accuracy: 0.6010 - val_loss: 0.4240 - val_accuracy: 0.5559\n",
      "Epoch 1090/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1497 - accuracy: 0.6007 - val_loss: 0.4431 - val_accuracy: 0.5500\n",
      "Epoch 1091/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1367 - accuracy: 0.6058 - val_loss: 0.4460 - val_accuracy: 0.5529\n",
      "Epoch 1092/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1214 - accuracy: 0.6073 - val_loss: 0.4443 - val_accuracy: 0.5618\n",
      "Epoch 1093/1600\n",
      "680/680 [==============================] - 1s 778us/step - loss: 0.1586 - accuracy: 0.5966 - val_loss: 0.4530 - val_accuracy: 0.5618\n",
      "Epoch 1094/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1322 - accuracy: 0.6062 - val_loss: 0.4639 - val_accuracy: 0.5559\n",
      "Epoch 1095/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1358 - accuracy: 0.6036 - val_loss: 0.4734 - val_accuracy: 0.5647\n",
      "Epoch 1096/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1535 - accuracy: 0.5988 - val_loss: 0.4797 - val_accuracy: 0.5559\n",
      "Epoch 1097/1600\n",
      "680/680 [==============================] - 1s 755us/step - loss: 0.1496 - accuracy: 0.6007 - val_loss: 0.4566 - val_accuracy: 0.5559\n",
      "Epoch 1098/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1395 - accuracy: 0.6036 - val_loss: 0.4864 - val_accuracy: 0.5529\n",
      "Epoch 1099/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1380 - accuracy: 0.6040 - val_loss: 0.4622 - val_accuracy: 0.5559\n",
      "Epoch 1100/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1430 - accuracy: 0.6036 - val_loss: 0.4500 - val_accuracy: 0.5529\n",
      "Epoch 1101/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1423 - accuracy: 0.6036 - val_loss: 0.4467 - val_accuracy: 0.5618\n",
      "Epoch 1102/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 782us/step - loss: 0.1601 - accuracy: 0.5977 - val_loss: 0.4827 - val_accuracy: 0.5559\n",
      "Epoch 1103/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1362 - accuracy: 0.6043 - val_loss: 0.4396 - val_accuracy: 0.5559\n",
      "Epoch 1104/1600\n",
      "680/680 [==============================] - 1s 780us/step - loss: 0.1383 - accuracy: 0.6036 - val_loss: 0.4890 - val_accuracy: 0.5353\n",
      "Epoch 1105/1600\n",
      "680/680 [==============================] - 1s 774us/step - loss: 0.1467 - accuracy: 0.6003 - val_loss: 0.4914 - val_accuracy: 0.5500\n",
      "Epoch 1106/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1292 - accuracy: 0.6043 - val_loss: 0.4651 - val_accuracy: 0.5500\n",
      "Epoch 1107/1600\n",
      "680/680 [==============================] - 1s 779us/step - loss: 0.1273 - accuracy: 0.6058 - val_loss: 0.4887 - val_accuracy: 0.5647\n",
      "Epoch 1108/1600\n",
      "680/680 [==============================] - 1s 777us/step - loss: 0.1533 - accuracy: 0.5988 - val_loss: 0.4459 - val_accuracy: 0.5529\n",
      "Epoch 1109/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1351 - accuracy: 0.6043 - val_loss: 0.4446 - val_accuracy: 0.5529\n",
      "Epoch 1110/1600\n",
      "680/680 [==============================] - 1s 772us/step - loss: 0.1549 - accuracy: 0.5992 - val_loss: 0.4499 - val_accuracy: 0.5647\n",
      "Epoch 1111/1600\n",
      "680/680 [==============================] - 1s 778us/step - loss: 0.1453 - accuracy: 0.6021 - val_loss: 0.4578 - val_accuracy: 0.5471\n",
      "Epoch 1112/1600\n",
      "680/680 [==============================] - 1s 738us/step - loss: 0.1463 - accuracy: 0.6036 - val_loss: 0.4752 - val_accuracy: 0.5529\n",
      "Epoch 1113/1600\n",
      "680/680 [==============================] - 1s 738us/step - loss: 0.1416 - accuracy: 0.5996 - val_loss: 0.4696 - val_accuracy: 0.5559\n",
      "Epoch 1114/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1373 - accuracy: 0.6047 - val_loss: 0.4770 - val_accuracy: 0.5647\n",
      "Epoch 1115/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1418 - accuracy: 0.6029 - val_loss: 0.4920 - val_accuracy: 0.5588\n",
      "Epoch 1116/1600\n",
      "680/680 [==============================] - 1s 737us/step - loss: 0.1310 - accuracy: 0.6062 - val_loss: 0.4874 - val_accuracy: 0.5529\n",
      "Epoch 1117/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1438 - accuracy: 0.6029 - val_loss: 0.4905 - val_accuracy: 0.5441\n",
      "Epoch 1118/1600\n",
      "680/680 [==============================] - 1s 765us/step - loss: 0.1424 - accuracy: 0.6021 - val_loss: 0.4760 - val_accuracy: 0.5559\n",
      "Epoch 1119/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1375 - accuracy: 0.6040 - val_loss: 0.4628 - val_accuracy: 0.5529\n",
      "Epoch 1120/1600\n",
      "680/680 [==============================] - 0s 734us/step - loss: 0.1347 - accuracy: 0.6040 - val_loss: 0.4247 - val_accuracy: 0.5588\n",
      "Epoch 1121/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1425 - accuracy: 0.6047 - val_loss: 0.4606 - val_accuracy: 0.5382\n",
      "Epoch 1122/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1459 - accuracy: 0.6007 - val_loss: 0.4963 - val_accuracy: 0.5588\n",
      "Epoch 1123/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1406 - accuracy: 0.6036 - val_loss: 0.4553 - val_accuracy: 0.5471\n",
      "Epoch 1124/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1417 - accuracy: 0.6025 - val_loss: 0.4509 - val_accuracy: 0.5441\n",
      "Epoch 1125/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1294 - accuracy: 0.6054 - val_loss: 0.4338 - val_accuracy: 0.5500\n",
      "Epoch 1126/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1445 - accuracy: 0.6014 - val_loss: 0.4449 - val_accuracy: 0.5676\n",
      "Epoch 1127/1600\n",
      "680/680 [==============================] - 1s 776us/step - loss: 0.1555 - accuracy: 0.5999 - val_loss: 0.4907 - val_accuracy: 0.5588\n",
      "Epoch 1128/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1378 - accuracy: 0.6047 - val_loss: 0.4155 - val_accuracy: 0.5588\n",
      "Epoch 1129/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1386 - accuracy: 0.6029 - val_loss: 0.4625 - val_accuracy: 0.5382\n",
      "Epoch 1130/1600\n",
      "680/680 [==============================] - 1s 833us/step - loss: 0.1423 - accuracy: 0.6007 - val_loss: 0.4453 - val_accuracy: 0.5647\n",
      "Epoch 1131/1600\n",
      "680/680 [==============================] - 1s 813us/step - loss: 0.1356 - accuracy: 0.6043 - val_loss: 0.4310 - val_accuracy: 0.5471\n",
      "Epoch 1132/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1339 - accuracy: 0.6032 - val_loss: 0.4649 - val_accuracy: 0.5559\n",
      "Epoch 1133/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1532 - accuracy: 0.6003 - val_loss: 0.4309 - val_accuracy: 0.5471\n",
      "Epoch 1134/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1373 - accuracy: 0.6032 - val_loss: 0.4524 - val_accuracy: 0.5500\n",
      "Epoch 1135/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1387 - accuracy: 0.6036 - val_loss: 0.4677 - val_accuracy: 0.5471\n",
      "Epoch 1136/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1422 - accuracy: 0.6010 - val_loss: 0.4622 - val_accuracy: 0.5500\n",
      "Epoch 1137/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1408 - accuracy: 0.6036 - val_loss: 0.4841 - val_accuracy: 0.5382\n",
      "Epoch 1138/1600\n",
      "680/680 [==============================] - 1s 765us/step - loss: 0.1355 - accuracy: 0.6036 - val_loss: 0.4695 - val_accuracy: 0.5529\n",
      "Epoch 1139/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1518 - accuracy: 0.6007 - val_loss: 0.5152 - val_accuracy: 0.5588\n",
      "Epoch 1140/1600\n",
      "680/680 [==============================] - 1s 781us/step - loss: 0.1405 - accuracy: 0.6029 - val_loss: 0.4457 - val_accuracy: 0.5588\n",
      "Epoch 1141/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1409 - accuracy: 0.6036 - val_loss: 0.4315 - val_accuracy: 0.5559\n",
      "Epoch 1142/1600\n",
      "680/680 [==============================] - 1s 782us/step - loss: 0.1321 - accuracy: 0.6025 - val_loss: 0.4932 - val_accuracy: 0.5618\n",
      "Epoch 1143/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1512 - accuracy: 0.5996 - val_loss: 0.4752 - val_accuracy: 0.5529\n",
      "Epoch 1144/1600\n",
      "680/680 [==============================] - 1s 832us/step - loss: 0.1369 - accuracy: 0.6032 - val_loss: 0.4395 - val_accuracy: 0.5559\n",
      "Epoch 1145/1600\n",
      "680/680 [==============================] - 1s 824us/step - loss: 0.1334 - accuracy: 0.6047 - val_loss: 0.5040 - val_accuracy: 0.5588\n",
      "Epoch 1146/1600\n",
      "680/680 [==============================] - 1s 836us/step - loss: 0.1493 - accuracy: 0.5999 - val_loss: 0.5278 - val_accuracy: 0.5676\n",
      "Epoch 1147/1600\n",
      "680/680 [==============================] - 1s 809us/step - loss: 0.1408 - accuracy: 0.6040 - val_loss: 0.4591 - val_accuracy: 0.5588\n",
      "Epoch 1148/1600\n",
      "680/680 [==============================] - 1s 781us/step - loss: 0.1318 - accuracy: 0.6047 - val_loss: 0.4597 - val_accuracy: 0.5559\n",
      "Epoch 1149/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1348 - accuracy: 0.6032 - val_loss: 0.4589 - val_accuracy: 0.5500\n",
      "Epoch 1150/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1636 - accuracy: 0.5988 - val_loss: 0.4618 - val_accuracy: 0.5529\n",
      "Epoch 1151/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1349 - accuracy: 0.6040 - val_loss: 0.4922 - val_accuracy: 0.5471\n",
      "Epoch 1152/1600\n",
      "680/680 [==============================] - 1s 753us/step - loss: 0.1434 - accuracy: 0.5977 - val_loss: 0.4811 - val_accuracy: 0.5588\n",
      "Epoch 1153/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1425 - accuracy: 0.6007 - val_loss: 0.4766 - val_accuracy: 0.5529\n",
      "Epoch 1154/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1391 - accuracy: 0.6047 - val_loss: 0.4562 - val_accuracy: 0.5559\n",
      "Epoch 1155/1600\n",
      "680/680 [==============================] - 1s 779us/step - loss: 0.1339 - accuracy: 0.6032 - val_loss: 0.4939 - val_accuracy: 0.5529\n",
      "Epoch 1156/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1813 - accuracy: 0.5985 - val_loss: 0.5306 - val_accuracy: 0.5588\n",
      "Epoch 1157/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 792us/step - loss: 0.1411 - accuracy: 0.6043 - val_loss: 0.4682 - val_accuracy: 0.5500\n",
      "Epoch 1158/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1358 - accuracy: 0.6043 - val_loss: 0.4597 - val_accuracy: 0.5559\n",
      "Epoch 1159/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1374 - accuracy: 0.6014 - val_loss: 0.4594 - val_accuracy: 0.5559\n",
      "Epoch 1160/1600\n",
      "680/680 [==============================] - 1s 740us/step - loss: 0.1397 - accuracy: 0.6029 - val_loss: 0.4389 - val_accuracy: 0.5559\n",
      "Epoch 1161/1600\n",
      "680/680 [==============================] - 0s 710us/step - loss: 0.1341 - accuracy: 0.6043 - val_loss: 0.4868 - val_accuracy: 0.5471\n",
      "Epoch 1162/1600\n",
      "680/680 [==============================] - 0s 708us/step - loss: 0.1521 - accuracy: 0.6018 - val_loss: 0.4447 - val_accuracy: 0.5588\n",
      "Epoch 1163/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1425 - accuracy: 0.6010 - val_loss: 0.4694 - val_accuracy: 0.5500\n",
      "Epoch 1164/1600\n",
      "680/680 [==============================] - 1s 740us/step - loss: 0.1330 - accuracy: 0.6029 - val_loss: 0.4920 - val_accuracy: 0.5500\n",
      "Epoch 1165/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1387 - accuracy: 0.6029 - val_loss: 0.4464 - val_accuracy: 0.5471\n",
      "Epoch 1166/1600\n",
      "680/680 [==============================] - 0s 735us/step - loss: 0.1345 - accuracy: 0.6021 - val_loss: 0.4866 - val_accuracy: 0.5471\n",
      "Epoch 1167/1600\n",
      "680/680 [==============================] - 1s 737us/step - loss: 0.1302 - accuracy: 0.6073 - val_loss: 0.4648 - val_accuracy: 0.5500\n",
      "Epoch 1168/1600\n",
      "680/680 [==============================] - 1s 740us/step - loss: 0.1375 - accuracy: 0.6036 - val_loss: 0.4857 - val_accuracy: 0.5618\n",
      "Epoch 1169/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1335 - accuracy: 0.6054 - val_loss: 0.4342 - val_accuracy: 0.5559\n",
      "Epoch 1170/1600\n",
      "680/680 [==============================] - 1s 738us/step - loss: 0.1407 - accuracy: 0.6018 - val_loss: 0.4380 - val_accuracy: 0.5647\n",
      "Epoch 1171/1600\n",
      "680/680 [==============================] - 0s 733us/step - loss: 0.1465 - accuracy: 0.6018 - val_loss: 0.4173 - val_accuracy: 0.5647\n",
      "Epoch 1172/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1384 - accuracy: 0.6029 - val_loss: 0.4902 - val_accuracy: 0.5441\n",
      "Epoch 1173/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1543 - accuracy: 0.5999 - val_loss: 0.4849 - val_accuracy: 0.5588\n",
      "Epoch 1174/1600\n",
      "680/680 [==============================] - 1s 737us/step - loss: 0.1374 - accuracy: 0.6040 - val_loss: 0.4428 - val_accuracy: 0.5500\n",
      "Epoch 1175/1600\n",
      "680/680 [==============================] - 1s 740us/step - loss: 0.1337 - accuracy: 0.6040 - val_loss: 0.4399 - val_accuracy: 0.5559\n",
      "Epoch 1176/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1315 - accuracy: 0.6047 - val_loss: 0.4579 - val_accuracy: 0.5412\n",
      "Epoch 1177/1600\n",
      "680/680 [==============================] - 0s 731us/step - loss: 0.1395 - accuracy: 0.6021 - val_loss: 0.4268 - val_accuracy: 0.5471\n",
      "Epoch 1178/1600\n",
      "680/680 [==============================] - 1s 777us/step - loss: 0.1547 - accuracy: 0.6010 - val_loss: 0.4949 - val_accuracy: 0.5529\n",
      "Epoch 1179/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1427 - accuracy: 0.6007 - val_loss: 0.4729 - val_accuracy: 0.5559\n",
      "Epoch 1180/1600\n",
      "680/680 [==============================] - 1s 736us/step - loss: 0.1440 - accuracy: 0.6032 - val_loss: 0.4795 - val_accuracy: 0.5618\n",
      "Epoch 1181/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1389 - accuracy: 0.6047 - val_loss: 0.4735 - val_accuracy: 0.5559\n",
      "Epoch 1182/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1622 - accuracy: 0.5988 - val_loss: 0.4992 - val_accuracy: 0.5559\n",
      "Epoch 1183/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1274 - accuracy: 0.6069 - val_loss: 0.4659 - val_accuracy: 0.5500\n",
      "Epoch 1184/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1475 - accuracy: 0.6007 - val_loss: 0.4484 - val_accuracy: 0.5559\n",
      "Epoch 1185/1600\n",
      "680/680 [==============================] - 1s 782us/step - loss: 0.1381 - accuracy: 0.6043 - val_loss: 0.4274 - val_accuracy: 0.5500\n",
      "Epoch 1186/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1419 - accuracy: 0.6007 - val_loss: 0.4950 - val_accuracy: 0.5500\n",
      "Epoch 1187/1600\n",
      "680/680 [==============================] - 1s 780us/step - loss: 0.1435 - accuracy: 0.6018 - val_loss: 0.4552 - val_accuracy: 0.5500\n",
      "Epoch 1188/1600\n",
      "680/680 [==============================] - 1s 826us/step - loss: 0.1350 - accuracy: 0.6047 - val_loss: 0.4604 - val_accuracy: 0.5559\n",
      "Epoch 1189/1600\n",
      "680/680 [==============================] - 1s 825us/step - loss: 0.1439 - accuracy: 0.6021 - val_loss: 0.4826 - val_accuracy: 0.5559\n",
      "Epoch 1190/1600\n",
      "680/680 [==============================] - 1s 825us/step - loss: 0.1435 - accuracy: 0.6014 - val_loss: 0.4972 - val_accuracy: 0.5441\n",
      "Epoch 1191/1600\n",
      "680/680 [==============================] - 1s 778us/step - loss: 0.1402 - accuracy: 0.6021 - val_loss: 0.4833 - val_accuracy: 0.5500\n",
      "Epoch 1192/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1441 - accuracy: 0.6032 - val_loss: 0.4580 - val_accuracy: 0.5559\n",
      "Epoch 1193/1600\n",
      "680/680 [==============================] - 1s 801us/step - loss: 0.1432 - accuracy: 0.6029 - val_loss: 0.4640 - val_accuracy: 0.5588\n",
      "Epoch 1194/1600\n",
      "680/680 [==============================] - 1s 847us/step - loss: 0.1338 - accuracy: 0.6047 - val_loss: 0.4830 - val_accuracy: 0.5529\n",
      "Epoch 1195/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1400 - accuracy: 0.6036 - val_loss: 0.5113 - val_accuracy: 0.5529\n",
      "Epoch 1196/1600\n",
      "680/680 [==============================] - 1s 826us/step - loss: 0.1470 - accuracy: 0.6010 - val_loss: 0.4669 - val_accuracy: 0.5471\n",
      "Epoch 1197/1600\n",
      "680/680 [==============================] - 0s 734us/step - loss: 0.1278 - accuracy: 0.6058 - val_loss: 0.4358 - val_accuracy: 0.5500\n",
      "Epoch 1198/1600\n",
      "680/680 [==============================] - 0s 684us/step - loss: 0.1428 - accuracy: 0.6007 - val_loss: 0.4875 - val_accuracy: 0.5471\n",
      "Epoch 1199/1600\n",
      "680/680 [==============================] - 0s 710us/step - loss: 0.1382 - accuracy: 0.6051 - val_loss: 0.4842 - val_accuracy: 0.5412\n",
      "Epoch 1200/1600\n",
      "680/680 [==============================] - 0s 717us/step - loss: 0.1380 - accuracy: 0.6025 - val_loss: 0.4534 - val_accuracy: 0.5559\n",
      "Epoch 1201/1600\n",
      "680/680 [==============================] - 0s 709us/step - loss: 0.1404 - accuracy: 0.6043 - val_loss: 0.4330 - val_accuracy: 0.5471\n",
      "Epoch 1202/1600\n",
      "680/680 [==============================] - 0s 705us/step - loss: 0.1370 - accuracy: 0.6032 - val_loss: 0.4792 - val_accuracy: 0.5529\n",
      "Epoch 1203/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1387 - accuracy: 0.6029 - val_loss: 0.4582 - val_accuracy: 0.5559\n",
      "Epoch 1204/1600\n",
      "680/680 [==============================] - 1s 758us/step - loss: 0.1349 - accuracy: 0.6066 - val_loss: 0.4369 - val_accuracy: 0.5441\n",
      "Epoch 1205/1600\n",
      "680/680 [==============================] - 1s 772us/step - loss: 0.1429 - accuracy: 0.6021 - val_loss: 0.4595 - val_accuracy: 0.5647\n",
      "Epoch 1206/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1461 - accuracy: 0.6018 - val_loss: 0.4432 - val_accuracy: 0.5618\n",
      "Epoch 1207/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1376 - accuracy: 0.6029 - val_loss: 0.4493 - val_accuracy: 0.5618\n",
      "Epoch 1208/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1319 - accuracy: 0.6043 - val_loss: 0.4864 - val_accuracy: 0.5529\n",
      "Epoch 1209/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1335 - accuracy: 0.6066 - val_loss: 0.4406 - val_accuracy: 0.5618\n",
      "Epoch 1210/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1650 - accuracy: 0.5981 - val_loss: 0.4891 - val_accuracy: 0.5588\n",
      "Epoch 1211/1600\n",
      "680/680 [==============================] - 1s 756us/step - loss: 0.1397 - accuracy: 0.6021 - val_loss: 0.4807 - val_accuracy: 0.5559\n",
      "Epoch 1212/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 746us/step - loss: 0.1250 - accuracy: 0.6062 - val_loss: 0.4771 - val_accuracy: 0.5529\n",
      "Epoch 1213/1600\n",
      "680/680 [==============================] - 1s 758us/step - loss: 0.1385 - accuracy: 0.6043 - val_loss: 0.4699 - val_accuracy: 0.5500\n",
      "Epoch 1214/1600\n",
      "680/680 [==============================] - 1s 752us/step - loss: 0.1469 - accuracy: 0.5996 - val_loss: 0.4953 - val_accuracy: 0.5471\n",
      "Epoch 1215/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1529 - accuracy: 0.5999 - val_loss: 0.4758 - val_accuracy: 0.5559\n",
      "Epoch 1216/1600\n",
      "680/680 [==============================] - 1s 756us/step - loss: 0.1454 - accuracy: 0.6032 - val_loss: 0.4312 - val_accuracy: 0.5529\n",
      "Epoch 1217/1600\n",
      "680/680 [==============================] - 1s 751us/step - loss: 0.1346 - accuracy: 0.6021 - val_loss: 0.4930 - val_accuracy: 0.5529\n",
      "Epoch 1218/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1259 - accuracy: 0.6069 - val_loss: 0.4454 - val_accuracy: 0.5559\n",
      "Epoch 1219/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1305 - accuracy: 0.6077 - val_loss: 0.4676 - val_accuracy: 0.5618\n",
      "Epoch 1220/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1480 - accuracy: 0.6018 - val_loss: 0.4714 - val_accuracy: 0.5618\n",
      "Epoch 1221/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1556 - accuracy: 0.5985 - val_loss: 0.4567 - val_accuracy: 0.5529\n",
      "Epoch 1222/1600\n",
      "680/680 [==============================] - 0s 726us/step - loss: 0.1377 - accuracy: 0.6025 - val_loss: 0.4259 - val_accuracy: 0.5500\n",
      "Epoch 1223/1600\n",
      "680/680 [==============================] - 0s 707us/step - loss: 0.1385 - accuracy: 0.6025 - val_loss: 0.5047 - val_accuracy: 0.5647\n",
      "Epoch 1224/1600\n",
      "680/680 [==============================] - 1s 755us/step - loss: 0.1386 - accuracy: 0.6054 - val_loss: 0.4789 - val_accuracy: 0.5412\n",
      "Epoch 1225/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1502 - accuracy: 0.5999 - val_loss: 0.4646 - val_accuracy: 0.5500\n",
      "Epoch 1226/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1300 - accuracy: 0.6054 - val_loss: 0.4613 - val_accuracy: 0.5588\n",
      "Epoch 1227/1600\n",
      "680/680 [==============================] - 1s 755us/step - loss: 0.1392 - accuracy: 0.6025 - val_loss: 0.4283 - val_accuracy: 0.5588\n",
      "Epoch 1228/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1307 - accuracy: 0.6047 - val_loss: 0.4911 - val_accuracy: 0.5559\n",
      "Epoch 1229/1600\n",
      "680/680 [==============================] - 0s 712us/step - loss: 0.1408 - accuracy: 0.6021 - val_loss: 0.4727 - val_accuracy: 0.5500\n",
      "Epoch 1230/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1458 - accuracy: 0.6032 - val_loss: 0.4937 - val_accuracy: 0.5559\n",
      "Epoch 1231/1600\n",
      "680/680 [==============================] - 0s 723us/step - loss: 0.1358 - accuracy: 0.6040 - val_loss: 0.4425 - val_accuracy: 0.5529\n",
      "Epoch 1232/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1515 - accuracy: 0.5996 - val_loss: 0.4422 - val_accuracy: 0.5529\n",
      "Epoch 1233/1600\n",
      "680/680 [==============================] - 0s 672us/step - loss: 0.1417 - accuracy: 0.6014 - val_loss: 0.4540 - val_accuracy: 0.5441\n",
      "Epoch 1234/1600\n",
      "680/680 [==============================] - 0s 717us/step - loss: 0.1467 - accuracy: 0.6040 - val_loss: 0.4715 - val_accuracy: 0.5441\n",
      "Epoch 1235/1600\n",
      "680/680 [==============================] - 0s 710us/step - loss: 0.1340 - accuracy: 0.6054 - val_loss: 0.4326 - val_accuracy: 0.5471\n",
      "Epoch 1236/1600\n",
      "680/680 [==============================] - 0s 715us/step - loss: 0.1344 - accuracy: 0.6036 - val_loss: 0.4778 - val_accuracy: 0.5471\n",
      "Epoch 1237/1600\n",
      "680/680 [==============================] - 1s 760us/step - loss: 0.1397 - accuracy: 0.6025 - val_loss: 0.4991 - val_accuracy: 0.5529\n",
      "Epoch 1238/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1528 - accuracy: 0.6032 - val_loss: 0.4785 - val_accuracy: 0.5441\n",
      "Epoch 1239/1600\n",
      "680/680 [==============================] - 1s 818us/step - loss: 0.1502 - accuracy: 0.6014 - val_loss: 0.4628 - val_accuracy: 0.5529\n",
      "Epoch 1240/1600\n",
      "680/680 [==============================] - 0s 692us/step - loss: 0.1413 - accuracy: 0.6029 - val_loss: 0.4375 - val_accuracy: 0.5559\n",
      "Epoch 1241/1600\n",
      "680/680 [==============================] - 0s 659us/step - loss: 0.1397 - accuracy: 0.6029 - val_loss: 0.4364 - val_accuracy: 0.5500\n",
      "Epoch 1242/1600\n",
      "680/680 [==============================] - 0s 732us/step - loss: 0.1390 - accuracy: 0.6062 - val_loss: 0.4513 - val_accuracy: 0.5618\n",
      "Epoch 1243/1600\n",
      "680/680 [==============================] - 0s 678us/step - loss: 0.1356 - accuracy: 0.6036 - val_loss: 0.4707 - val_accuracy: 0.5529\n",
      "Epoch 1244/1600\n",
      "680/680 [==============================] - 0s 706us/step - loss: 0.1266 - accuracy: 0.6066 - val_loss: 0.4756 - val_accuracy: 0.5588\n",
      "Epoch 1245/1600\n",
      "680/680 [==============================] - 1s 753us/step - loss: 0.1327 - accuracy: 0.6047 - val_loss: 0.4469 - val_accuracy: 0.5559\n",
      "Epoch 1246/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1469 - accuracy: 0.5999 - val_loss: 0.4844 - val_accuracy: 0.5618\n",
      "Epoch 1247/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1474 - accuracy: 0.6007 - val_loss: 0.5050 - val_accuracy: 0.5559\n",
      "Epoch 1248/1600\n",
      "680/680 [==============================] - 1s 736us/step - loss: 0.1488 - accuracy: 0.5988 - val_loss: 0.4373 - val_accuracy: 0.5618\n",
      "Epoch 1249/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1376 - accuracy: 0.6043 - val_loss: 0.4487 - val_accuracy: 0.5529\n",
      "Epoch 1250/1600\n",
      "680/680 [==============================] - 1s 740us/step - loss: 0.1617 - accuracy: 0.6021 - val_loss: 0.4318 - val_accuracy: 0.5559\n",
      "Epoch 1251/1600\n",
      "680/680 [==============================] - 1s 752us/step - loss: 0.1476 - accuracy: 0.6025 - val_loss: 0.4515 - val_accuracy: 0.5588\n",
      "Epoch 1252/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1384 - accuracy: 0.6029 - val_loss: 0.4796 - val_accuracy: 0.5559\n",
      "Epoch 1253/1600\n",
      "680/680 [==============================] - 0s 735us/step - loss: 0.1563 - accuracy: 0.6003 - val_loss: 0.5109 - val_accuracy: 0.5441\n",
      "Epoch 1254/1600\n",
      "680/680 [==============================] - 1s 771us/step - loss: 0.1394 - accuracy: 0.6032 - val_loss: 0.4772 - val_accuracy: 0.5559\n",
      "Epoch 1255/1600\n",
      "680/680 [==============================] - 0s 728us/step - loss: 0.1386 - accuracy: 0.6054 - val_loss: 0.4584 - val_accuracy: 0.5529\n",
      "Epoch 1256/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1502 - accuracy: 0.6007 - val_loss: 0.4721 - val_accuracy: 0.5500\n",
      "Epoch 1257/1600\n",
      "680/680 [==============================] - 1s 751us/step - loss: 0.1355 - accuracy: 0.6036 - val_loss: 0.4560 - val_accuracy: 0.5588\n",
      "Epoch 1258/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1377 - accuracy: 0.6032 - val_loss: 0.4926 - val_accuracy: 0.5588\n",
      "Epoch 1259/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1449 - accuracy: 0.6036 - val_loss: 0.4766 - val_accuracy: 0.5588\n",
      "Epoch 1260/1600\n",
      "680/680 [==============================] - 1s 846us/step - loss: 0.1346 - accuracy: 0.6047 - val_loss: 0.4705 - val_accuracy: 0.5441\n",
      "Epoch 1261/1600\n",
      "680/680 [==============================] - 0s 654us/step - loss: 0.1397 - accuracy: 0.6010 - val_loss: 0.4700 - val_accuracy: 0.5559\n",
      "Epoch 1262/1600\n",
      "680/680 [==============================] - 0s 710us/step - loss: 0.1278 - accuracy: 0.6077 - val_loss: 0.4724 - val_accuracy: 0.5294\n",
      "Epoch 1263/1600\n",
      "680/680 [==============================] - 0s 717us/step - loss: 0.1584 - accuracy: 0.5988 - val_loss: 0.4761 - val_accuracy: 0.5559\n",
      "Epoch 1264/1600\n",
      "680/680 [==============================] - 0s 713us/step - loss: 0.1510 - accuracy: 0.6007 - val_loss: 0.4839 - val_accuracy: 0.5588\n",
      "Epoch 1265/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1329 - accuracy: 0.6069 - val_loss: 0.4644 - val_accuracy: 0.5559\n",
      "Epoch 1266/1600\n",
      "680/680 [==============================] - 1s 739us/step - loss: 0.1386 - accuracy: 0.6025 - val_loss: 0.4634 - val_accuracy: 0.5559\n",
      "Epoch 1267/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 742us/step - loss: 0.1272 - accuracy: 0.6058 - val_loss: 0.4484 - val_accuracy: 0.5618\n",
      "Epoch 1268/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1485 - accuracy: 0.6007 - val_loss: 0.4652 - val_accuracy: 0.5559\n",
      "Epoch 1269/1600\n",
      "680/680 [==============================] - 1s 756us/step - loss: 0.1416 - accuracy: 0.6043 - val_loss: 0.4899 - val_accuracy: 0.5647\n",
      "Epoch 1270/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1335 - accuracy: 0.6054 - val_loss: 0.4449 - val_accuracy: 0.5559\n",
      "Epoch 1271/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1422 - accuracy: 0.6014 - val_loss: 0.4615 - val_accuracy: 0.5618\n",
      "Epoch 1272/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1331 - accuracy: 0.6043 - val_loss: 0.4732 - val_accuracy: 0.5441\n",
      "Epoch 1273/1600\n",
      "680/680 [==============================] - 1s 797us/step - loss: 0.1511 - accuracy: 0.5999 - val_loss: 0.4815 - val_accuracy: 0.5500\n",
      "Epoch 1274/1600\n",
      "680/680 [==============================] - 1s 762us/step - loss: 0.1549 - accuracy: 0.6010 - val_loss: 0.4553 - val_accuracy: 0.5588\n",
      "Epoch 1275/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1387 - accuracy: 0.6029 - val_loss: 0.4698 - val_accuracy: 0.5529\n",
      "Epoch 1276/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1323 - accuracy: 0.6054 - val_loss: 0.4615 - val_accuracy: 0.5529\n",
      "Epoch 1277/1600\n",
      "680/680 [==============================] - 1s 805us/step - loss: 0.1268 - accuracy: 0.6080 - val_loss: 0.4371 - val_accuracy: 0.5647\n",
      "Epoch 1278/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1542 - accuracy: 0.5999 - val_loss: 0.4965 - val_accuracy: 0.5588\n",
      "Epoch 1279/1600\n",
      "680/680 [==============================] - 1s 779us/step - loss: 0.1450 - accuracy: 0.6025 - val_loss: 0.4783 - val_accuracy: 0.5618\n",
      "Epoch 1280/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1359 - accuracy: 0.6032 - val_loss: 0.4320 - val_accuracy: 0.5588\n",
      "Epoch 1281/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1156 - accuracy: 0.6117 - val_loss: 0.4591 - val_accuracy: 0.5559\n",
      "Epoch 1282/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1329 - accuracy: 0.6054 - val_loss: 0.4315 - val_accuracy: 0.5559\n",
      "Epoch 1283/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1516 - accuracy: 0.6010 - val_loss: 0.5009 - val_accuracy: 0.5529\n",
      "Epoch 1284/1600\n",
      "680/680 [==============================] - 1s 781us/step - loss: 0.1450 - accuracy: 0.6032 - val_loss: 0.4641 - val_accuracy: 0.5500\n",
      "Epoch 1285/1600\n",
      "680/680 [==============================] - 1s 864us/step - loss: 0.1322 - accuracy: 0.6032 - val_loss: 0.4794 - val_accuracy: 0.5618\n",
      "Epoch 1286/1600\n",
      "680/680 [==============================] - 0s 725us/step - loss: 0.1379 - accuracy: 0.6025 - val_loss: 0.4808 - val_accuracy: 0.5559\n",
      "Epoch 1287/1600\n",
      "680/680 [==============================] - 0s 721us/step - loss: 0.1314 - accuracy: 0.6080 - val_loss: 0.4981 - val_accuracy: 0.5441\n",
      "Epoch 1288/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1432 - accuracy: 0.6014 - val_loss: 0.4705 - val_accuracy: 0.5559\n",
      "Epoch 1289/1600\n",
      "680/680 [==============================] - 1s 752us/step - loss: 0.1485 - accuracy: 0.6021 - val_loss: 0.4482 - val_accuracy: 0.5618\n",
      "Epoch 1290/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1424 - accuracy: 0.6040 - val_loss: 0.4786 - val_accuracy: 0.5559\n",
      "Epoch 1291/1600\n",
      "680/680 [==============================] - 1s 756us/step - loss: 0.1270 - accuracy: 0.6073 - val_loss: 0.4777 - val_accuracy: 0.5500\n",
      "Epoch 1292/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1427 - accuracy: 0.6032 - val_loss: 0.4644 - val_accuracy: 0.5647\n",
      "Epoch 1293/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1408 - accuracy: 0.6032 - val_loss: 0.4000 - val_accuracy: 0.5500\n",
      "Epoch 1294/1600\n",
      "680/680 [==============================] - 1s 772us/step - loss: 0.1330 - accuracy: 0.6036 - val_loss: 0.4737 - val_accuracy: 0.5412\n",
      "Epoch 1295/1600\n",
      "680/680 [==============================] - 0s 679us/step - loss: 0.1421 - accuracy: 0.6010 - val_loss: 0.4568 - val_accuracy: 0.5588\n",
      "Epoch 1296/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1379 - accuracy: 0.6047 - val_loss: 0.4669 - val_accuracy: 0.5529\n",
      "Epoch 1297/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1343 - accuracy: 0.6043 - val_loss: 0.4485 - val_accuracy: 0.5559\n",
      "Epoch 1298/1600\n",
      "680/680 [==============================] - 1s 738us/step - loss: 0.1239 - accuracy: 0.6069 - val_loss: 0.4687 - val_accuracy: 0.5588\n",
      "Epoch 1299/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1433 - accuracy: 0.6029 - val_loss: 0.4598 - val_accuracy: 0.5588\n",
      "Epoch 1300/1600\n",
      "680/680 [==============================] - 1s 776us/step - loss: 0.1350 - accuracy: 0.6058 - val_loss: 0.4296 - val_accuracy: 0.5559\n",
      "Epoch 1301/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1449 - accuracy: 0.6036 - val_loss: 0.5038 - val_accuracy: 0.5618\n",
      "Epoch 1302/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1310 - accuracy: 0.6054 - val_loss: 0.4835 - val_accuracy: 0.5412\n",
      "Epoch 1303/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1387 - accuracy: 0.6018 - val_loss: 0.4790 - val_accuracy: 0.5471\n",
      "Epoch 1304/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1308 - accuracy: 0.6058 - val_loss: 0.4867 - val_accuracy: 0.5471\n",
      "Epoch 1305/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1566 - accuracy: 0.6007 - val_loss: 0.5006 - val_accuracy: 0.5471\n",
      "Epoch 1306/1600\n",
      "680/680 [==============================] - 1s 772us/step - loss: 0.1431 - accuracy: 0.6025 - val_loss: 0.4968 - val_accuracy: 0.5529\n",
      "Epoch 1307/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1346 - accuracy: 0.6069 - val_loss: 0.4604 - val_accuracy: 0.5559\n",
      "Epoch 1308/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1326 - accuracy: 0.6043 - val_loss: 0.5034 - val_accuracy: 0.5441\n",
      "Epoch 1309/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1441 - accuracy: 0.6018 - val_loss: 0.4641 - val_accuracy: 0.5471\n",
      "Epoch 1310/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1520 - accuracy: 0.5999 - val_loss: 0.4925 - val_accuracy: 0.5500\n",
      "Epoch 1311/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1329 - accuracy: 0.6036 - val_loss: 0.4451 - val_accuracy: 0.5471\n",
      "Epoch 1312/1600\n",
      "680/680 [==============================] - 1s 816us/step - loss: 0.1273 - accuracy: 0.6066 - val_loss: 0.4635 - val_accuracy: 0.5529\n",
      "Epoch 1313/1600\n",
      "680/680 [==============================] - 1s 781us/step - loss: 0.1317 - accuracy: 0.6047 - val_loss: 0.4829 - val_accuracy: 0.5559\n",
      "Epoch 1314/1600\n",
      "680/680 [==============================] - 1s 843us/step - loss: 0.1505 - accuracy: 0.5985 - val_loss: 0.4852 - val_accuracy: 0.5559\n",
      "Epoch 1315/1600\n",
      "680/680 [==============================] - 1s 833us/step - loss: 0.1449 - accuracy: 0.6029 - val_loss: 0.4982 - val_accuracy: 0.5500\n",
      "Epoch 1316/1600\n",
      "680/680 [==============================] - 1s 871us/step - loss: 0.1436 - accuracy: 0.6025 - val_loss: 0.4929 - val_accuracy: 0.5471\n",
      "Epoch 1317/1600\n",
      "680/680 [==============================] - 1s 870us/step - loss: 0.1395 - accuracy: 0.6047 - val_loss: 0.4361 - val_accuracy: 0.5529\n",
      "Epoch 1318/1600\n",
      "680/680 [==============================] - 1s 869us/step - loss: 0.1424 - accuracy: 0.6021 - val_loss: 0.4554 - val_accuracy: 0.5529\n",
      "Epoch 1319/1600\n",
      "680/680 [==============================] - 1s 876us/step - loss: 0.1354 - accuracy: 0.6077 - val_loss: 0.4490 - val_accuracy: 0.5529\n",
      "Epoch 1320/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1464 - accuracy: 0.6036 - val_loss: 0.4440 - val_accuracy: 0.5618\n",
      "Epoch 1321/1600\n",
      "680/680 [==============================] - 0s 707us/step - loss: 0.1424 - accuracy: 0.6021 - val_loss: 0.4719 - val_accuracy: 0.5529\n",
      "Epoch 1322/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 802us/step - loss: 0.1498 - accuracy: 0.6021 - val_loss: 0.4430 - val_accuracy: 0.5559\n",
      "Epoch 1323/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1471 - accuracy: 0.6014 - val_loss: 0.4573 - val_accuracy: 0.5500\n",
      "Epoch 1324/1600\n",
      "680/680 [==============================] - 1s 834us/step - loss: 0.1407 - accuracy: 0.6029 - val_loss: 0.4689 - val_accuracy: 0.5559\n",
      "Epoch 1325/1600\n",
      "680/680 [==============================] - 1s 833us/step - loss: 0.1370 - accuracy: 0.6036 - val_loss: 0.4464 - val_accuracy: 0.5559\n",
      "Epoch 1326/1600\n",
      "680/680 [==============================] - 1s 838us/step - loss: 0.1383 - accuracy: 0.6058 - val_loss: 0.4566 - val_accuracy: 0.5559\n",
      "Epoch 1327/1600\n",
      "680/680 [==============================] - 1s 839us/step - loss: 0.1426 - accuracy: 0.6010 - val_loss: 0.4835 - val_accuracy: 0.5529\n",
      "Epoch 1328/1600\n",
      "680/680 [==============================] - 1s 797us/step - loss: 0.1287 - accuracy: 0.6043 - val_loss: 0.4907 - val_accuracy: 0.5618\n",
      "Epoch 1329/1600\n",
      "680/680 [==============================] - 1s 804us/step - loss: 0.1370 - accuracy: 0.6058 - val_loss: 0.4642 - val_accuracy: 0.5588\n",
      "Epoch 1330/1600\n",
      "680/680 [==============================] - 1s 818us/step - loss: 0.1638 - accuracy: 0.5966 - val_loss: 0.4831 - val_accuracy: 0.5353\n",
      "Epoch 1331/1600\n",
      "680/680 [==============================] - 1s 803us/step - loss: 0.1277 - accuracy: 0.6054 - val_loss: 0.4982 - val_accuracy: 0.5500\n",
      "Epoch 1332/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1388 - accuracy: 0.6040 - val_loss: 0.4572 - val_accuracy: 0.5441\n",
      "Epoch 1333/1600\n",
      "680/680 [==============================] - 1s 814us/step - loss: 0.1433 - accuracy: 0.6010 - val_loss: 0.4603 - val_accuracy: 0.5618\n",
      "Epoch 1334/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1477 - accuracy: 0.5999 - val_loss: 0.4676 - val_accuracy: 0.5529\n",
      "Epoch 1335/1600\n",
      "680/680 [==============================] - 1s 779us/step - loss: 0.1397 - accuracy: 0.6032 - val_loss: 0.4805 - val_accuracy: 0.5647\n",
      "Epoch 1336/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1387 - accuracy: 0.6054 - val_loss: 0.4879 - val_accuracy: 0.5441\n",
      "Epoch 1337/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1285 - accuracy: 0.6047 - val_loss: 0.4704 - val_accuracy: 0.5500\n",
      "Epoch 1338/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1465 - accuracy: 0.6036 - val_loss: 0.4722 - val_accuracy: 0.5441\n",
      "Epoch 1339/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1486 - accuracy: 0.6018 - val_loss: 0.5351 - val_accuracy: 0.5618\n",
      "Epoch 1340/1600\n",
      "680/680 [==============================] - 1s 783us/step - loss: 0.1397 - accuracy: 0.6029 - val_loss: 0.4816 - val_accuracy: 0.5618\n",
      "Epoch 1341/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1498 - accuracy: 0.6032 - val_loss: 0.4867 - val_accuracy: 0.5529\n",
      "Epoch 1342/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1257 - accuracy: 0.6077 - val_loss: 0.4696 - val_accuracy: 0.5559\n",
      "Epoch 1343/1600\n",
      "680/680 [==============================] - 1s 781us/step - loss: 0.1407 - accuracy: 0.6025 - val_loss: 0.4571 - val_accuracy: 0.5471\n",
      "Epoch 1344/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1413 - accuracy: 0.6018 - val_loss: 0.4901 - val_accuracy: 0.5588\n",
      "Epoch 1345/1600\n",
      "680/680 [==============================] - 1s 829us/step - loss: 0.1276 - accuracy: 0.6062 - val_loss: 0.4517 - val_accuracy: 0.5618\n",
      "Epoch 1346/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1425 - accuracy: 0.6032 - val_loss: 0.4338 - val_accuracy: 0.5500\n",
      "Epoch 1347/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1532 - accuracy: 0.6010 - val_loss: 0.4341 - val_accuracy: 0.5559\n",
      "Epoch 1348/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1521 - accuracy: 0.5992 - val_loss: 0.4475 - val_accuracy: 0.5559\n",
      "Epoch 1349/1600\n",
      "680/680 [==============================] - 1s 752us/step - loss: 0.1441 - accuracy: 0.6007 - val_loss: 0.4466 - val_accuracy: 0.5588\n",
      "Epoch 1350/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1384 - accuracy: 0.6025 - val_loss: 0.4842 - val_accuracy: 0.5588\n",
      "Epoch 1351/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1385 - accuracy: 0.6036 - val_loss: 0.4352 - val_accuracy: 0.5588\n",
      "Epoch 1352/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1426 - accuracy: 0.6040 - val_loss: 0.4321 - val_accuracy: 0.5618\n",
      "Epoch 1353/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1499 - accuracy: 0.6014 - val_loss: 0.4544 - val_accuracy: 0.5559\n",
      "Epoch 1354/1600\n",
      "680/680 [==============================] - 1s 776us/step - loss: 0.1418 - accuracy: 0.6014 - val_loss: 0.5177 - val_accuracy: 0.5588\n",
      "Epoch 1355/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1390 - accuracy: 0.6054 - val_loss: 0.4512 - val_accuracy: 0.5588\n",
      "Epoch 1356/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1412 - accuracy: 0.6014 - val_loss: 0.4455 - val_accuracy: 0.5559\n",
      "Epoch 1357/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1239 - accuracy: 0.6080 - val_loss: 0.4381 - val_accuracy: 0.5618\n",
      "Epoch 1358/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1454 - accuracy: 0.6010 - val_loss: 0.4706 - val_accuracy: 0.5618\n",
      "Epoch 1359/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1480 - accuracy: 0.6018 - val_loss: 0.5300 - val_accuracy: 0.5529\n",
      "Epoch 1360/1600\n",
      "680/680 [==============================] - 1s 798us/step - loss: 0.1512 - accuracy: 0.6014 - val_loss: 0.4800 - val_accuracy: 0.5500\n",
      "Epoch 1361/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1287 - accuracy: 0.6066 - val_loss: 0.4627 - val_accuracy: 0.5706\n",
      "Epoch 1362/1600\n",
      "680/680 [==============================] - 1s 812us/step - loss: 0.1291 - accuracy: 0.6058 - val_loss: 0.4594 - val_accuracy: 0.5559\n",
      "Epoch 1363/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1393 - accuracy: 0.6029 - val_loss: 0.4749 - val_accuracy: 0.5618\n",
      "Epoch 1364/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1361 - accuracy: 0.6069 - val_loss: 0.4966 - val_accuracy: 0.5618\n",
      "Epoch 1365/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1481 - accuracy: 0.6029 - val_loss: 0.4439 - val_accuracy: 0.5588\n",
      "Epoch 1366/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1420 - accuracy: 0.6032 - val_loss: 0.4541 - val_accuracy: 0.5618\n",
      "Epoch 1367/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1303 - accuracy: 0.6054 - val_loss: 0.4470 - val_accuracy: 0.5676\n",
      "Epoch 1368/1600\n",
      "680/680 [==============================] - 0s 722us/step - loss: 0.1444 - accuracy: 0.6036 - val_loss: 0.4924 - val_accuracy: 0.5618\n",
      "Epoch 1369/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1363 - accuracy: 0.6066 - val_loss: 0.4708 - val_accuracy: 0.5529\n",
      "Epoch 1370/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1391 - accuracy: 0.6036 - val_loss: 0.4565 - val_accuracy: 0.5559\n",
      "Epoch 1371/1600\n",
      "680/680 [==============================] - 1s 799us/step - loss: 0.1398 - accuracy: 0.6018 - val_loss: 0.4614 - val_accuracy: 0.5618\n",
      "Epoch 1372/1600\n",
      "680/680 [==============================] - 1s 757us/step - loss: 0.1441 - accuracy: 0.6025 - val_loss: 0.4564 - val_accuracy: 0.5441\n",
      "Epoch 1373/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1463 - accuracy: 0.6014 - val_loss: 0.4719 - val_accuracy: 0.5441\n",
      "Epoch 1374/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1293 - accuracy: 0.6040 - val_loss: 0.4717 - val_accuracy: 0.5559\n",
      "Epoch 1375/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1263 - accuracy: 0.6080 - val_loss: 0.4079 - val_accuracy: 0.5588\n",
      "Epoch 1376/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1476 - accuracy: 0.6025 - val_loss: 0.4162 - val_accuracy: 0.5559\n",
      "Epoch 1377/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 798us/step - loss: 0.1554 - accuracy: 0.6018 - val_loss: 0.4562 - val_accuracy: 0.5559\n",
      "Epoch 1378/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1558 - accuracy: 0.5999 - val_loss: 0.4121 - val_accuracy: 0.5500\n",
      "Epoch 1379/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1382 - accuracy: 0.6032 - val_loss: 0.4481 - val_accuracy: 0.5588\n",
      "Epoch 1380/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1494 - accuracy: 0.6003 - val_loss: 0.4666 - val_accuracy: 0.5588\n",
      "Epoch 1381/1600\n",
      "680/680 [==============================] - 1s 822us/step - loss: 0.1333 - accuracy: 0.6051 - val_loss: 0.4836 - val_accuracy: 0.5529\n",
      "Epoch 1382/1600\n",
      "680/680 [==============================] - 1s 823us/step - loss: 0.1290 - accuracy: 0.6025 - val_loss: 0.4592 - val_accuracy: 0.5500\n",
      "Epoch 1383/1600\n",
      "680/680 [==============================] - 1s 801us/step - loss: 0.1363 - accuracy: 0.6062 - val_loss: 0.4788 - val_accuracy: 0.5471\n",
      "Epoch 1384/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1292 - accuracy: 0.6043 - val_loss: 0.4515 - val_accuracy: 0.5529\n",
      "Epoch 1385/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1517 - accuracy: 0.5996 - val_loss: 0.5057 - val_accuracy: 0.5471\n",
      "Epoch 1386/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1475 - accuracy: 0.6021 - val_loss: 0.4363 - val_accuracy: 0.5559\n",
      "Epoch 1387/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1300 - accuracy: 0.6073 - val_loss: 0.4552 - val_accuracy: 0.5559\n",
      "Epoch 1388/1600\n",
      "680/680 [==============================] - 0s 686us/step - loss: 0.1447 - accuracy: 0.6040 - val_loss: 0.4942 - val_accuracy: 0.5471\n",
      "Epoch 1389/1600\n",
      "680/680 [==============================] - 1s 738us/step - loss: 0.1401 - accuracy: 0.6014 - val_loss: 0.4617 - val_accuracy: 0.5647\n",
      "Epoch 1390/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1500 - accuracy: 0.6010 - val_loss: 0.4905 - val_accuracy: 0.5500\n",
      "Epoch 1391/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1416 - accuracy: 0.6047 - val_loss: 0.5283 - val_accuracy: 0.5588\n",
      "Epoch 1392/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1341 - accuracy: 0.6054 - val_loss: 0.5297 - val_accuracy: 0.5735\n",
      "Epoch 1393/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1460 - accuracy: 0.6010 - val_loss: 0.4563 - val_accuracy: 0.5559\n",
      "Epoch 1394/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1477 - accuracy: 0.6003 - val_loss: 0.4652 - val_accuracy: 0.5588\n",
      "Epoch 1395/1600\n",
      "680/680 [==============================] - 1s 797us/step - loss: 0.1418 - accuracy: 0.6036 - val_loss: 0.4567 - val_accuracy: 0.5588\n",
      "Epoch 1396/1600\n",
      "680/680 [==============================] - 1s 873us/step - loss: 0.1535 - accuracy: 0.5996 - val_loss: 0.4736 - val_accuracy: 0.5647\n",
      "Epoch 1397/1600\n",
      "680/680 [==============================] - 1s 855us/step - loss: 0.1352 - accuracy: 0.6047 - val_loss: 0.4844 - val_accuracy: 0.5559\n",
      "Epoch 1398/1600\n",
      "680/680 [==============================] - 1s 823us/step - loss: 0.1339 - accuracy: 0.6051 - val_loss: 0.4560 - val_accuracy: 0.5559\n",
      "Epoch 1399/1600\n",
      "680/680 [==============================] - 1s 836us/step - loss: 0.1466 - accuracy: 0.6032 - val_loss: 0.4319 - val_accuracy: 0.5618\n",
      "Epoch 1400/1600\n",
      "680/680 [==============================] - 1s 831us/step - loss: 0.1313 - accuracy: 0.6051 - val_loss: 0.4615 - val_accuracy: 0.5618\n",
      "Epoch 1401/1600\n",
      "680/680 [==============================] - 1s 822us/step - loss: 0.1381 - accuracy: 0.6054 - val_loss: 0.4539 - val_accuracy: 0.5529\n",
      "Epoch 1402/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1362 - accuracy: 0.6036 - val_loss: 0.4662 - val_accuracy: 0.5471\n",
      "Epoch 1403/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1634 - accuracy: 0.6007 - val_loss: 0.4566 - val_accuracy: 0.5412\n",
      "Epoch 1404/1600\n",
      "680/680 [==============================] - 1s 761us/step - loss: 0.1501 - accuracy: 0.6018 - val_loss: 0.4383 - val_accuracy: 0.5500\n",
      "Epoch 1405/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1524 - accuracy: 0.6025 - val_loss: 0.4467 - val_accuracy: 0.5500\n",
      "Epoch 1406/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1351 - accuracy: 0.6058 - val_loss: 0.4815 - val_accuracy: 0.5647\n",
      "Epoch 1407/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1378 - accuracy: 0.6036 - val_loss: 0.4683 - val_accuracy: 0.5500\n",
      "Epoch 1408/1600\n",
      "680/680 [==============================] - 0s 684us/step - loss: 0.1305 - accuracy: 0.6069 - val_loss: 0.4307 - val_accuracy: 0.5529\n",
      "Epoch 1409/1600\n",
      "680/680 [==============================] - 0s 703us/step - loss: 0.1265 - accuracy: 0.6054 - val_loss: 0.4755 - val_accuracy: 0.5647\n",
      "Epoch 1410/1600\n",
      "680/680 [==============================] - 0s 719us/step - loss: 0.1496 - accuracy: 0.5999 - val_loss: 0.4815 - val_accuracy: 0.5588\n",
      "Epoch 1411/1600\n",
      "680/680 [==============================] - 1s 752us/step - loss: 0.1318 - accuracy: 0.6043 - val_loss: 0.4883 - val_accuracy: 0.5559\n",
      "Epoch 1412/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1397 - accuracy: 0.6054 - val_loss: 0.4922 - val_accuracy: 0.5559\n",
      "Epoch 1413/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1541 - accuracy: 0.6010 - val_loss: 0.4873 - val_accuracy: 0.5559\n",
      "Epoch 1414/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1453 - accuracy: 0.6010 - val_loss: 0.4838 - val_accuracy: 0.5588\n",
      "Epoch 1415/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1304 - accuracy: 0.6066 - val_loss: 0.4445 - val_accuracy: 0.5559\n",
      "Epoch 1416/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1412 - accuracy: 0.6047 - val_loss: 0.4516 - val_accuracy: 0.5559\n",
      "Epoch 1417/1600\n",
      "680/680 [==============================] - 1s 797us/step - loss: 0.1333 - accuracy: 0.6058 - val_loss: 0.4676 - val_accuracy: 0.5559\n",
      "Epoch 1418/1600\n",
      "680/680 [==============================] - 1s 798us/step - loss: 0.1476 - accuracy: 0.6018 - val_loss: 0.4669 - val_accuracy: 0.5618\n",
      "Epoch 1419/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1449 - accuracy: 0.6010 - val_loss: 0.4883 - val_accuracy: 0.5471\n",
      "Epoch 1420/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1509 - accuracy: 0.6018 - val_loss: 0.4779 - val_accuracy: 0.5647\n",
      "Epoch 1421/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1268 - accuracy: 0.6054 - val_loss: 0.4446 - val_accuracy: 0.5588\n",
      "Epoch 1422/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1614 - accuracy: 0.5988 - val_loss: 0.4795 - val_accuracy: 0.5618\n",
      "Epoch 1423/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1499 - accuracy: 0.5996 - val_loss: 0.4943 - val_accuracy: 0.5588\n",
      "Epoch 1424/1600\n",
      "680/680 [==============================] - 1s 780us/step - loss: 0.1291 - accuracy: 0.6054 - val_loss: 0.4851 - val_accuracy: 0.5529\n",
      "Epoch 1425/1600\n",
      "680/680 [==============================] - 0s 688us/step - loss: 0.1400 - accuracy: 0.6036 - val_loss: 0.4821 - val_accuracy: 0.5529\n",
      "Epoch 1426/1600\n",
      "680/680 [==============================] - 0s 715us/step - loss: 0.1436 - accuracy: 0.5999 - val_loss: 0.4584 - val_accuracy: 0.5618\n",
      "Epoch 1427/1600\n",
      "680/680 [==============================] - 0s 702us/step - loss: 0.1285 - accuracy: 0.6066 - val_loss: 0.4110 - val_accuracy: 0.5559\n",
      "Epoch 1428/1600\n",
      "680/680 [==============================] - 0s 716us/step - loss: 0.1348 - accuracy: 0.6032 - val_loss: 0.4479 - val_accuracy: 0.5618\n",
      "Epoch 1429/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1398 - accuracy: 0.6051 - val_loss: 0.4381 - val_accuracy: 0.5559\n",
      "Epoch 1430/1600\n",
      "680/680 [==============================] - 0s 713us/step - loss: 0.1318 - accuracy: 0.6043 - val_loss: 0.4805 - val_accuracy: 0.5529\n",
      "Epoch 1431/1600\n",
      "680/680 [==============================] - 0s 709us/step - loss: 0.1485 - accuracy: 0.6010 - val_loss: 0.5131 - val_accuracy: 0.5529\n",
      "Epoch 1432/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 0s 716us/step - loss: 0.1315 - accuracy: 0.6051 - val_loss: 0.4565 - val_accuracy: 0.5588\n",
      "Epoch 1433/1600\n",
      "680/680 [==============================] - 0s 708us/step - loss: 0.1285 - accuracy: 0.6054 - val_loss: 0.4840 - val_accuracy: 0.5618\n",
      "Epoch 1434/1600\n",
      "680/680 [==============================] - 0s 723us/step - loss: 0.1365 - accuracy: 0.6040 - val_loss: 0.4658 - val_accuracy: 0.5500\n",
      "Epoch 1435/1600\n",
      "680/680 [==============================] - 0s 712us/step - loss: 0.1362 - accuracy: 0.6040 - val_loss: 0.4952 - val_accuracy: 0.5500\n",
      "Epoch 1436/1600\n",
      "680/680 [==============================] - 0s 707us/step - loss: 0.1535 - accuracy: 0.6018 - val_loss: 0.4440 - val_accuracy: 0.5559\n",
      "Epoch 1437/1600\n",
      "680/680 [==============================] - 0s 710us/step - loss: 0.1428 - accuracy: 0.6032 - val_loss: 0.4412 - val_accuracy: 0.5618\n",
      "Epoch 1438/1600\n",
      "680/680 [==============================] - 0s 713us/step - loss: 0.1394 - accuracy: 0.6032 - val_loss: 0.4543 - val_accuracy: 0.5529\n",
      "Epoch 1439/1600\n",
      "680/680 [==============================] - 0s 711us/step - loss: 0.1152 - accuracy: 0.6110 - val_loss: 0.4583 - val_accuracy: 0.5559\n",
      "Epoch 1440/1600\n",
      "680/680 [==============================] - 0s 721us/step - loss: 0.1406 - accuracy: 0.6014 - val_loss: 0.4540 - val_accuracy: 0.5382\n",
      "Epoch 1441/1600\n",
      "680/680 [==============================] - 0s 703us/step - loss: 0.1413 - accuracy: 0.6032 - val_loss: 0.4579 - val_accuracy: 0.5618\n",
      "Epoch 1442/1600\n",
      "680/680 [==============================] - 0s 707us/step - loss: 0.1514 - accuracy: 0.6029 - val_loss: 0.4374 - val_accuracy: 0.5618\n",
      "Epoch 1443/1600\n",
      "680/680 [==============================] - 0s 710us/step - loss: 0.1463 - accuracy: 0.6025 - val_loss: 0.5041 - val_accuracy: 0.5559\n",
      "Epoch 1444/1600\n",
      "680/680 [==============================] - 0s 716us/step - loss: 0.1622 - accuracy: 0.6021 - val_loss: 0.4546 - val_accuracy: 0.5706\n",
      "Epoch 1445/1600\n",
      "680/680 [==============================] - 0s 704us/step - loss: 0.1384 - accuracy: 0.6036 - val_loss: 0.4887 - val_accuracy: 0.5471\n",
      "Epoch 1446/1600\n",
      "680/680 [==============================] - 1s 756us/step - loss: 0.1400 - accuracy: 0.6036 - val_loss: 0.4314 - val_accuracy: 0.5529\n",
      "Epoch 1447/1600\n",
      "680/680 [==============================] - 1s 752us/step - loss: 0.1324 - accuracy: 0.6051 - val_loss: 0.4642 - val_accuracy: 0.5588\n",
      "Epoch 1448/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1804 - accuracy: 0.5933 - val_loss: 0.4936 - val_accuracy: 0.5618\n",
      "Epoch 1449/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1287 - accuracy: 0.6051 - val_loss: 0.4749 - val_accuracy: 0.5471\n",
      "Epoch 1450/1600\n",
      "680/680 [==============================] - 1s 782us/step - loss: 0.1342 - accuracy: 0.6047 - val_loss: 0.4750 - val_accuracy: 0.5559\n",
      "Epoch 1451/1600\n",
      "680/680 [==============================] - 1s 792us/step - loss: 0.1358 - accuracy: 0.6051 - val_loss: 0.4651 - val_accuracy: 0.5529\n",
      "Epoch 1452/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1334 - accuracy: 0.6047 - val_loss: 0.4621 - val_accuracy: 0.5529\n",
      "Epoch 1453/1600\n",
      "680/680 [==============================] - 1s 799us/step - loss: 0.1438 - accuracy: 0.6010 - val_loss: 0.4914 - val_accuracy: 0.5588\n",
      "Epoch 1454/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1337 - accuracy: 0.6088 - val_loss: 0.4414 - val_accuracy: 0.5529\n",
      "Epoch 1455/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1525 - accuracy: 0.6003 - val_loss: 0.4731 - val_accuracy: 0.5382\n",
      "Epoch 1456/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1354 - accuracy: 0.6029 - val_loss: 0.4451 - val_accuracy: 0.5647\n",
      "Epoch 1457/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1346 - accuracy: 0.6047 - val_loss: 0.4869 - val_accuracy: 0.5529\n",
      "Epoch 1458/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1378 - accuracy: 0.6043 - val_loss: 0.4726 - val_accuracy: 0.5471\n",
      "Epoch 1459/1600\n",
      "680/680 [==============================] - 1s 761us/step - loss: 0.1441 - accuracy: 0.6029 - val_loss: 0.4618 - val_accuracy: 0.5500\n",
      "Epoch 1460/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1321 - accuracy: 0.6036 - val_loss: 0.4572 - val_accuracy: 0.5588\n",
      "Epoch 1461/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1336 - accuracy: 0.6066 - val_loss: 0.4523 - val_accuracy: 0.5647\n",
      "Epoch 1462/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1293 - accuracy: 0.6054 - val_loss: 0.4797 - val_accuracy: 0.5559\n",
      "Epoch 1463/1600\n",
      "680/680 [==============================] - 1s 751us/step - loss: 0.1571 - accuracy: 0.6021 - val_loss: 0.5127 - val_accuracy: 0.5529\n",
      "Epoch 1464/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1552 - accuracy: 0.5996 - val_loss: 0.5205 - val_accuracy: 0.5676\n",
      "Epoch 1465/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1502 - accuracy: 0.6021 - val_loss: 0.5135 - val_accuracy: 0.5588\n",
      "Epoch 1466/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1340 - accuracy: 0.6040 - val_loss: 0.4849 - val_accuracy: 0.5500\n",
      "Epoch 1467/1600\n",
      "680/680 [==============================] - 1s 782us/step - loss: 0.1334 - accuracy: 0.6051 - val_loss: 0.4960 - val_accuracy: 0.5618\n",
      "Epoch 1468/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1349 - accuracy: 0.6058 - val_loss: 0.4705 - val_accuracy: 0.5559\n",
      "Epoch 1469/1600\n",
      "680/680 [==============================] - 1s 797us/step - loss: 0.1532 - accuracy: 0.5999 - val_loss: 0.4493 - val_accuracy: 0.5529\n",
      "Epoch 1470/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1476 - accuracy: 0.6029 - val_loss: 0.4768 - val_accuracy: 0.5559\n",
      "Epoch 1471/1600\n",
      "680/680 [==============================] - 1s 785us/step - loss: 0.1320 - accuracy: 0.6047 - val_loss: 0.4477 - val_accuracy: 0.5588\n",
      "Epoch 1472/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1375 - accuracy: 0.6025 - val_loss: 0.4524 - val_accuracy: 0.5647\n",
      "Epoch 1473/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1427 - accuracy: 0.6040 - val_loss: 0.4593 - val_accuracy: 0.5588\n",
      "Epoch 1474/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1315 - accuracy: 0.6047 - val_loss: 0.4650 - val_accuracy: 0.5647\n",
      "Epoch 1475/1600\n",
      "680/680 [==============================] - 1s 801us/step - loss: 0.1400 - accuracy: 0.6029 - val_loss: 0.4483 - val_accuracy: 0.5559\n",
      "Epoch 1476/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1425 - accuracy: 0.6040 - val_loss: 0.4223 - val_accuracy: 0.5559\n",
      "Epoch 1477/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1370 - accuracy: 0.6047 - val_loss: 0.4664 - val_accuracy: 0.5559\n",
      "Epoch 1478/1600\n",
      "680/680 [==============================] - 1s 798us/step - loss: 0.1441 - accuracy: 0.6018 - val_loss: 0.4539 - val_accuracy: 0.5647\n",
      "Epoch 1479/1600\n",
      "680/680 [==============================] - 1s 797us/step - loss: 0.1450 - accuracy: 0.6010 - val_loss: 0.4725 - val_accuracy: 0.5471\n",
      "Epoch 1480/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1389 - accuracy: 0.6032 - val_loss: 0.4592 - val_accuracy: 0.5529\n",
      "Epoch 1481/1600\n",
      "680/680 [==============================] - 1s 798us/step - loss: 0.1474 - accuracy: 0.6025 - val_loss: 0.4556 - val_accuracy: 0.5735\n",
      "Epoch 1482/1600\n",
      "680/680 [==============================] - 1s 796us/step - loss: 0.1356 - accuracy: 0.6040 - val_loss: 0.4463 - val_accuracy: 0.5618\n",
      "Epoch 1483/1600\n",
      "680/680 [==============================] - 0s 702us/step - loss: 0.1345 - accuracy: 0.6036 - val_loss: 0.4590 - val_accuracy: 0.5529\n",
      "Epoch 1484/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1359 - accuracy: 0.6054 - val_loss: 0.4657 - val_accuracy: 0.5529\n",
      "Epoch 1485/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1357 - accuracy: 0.6021 - val_loss: 0.4617 - val_accuracy: 0.5471\n",
      "Epoch 1486/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1413 - accuracy: 0.6018 - val_loss: 0.4365 - val_accuracy: 0.5588\n",
      "Epoch 1487/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 746us/step - loss: 0.1337 - accuracy: 0.6036 - val_loss: 0.4329 - val_accuracy: 0.5471\n",
      "Epoch 1488/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1518 - accuracy: 0.6010 - val_loss: 0.4644 - val_accuracy: 0.5529\n",
      "Epoch 1489/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1219 - accuracy: 0.6091 - val_loss: 0.4551 - val_accuracy: 0.5471\n",
      "Epoch 1490/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1482 - accuracy: 0.6014 - val_loss: 0.4444 - val_accuracy: 0.5618\n",
      "Epoch 1491/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1334 - accuracy: 0.6058 - val_loss: 0.4570 - val_accuracy: 0.5588\n",
      "Epoch 1492/1600\n",
      "680/680 [==============================] - 1s 800us/step - loss: 0.1457 - accuracy: 0.6025 - val_loss: 0.4965 - val_accuracy: 0.5559\n",
      "Epoch 1493/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1484 - accuracy: 0.5992 - val_loss: 0.4943 - val_accuracy: 0.5529\n",
      "Epoch 1494/1600\n",
      "680/680 [==============================] - 1s 780us/step - loss: 0.1490 - accuracy: 0.5996 - val_loss: 0.4576 - val_accuracy: 0.5500\n",
      "Epoch 1495/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1310 - accuracy: 0.6043 - val_loss: 0.4164 - val_accuracy: 0.5647\n",
      "Epoch 1496/1600\n",
      "680/680 [==============================] - 1s 738us/step - loss: 0.1294 - accuracy: 0.6066 - val_loss: 0.4320 - val_accuracy: 0.5588\n",
      "Epoch 1497/1600\n",
      "680/680 [==============================] - 1s 754us/step - loss: 0.1374 - accuracy: 0.6040 - val_loss: 0.4605 - val_accuracy: 0.5529\n",
      "Epoch 1498/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1321 - accuracy: 0.6047 - val_loss: 0.5126 - val_accuracy: 0.5676\n",
      "Epoch 1499/1600\n",
      "680/680 [==============================] - 1s 743us/step - loss: 0.1437 - accuracy: 0.6010 - val_loss: 0.4735 - val_accuracy: 0.5529\n",
      "Epoch 1500/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1446 - accuracy: 0.6032 - val_loss: 0.4931 - val_accuracy: 0.5588\n",
      "Epoch 1501/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1324 - accuracy: 0.6029 - val_loss: 0.4685 - val_accuracy: 0.5559\n",
      "Epoch 1502/1600\n",
      "680/680 [==============================] - 1s 741us/step - loss: 0.1300 - accuracy: 0.6054 - val_loss: 0.4423 - val_accuracy: 0.5559\n",
      "Epoch 1503/1600\n",
      "680/680 [==============================] - 1s 755us/step - loss: 0.1401 - accuracy: 0.6040 - val_loss: 0.4515 - val_accuracy: 0.5471\n",
      "Epoch 1504/1600\n",
      "680/680 [==============================] - 1s 737us/step - loss: 0.1453 - accuracy: 0.6003 - val_loss: 0.4258 - val_accuracy: 0.5529\n",
      "Epoch 1505/1600\n",
      "680/680 [==============================] - 1s 738us/step - loss: 0.1539 - accuracy: 0.5992 - val_loss: 0.4712 - val_accuracy: 0.5588\n",
      "Epoch 1506/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1308 - accuracy: 0.6077 - val_loss: 0.4553 - val_accuracy: 0.5500\n",
      "Epoch 1507/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1320 - accuracy: 0.6047 - val_loss: 0.4651 - val_accuracy: 0.5441\n",
      "Epoch 1508/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1531 - accuracy: 0.5992 - val_loss: 0.4913 - val_accuracy: 0.5441\n",
      "Epoch 1509/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1467 - accuracy: 0.6018 - val_loss: 0.4642 - val_accuracy: 0.5559\n",
      "Epoch 1510/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1266 - accuracy: 0.6062 - val_loss: 0.4415 - val_accuracy: 0.5559\n",
      "Epoch 1511/1600\n",
      "680/680 [==============================] - 1s 740us/step - loss: 0.1337 - accuracy: 0.6036 - val_loss: 0.4946 - val_accuracy: 0.5618\n",
      "Epoch 1512/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1313 - accuracy: 0.6058 - val_loss: 0.4539 - val_accuracy: 0.5588\n",
      "Epoch 1513/1600\n",
      "680/680 [==============================] - 1s 740us/step - loss: 0.1514 - accuracy: 0.6010 - val_loss: 0.4441 - val_accuracy: 0.5618\n",
      "Epoch 1514/1600\n",
      "680/680 [==============================] - 1s 742us/step - loss: 0.1362 - accuracy: 0.6036 - val_loss: 0.5338 - val_accuracy: 0.5647\n",
      "Epoch 1515/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1442 - accuracy: 0.6021 - val_loss: 0.4753 - val_accuracy: 0.5353\n",
      "Epoch 1516/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1463 - accuracy: 0.6025 - val_loss: 0.4813 - val_accuracy: 0.5471\n",
      "Epoch 1517/1600\n",
      "680/680 [==============================] - 1s 787us/step - loss: 0.1376 - accuracy: 0.6025 - val_loss: 0.4985 - val_accuracy: 0.5471\n",
      "Epoch 1518/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1339 - accuracy: 0.6054 - val_loss: 0.4525 - val_accuracy: 0.5500\n",
      "Epoch 1519/1600\n",
      "680/680 [==============================] - 1s 818us/step - loss: 0.1375 - accuracy: 0.6054 - val_loss: 0.4971 - val_accuracy: 0.5618\n",
      "Epoch 1520/1600\n",
      "680/680 [==============================] - 0s 673us/step - loss: 0.1558 - accuracy: 0.6007 - val_loss: 0.4532 - val_accuracy: 0.5529\n",
      "Epoch 1521/1600\n",
      "680/680 [==============================] - 0s 697us/step - loss: 0.1401 - accuracy: 0.6032 - val_loss: 0.4547 - val_accuracy: 0.5588\n",
      "Epoch 1522/1600\n",
      "680/680 [==============================] - 1s 786us/step - loss: 0.1363 - accuracy: 0.6043 - val_loss: 0.4811 - val_accuracy: 0.5529\n",
      "Epoch 1523/1600\n",
      "680/680 [==============================] - 0s 687us/step - loss: 0.1383 - accuracy: 0.6036 - val_loss: 0.4371 - val_accuracy: 0.5676\n",
      "Epoch 1524/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1513 - accuracy: 0.6018 - val_loss: 0.4642 - val_accuracy: 0.5618\n",
      "Epoch 1525/1600\n",
      "680/680 [==============================] - 0s 679us/step - loss: 0.1437 - accuracy: 0.6043 - val_loss: 0.4620 - val_accuracy: 0.5618\n",
      "Epoch 1526/1600\n",
      "680/680 [==============================] - 1s 861us/step - loss: 0.1542 - accuracy: 0.6010 - val_loss: 0.4459 - val_accuracy: 0.5618\n",
      "Epoch 1527/1600\n",
      "680/680 [==============================] - 1s 824us/step - loss: 0.1306 - accuracy: 0.6047 - val_loss: 0.4554 - val_accuracy: 0.5647\n",
      "Epoch 1528/1600\n",
      "680/680 [==============================] - 0s 705us/step - loss: 0.1263 - accuracy: 0.6066 - val_loss: 0.4856 - val_accuracy: 0.5618\n",
      "Epoch 1529/1600\n",
      "680/680 [==============================] - 1s 751us/step - loss: 0.1295 - accuracy: 0.6047 - val_loss: 0.4555 - val_accuracy: 0.5500\n",
      "Epoch 1530/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1407 - accuracy: 0.6036 - val_loss: 0.4316 - val_accuracy: 0.5618\n",
      "Epoch 1531/1600\n",
      "680/680 [==============================] - 1s 746us/step - loss: 0.1430 - accuracy: 0.6018 - val_loss: 0.4574 - val_accuracy: 0.5588\n",
      "Epoch 1532/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1394 - accuracy: 0.6021 - val_loss: 0.4719 - val_accuracy: 0.5588\n",
      "Epoch 1533/1600\n",
      "680/680 [==============================] - 1s 756us/step - loss: 0.1469 - accuracy: 0.6007 - val_loss: 0.4368 - val_accuracy: 0.5500\n",
      "Epoch 1534/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1425 - accuracy: 0.6025 - val_loss: 0.4526 - val_accuracy: 0.5647\n",
      "Epoch 1535/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1410 - accuracy: 0.6032 - val_loss: 0.4374 - val_accuracy: 0.5441\n",
      "Epoch 1536/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1532 - accuracy: 0.5996 - val_loss: 0.4531 - val_accuracy: 0.5559\n",
      "Epoch 1537/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1345 - accuracy: 0.6047 - val_loss: 0.4567 - val_accuracy: 0.5588\n",
      "Epoch 1538/1600\n",
      "680/680 [==============================] - 1s 795us/step - loss: 0.1305 - accuracy: 0.6047 - val_loss: 0.4762 - val_accuracy: 0.5529\n",
      "Epoch 1539/1600\n",
      "680/680 [==============================] - 1s 794us/step - loss: 0.1363 - accuracy: 0.6043 - val_loss: 0.4672 - val_accuracy: 0.5559\n",
      "Epoch 1540/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1562 - accuracy: 0.5992 - val_loss: 0.4435 - val_accuracy: 0.5618\n",
      "Epoch 1541/1600\n",
      "680/680 [==============================] - 1s 753us/step - loss: 0.1432 - accuracy: 0.6021 - val_loss: 0.4587 - val_accuracy: 0.5647\n",
      "Epoch 1542/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 0s 700us/step - loss: 0.1221 - accuracy: 0.6077 - val_loss: 0.4403 - val_accuracy: 0.5588\n",
      "Epoch 1543/1600\n",
      "680/680 [==============================] - 0s 717us/step - loss: 0.1540 - accuracy: 0.6003 - val_loss: 0.4215 - val_accuracy: 0.5559\n",
      "Epoch 1544/1600\n",
      "680/680 [==============================] - 1s 761us/step - loss: 0.1416 - accuracy: 0.6032 - val_loss: 0.4374 - val_accuracy: 0.5559\n",
      "Epoch 1545/1600\n",
      "680/680 [==============================] - 1s 755us/step - loss: 0.1310 - accuracy: 0.6054 - val_loss: 0.4361 - val_accuracy: 0.5588\n",
      "Epoch 1546/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1388 - accuracy: 0.6043 - val_loss: 0.4464 - val_accuracy: 0.5559\n",
      "Epoch 1547/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1382 - accuracy: 0.6036 - val_loss: 0.4548 - val_accuracy: 0.5647\n",
      "Epoch 1548/1600\n",
      "680/680 [==============================] - 1s 745us/step - loss: 0.1428 - accuracy: 0.6029 - val_loss: 0.4521 - val_accuracy: 0.5647\n",
      "Epoch 1549/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1435 - accuracy: 0.6029 - val_loss: 0.4285 - val_accuracy: 0.5618\n",
      "Epoch 1550/1600\n",
      "680/680 [==============================] - 1s 833us/step - loss: 0.1474 - accuracy: 0.6014 - val_loss: 0.4452 - val_accuracy: 0.5588\n",
      "Epoch 1551/1600\n",
      "680/680 [==============================] - 1s 803us/step - loss: 0.1301 - accuracy: 0.6054 - val_loss: 0.4510 - val_accuracy: 0.5559\n",
      "Epoch 1552/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1281 - accuracy: 0.6054 - val_loss: 0.4403 - val_accuracy: 0.5588\n",
      "Epoch 1553/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1467 - accuracy: 0.6018 - val_loss: 0.4776 - val_accuracy: 0.5529\n",
      "Epoch 1554/1600\n",
      "680/680 [==============================] - 1s 809us/step - loss: 0.1287 - accuracy: 0.6040 - val_loss: 0.4466 - val_accuracy: 0.5588\n",
      "Epoch 1555/1600\n",
      "680/680 [==============================] - 0s 688us/step - loss: 0.1455 - accuracy: 0.5992 - val_loss: 0.4559 - val_accuracy: 0.5618\n",
      "Epoch 1556/1600\n",
      "680/680 [==============================] - 1s 763us/step - loss: 0.1240 - accuracy: 0.6084 - val_loss: 0.4218 - val_accuracy: 0.5618\n",
      "Epoch 1557/1600\n",
      "680/680 [==============================] - 1s 758us/step - loss: 0.1374 - accuracy: 0.6040 - val_loss: 0.4849 - val_accuracy: 0.5559\n",
      "Epoch 1558/1600\n",
      "680/680 [==============================] - 1s 769us/step - loss: 0.1487 - accuracy: 0.5999 - val_loss: 0.4850 - val_accuracy: 0.5529\n",
      "Epoch 1559/1600\n",
      "680/680 [==============================] - 0s 729us/step - loss: 0.1414 - accuracy: 0.6029 - val_loss: 0.4502 - val_accuracy: 0.5529\n",
      "Epoch 1560/1600\n",
      "680/680 [==============================] - 1s 757us/step - loss: 0.1426 - accuracy: 0.6032 - val_loss: 0.4686 - val_accuracy: 0.5500\n",
      "Epoch 1561/1600\n",
      "680/680 [==============================] - 0s 689us/step - loss: 0.1421 - accuracy: 0.6029 - val_loss: 0.4185 - val_accuracy: 0.5588\n",
      "Epoch 1562/1600\n",
      "680/680 [==============================] - 0s 704us/step - loss: 0.1385 - accuracy: 0.6036 - val_loss: 0.4922 - val_accuracy: 0.5412\n",
      "Epoch 1563/1600\n",
      "680/680 [==============================] - 0s 710us/step - loss: 0.1354 - accuracy: 0.6054 - val_loss: 0.4566 - val_accuracy: 0.5588\n",
      "Epoch 1564/1600\n",
      "680/680 [==============================] - 0s 710us/step - loss: 0.1339 - accuracy: 0.6062 - val_loss: 0.4518 - val_accuracy: 0.5559\n",
      "Epoch 1565/1600\n",
      "680/680 [==============================] - 0s 719us/step - loss: 0.1531 - accuracy: 0.6014 - val_loss: 0.4925 - val_accuracy: 0.5441\n",
      "Epoch 1566/1600\n",
      "680/680 [==============================] - 1s 752us/step - loss: 0.1452 - accuracy: 0.5999 - val_loss: 0.4429 - val_accuracy: 0.5618\n",
      "Epoch 1567/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1321 - accuracy: 0.6054 - val_loss: 0.4932 - val_accuracy: 0.5471\n",
      "Epoch 1568/1600\n",
      "680/680 [==============================] - 1s 752us/step - loss: 0.1523 - accuracy: 0.6018 - val_loss: 0.4277 - val_accuracy: 0.5588\n",
      "Epoch 1569/1600\n",
      "680/680 [==============================] - 1s 747us/step - loss: 0.1379 - accuracy: 0.6043 - val_loss: 0.4579 - val_accuracy: 0.5647\n",
      "Epoch 1570/1600\n",
      "680/680 [==============================] - 1s 753us/step - loss: 0.1282 - accuracy: 0.6069 - val_loss: 0.4785 - val_accuracy: 0.5588\n",
      "Epoch 1571/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1439 - accuracy: 0.6025 - val_loss: 0.4701 - val_accuracy: 0.5647\n",
      "Epoch 1572/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1359 - accuracy: 0.6047 - val_loss: 0.4562 - val_accuracy: 0.5500\n",
      "Epoch 1573/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1448 - accuracy: 0.6029 - val_loss: 0.4671 - val_accuracy: 0.5471\n",
      "Epoch 1574/1600\n",
      "680/680 [==============================] - 1s 798us/step - loss: 0.1491 - accuracy: 0.6003 - val_loss: 0.4439 - val_accuracy: 0.5559\n",
      "Epoch 1575/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1226 - accuracy: 0.6069 - val_loss: 0.4589 - val_accuracy: 0.5676\n",
      "Epoch 1576/1600\n",
      "680/680 [==============================] - 1s 788us/step - loss: 0.1284 - accuracy: 0.6054 - val_loss: 0.4459 - val_accuracy: 0.5500\n",
      "Epoch 1577/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1584 - accuracy: 0.5992 - val_loss: 0.4684 - val_accuracy: 0.5647\n",
      "Epoch 1578/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1423 - accuracy: 0.6047 - val_loss: 0.4578 - val_accuracy: 0.5588\n",
      "Epoch 1579/1600\n",
      "680/680 [==============================] - 1s 800us/step - loss: 0.1409 - accuracy: 0.6040 - val_loss: 0.4484 - val_accuracy: 0.5500\n",
      "Epoch 1580/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1299 - accuracy: 0.6051 - val_loss: 0.4467 - val_accuracy: 0.5647\n",
      "Epoch 1581/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1400 - accuracy: 0.6025 - val_loss: 0.4393 - val_accuracy: 0.5382\n",
      "Epoch 1582/1600\n",
      "680/680 [==============================] - 1s 791us/step - loss: 0.1405 - accuracy: 0.6021 - val_loss: 0.4151 - val_accuracy: 0.5588\n",
      "Epoch 1583/1600\n",
      "680/680 [==============================] - 1s 790us/step - loss: 0.1422 - accuracy: 0.6036 - val_loss: 0.4564 - val_accuracy: 0.5588\n",
      "Epoch 1584/1600\n",
      "680/680 [==============================] - 1s 789us/step - loss: 0.1495 - accuracy: 0.5985 - val_loss: 0.4422 - val_accuracy: 0.5588\n",
      "Epoch 1585/1600\n",
      "680/680 [==============================] - 1s 793us/step - loss: 0.1235 - accuracy: 0.6077 - val_loss: 0.4481 - val_accuracy: 0.5618\n",
      "Epoch 1586/1600\n",
      "680/680 [==============================] - 1s 784us/step - loss: 0.1327 - accuracy: 0.6062 - val_loss: 0.4868 - val_accuracy: 0.5471\n",
      "Epoch 1587/1600\n",
      "680/680 [==============================] - 1s 753us/step - loss: 0.1388 - accuracy: 0.6043 - val_loss: 0.4347 - val_accuracy: 0.5618\n",
      "Epoch 1588/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1413 - accuracy: 0.6032 - val_loss: 0.4681 - val_accuracy: 0.5441\n",
      "Epoch 1589/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1547 - accuracy: 0.6021 - val_loss: 0.4618 - val_accuracy: 0.5441\n",
      "Epoch 1590/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1309 - accuracy: 0.6043 - val_loss: 0.4337 - val_accuracy: 0.5588\n",
      "Epoch 1591/1600\n",
      "680/680 [==============================] - 1s 753us/step - loss: 0.1296 - accuracy: 0.6040 - val_loss: 0.4864 - val_accuracy: 0.5500\n",
      "Epoch 1592/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1562 - accuracy: 0.6025 - val_loss: 0.4827 - val_accuracy: 0.5529\n",
      "Epoch 1593/1600\n",
      "680/680 [==============================] - 1s 748us/step - loss: 0.1492 - accuracy: 0.6032 - val_loss: 0.4725 - val_accuracy: 0.5647\n",
      "Epoch 1594/1600\n",
      "680/680 [==============================] - 1s 750us/step - loss: 0.1454 - accuracy: 0.6021 - val_loss: 0.4159 - val_accuracy: 0.5618\n",
      "Epoch 1595/1600\n",
      "680/680 [==============================] - 1s 744us/step - loss: 0.1280 - accuracy: 0.6054 - val_loss: 0.5057 - val_accuracy: 0.5500\n",
      "Epoch 1596/1600\n",
      "680/680 [==============================] - 1s 758us/step - loss: 0.1478 - accuracy: 0.6021 - val_loss: 0.4573 - val_accuracy: 0.5559\n",
      "Epoch 1597/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 1s 751us/step - loss: 0.1282 - accuracy: 0.6069 - val_loss: 0.4388 - val_accuracy: 0.5500\n",
      "Epoch 1598/1600\n",
      "680/680 [==============================] - 1s 749us/step - loss: 0.1285 - accuracy: 0.6077 - val_loss: 0.4479 - val_accuracy: 0.5588\n",
      "Epoch 1599/1600\n",
      "680/680 [==============================] - 1s 735us/step - loss: 0.1298 - accuracy: 0.6069 - val_loss: 0.4648 - val_accuracy: 0.5500\n",
      "Epoch 1600/1600\n",
      "680/680 [==============================] - 0s 694us/step - loss: 0.1381 - accuracy: 0.6032 - val_loss: 0.4764 - val_accuracy: 0.5559\n"
     ]
    }
   ],
   "source": [
    "classTrain = Transformer.toClassification(activitiesTrain)\n",
    "classVal = Transformer.toClassification(activitiesValidate)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=np.shape(trainData)[1], activation='softmax', kernel_regularizer = keras.regularizers.L2(0.001)))\n",
    "model.add(Dense(150, activation='relu', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model.add(Dense(50, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model.add(Dense(1, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "\n",
    "model.compile(loss='MeanSquaredError', optimizer=\"adam\", metrics=['accuracy'])\n",
    "history = model.fit(trainData, Transformer.toClassification(activitiesTrain), \n",
    "                    validation_data = (valData, classVal), epochs=1600, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d6b5a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In C:\\Users\\Tim\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Tim\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Tim\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In C:\\Users\\Tim\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Tim\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Tim\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Tim\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Tim\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x1e7304a7550>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "classify = Transformer.toClassification(activitiesTrain)\n",
    "constantGuess = (len(classify[classify == 1]))/len(classify)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,8))\n",
    "\n",
    "ax.plot(history.history[\"accuracy\"], color=\"green\")\n",
    "ax.plot(history.history[\"val_accuracy\"], color=\"purple\")\n",
    "ax.axhline(constantGuess, color=\"blue\", linestyle=\"dashed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
