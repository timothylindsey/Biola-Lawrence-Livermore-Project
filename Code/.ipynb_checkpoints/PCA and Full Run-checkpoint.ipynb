{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "induced-exhibition",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lemon\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import Loader\n",
    "import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eastern-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "compoundsTrain, smilesTrain, labelsTrain, compoundDataTrain, activitiesTrain = Loader.getTrain(defaultValue=0)\n",
    "compoundsTest, smilesTest, labelsTest, compoundDataTest, activitiesTest = Loader.getTest(defaultValue=0)\n",
    "compoundsValidate, smilesValidate, labelsValidate, compoundDataValidate, activitiesValidate = Loader.getValidate(defaultValue=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-identifier",
   "metadata": {},
   "source": [
    "### Notes on Principal Component Analysis\n",
    "\n",
    "Does it make sense to run PCA on like-type parts of the data?\n",
    "As there are clearly different \"sets\" of the data per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "important-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(labelsTrain)):\n",
    "#    print(i, '\\b:\\t', labelsTrain[i], compoundDataTrain[35:40, i])\n",
    "    \n",
    "#for all the following ranges begin is inclusive and end is not inclusive\n",
    "#[0:10] docking_score_? (0-9)\n",
    "#[10:20] fusion_score_? (0-9)\n",
    "#[37:45] bcut2d_? (8 in total)\n",
    "#[47:59] chi? (12, odd labels)\n",
    "#[65:79] paoe_vsa? (1-14)\n",
    "#[79:89] smr_vsa? (1-10)\n",
    "#[89:101] slogp_vsa? (1-12)\n",
    "#[102:113] estate_vsa? (1-11)\n",
    "#[113:123] vsa_estate? (1-10)\n",
    "#[143:228] fr_some_chemical? (85 total)\n",
    "\n",
    "#print(labelsTrain[0:10],\"\\n\")\n",
    "#print(labelsTrain[10:20],\"\\n\")\n",
    "#print(labelsTrain[37:45],\"\\n\")\n",
    "#print(labelsTrain[47:59],\"\\n\")\n",
    "#print(labelsTrain[65:79],\"\\n\")\n",
    "#print(labelsTrain[79:89],\"\\n\")\n",
    "#print(labelsTrain[89:101],\"\\n\")\n",
    "#print(labelsTrain[102:113],\"\\n\")\n",
    "#print(labelsTrain[113:123],\"\\n\")\n",
    "#print(labelsTrain[143:228],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-appreciation",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "PCA, modify fusion/docking, normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-compiler",
   "metadata": {},
   "source": [
    "### Apply Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fresh-malta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcut2d retention: [0.99364773]\n",
      "\ttotal: 99.36477273412324%\n",
      "chi retention: [0.9541968]\n",
      "\ttotal: 95.41967968597899%\n",
      "paoe retention: [0.31495127 0.19509321 0.1390042  0.08838179]\n",
      "\ttotal: 73.74304791821297%\n",
      "smr retention: [0.50634726 0.24568468 0.08792319 0.07435827]\n",
      "\ttotal: 91.43133907840088%\n",
      "slogp retention: [0.44620276 0.22134465 0.15753302]\n",
      "\ttotal: 82.50804204396243%\n",
      "estate_vsa retention: [0.29224011 0.18964809 0.14361318]\n",
      "\ttotal: 62.550138683064425%\n",
      "vsa_estate retention: [0.49255304 0.32866098 0.09719245]\n",
      "\ttotal: 91.84064669216781%\n",
      "fr retention: [0.32153498 0.12578177 0.09944384]\n",
      "\ttotal: 54.67605936051208%\n",
      "PCA done, new dimensions: 88\n"
     ]
    }
   ],
   "source": [
    "#reduce dimension through PCA\n",
    "#in order endDims sections are chi, paoe, smr, slogp, estate_vsa, vsa_estate, fr\n",
    "labelsPCA, trainPCA, testPCA, valPCA = Transformer.applyPCA(labelsTrain,  compoundDataTrain, \n",
    "                                                            compoundDataTest, compoundDataValidate,\n",
    "                                                            endDims=[1,1,4,4,3,3,3,3])\n",
    "print(\"PCA done, new dimensions:\", len(labelsPCA))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-stage",
   "metadata": {},
   "source": [
    "### Use either mean or max magnitude of docking and fusion\n",
    "\n",
    "Actually quite similar e.g. for the first 3 rows:\n",
    "\n",
    "fusion max:  $[5.3211, 5.3258, 5.3936]$\n",
    "\n",
    "fusion avg:  $[5.0530, 5.2303, 5.1805]$\n",
    "\n",
    "docking max: $[6.7, 7.2, 7.3]$\n",
    "\n",
    "docking avg: $[-6.51, -6.92, -7.18]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "acceptable-worthy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Fusion and Docking, new dimensions:  70\n"
     ]
    }
   ],
   "source": [
    "#use mean of the docking and fusion\n",
    "labelsMeanPCA, trainMeanPCA = Transformer.useAverageFD(labelsPCA, trainPCA)\n",
    "_, testMeanPCA = Transformer.useAverageFD(labelsPCA, testPCA)\n",
    "_, valMeanPCA = Transformer.useAverageFD(labelsPCA, valPCA)\n",
    "print(\"Mean of Fusion and Docking, new dimensions: \", len(labelsMeanPCA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "italian-looking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max of Fusion and Docking, new dimensions:  70\n"
     ]
    }
   ],
   "source": [
    "#use max magnitude of the docking and fusion\n",
    "labelsMaxPCA, trainMaxPCA = Transformer.useMaxFD(labelsPCA, trainPCA)\n",
    "_, testMaxPCA = Transformer.useMaxFD(labelsPCA, testPCA)\n",
    "_, valMaxPCA = Transformer.useMaxFD(labelsPCA, valPCA)\n",
    "print(\"Max of Fusion and Docking, new dimensions: \", len(labelsMeanPCA))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-crash",
   "metadata": {},
   "source": [
    "### Assign final data and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "annoying-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after transformations are done assign data\n",
    "dataLabels = labelsMeanPCA\n",
    "trainData = trainMeanPCA\n",
    "testData = testMeanPCA\n",
    "valData = valMeanPCA\n",
    "\n",
    "#trainData, testData, valData = Transformer.normalizeData(trainData, testData, valData, newMean=0, newStd=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-appendix",
   "metadata": {},
   "source": [
    "## See modified labels and example data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "institutional-advice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2717, 70)\n",
      "0 \b:\t docking_score_average \t [-7.45 -5.27 -6.89]\n",
      "1 \b:\t fusion_score_average \t [4.70133185 4.37893329 5.03581705]\n",
      "2 \b:\t maxestateindex \t [13.33612248 13.26439532 13.57126317]\n",
      "3 \b:\t minestateindex \t [-3.61530057 -2.03494271 -0.54534029]\n",
      "4 \b:\t maxabsestateindex \t [13.33612248 13.26439532 13.57126317]\n",
      "5 \b:\t minabsestateindex \t [0.01008786 0.00721249 0.04428773]\n",
      "6 \b:\t qed \t [0.52948475 0.06054508 0.88855726]\n",
      "7 \b:\t molwt \t [558.682 754.735 374.412]\n",
      "8 \b:\t heavyatommolwt \t [528.442 708.367 351.228]\n",
      "9 \b:\t exactmolwt \t [558.16067668 754.26841463 374.16418544]\n",
      "10 \b:\t numvalenceelectrons \t [202. 294. 144.]\n",
      "11 \b:\t numradicalelectrons \t [0. 0. 0.]\n",
      "12 \b:\t maxpartialcharge \t [0.24272788 0.33073297 0.25189738]\n",
      "13 \b:\t minpartialcharge \t [-0.37871066 -0.50425989 -0.38102961]\n",
      "14 \b:\t maxabspartialcharge \t [0.37871066 0.50425989 0.38102961]\n",
      "15 \b:\t minabspartialcharge \t [0.24272788 0.33073297 0.25189738]\n",
      "16 \b:\t fpdensitymorgan1 \t [1.05263158 0.86792453 1.48148148]\n",
      "17 \b:\t fpdensitymorgan2 \t [1.78947368 1.56603774 2.33333333]\n",
      "18 \b:\t fpdensitymorgan3 \t [2.47368421 2.22641509 3.07407407]\n",
      "19 \b:\t bcut2d_0 \t [  3.26173569 -12.33649692  -9.9384087 ]\n",
      "20 \b:\t balabanj \t [1.32168766 1.41769949 1.63156178]\n",
      "21 \b:\t bertzct \t [1274.57645896 1571.42426464  920.07304428]\n",
      "22 \b:\t chi_0 \t [ 8.91918469 26.28785189 -6.9620481 ]\n",
      "23 \b:\t hallkieralpha \t [-2.77 -3.63 -2.25]\n",
      "24 \b:\t ipc \t [5.84695028e+08 2.96242609e+11 2.26923699e+06]\n",
      "25 \b:\t kappa1 \t [26.82191768 40.55280473 18.12910884]\n",
      "26 \b:\t kappa2 \t [11.95449995 17.22939772  7.40237683]\n",
      "27 \b:\t kappa3 \t [6.26458103 9.36734076 3.36882274]\n",
      "28 \b:\t labuteasa \t [225.97906605 302.75443422 155.81092032]\n",
      "29 \b:\t paoe_0 \t [ -1.74554223   0.04066402 -34.16329475]\n",
      "30 \b:\t paoe_1 \t [ 8.1907869  42.1673738  10.00461074]\n",
      "31 \b:\t paoe_2 \t [ -9.88095214  29.66335521 -20.67159113]\n",
      "32 \b:\t paoe_3 \t [-4.41108553 54.61974915  5.85255187]\n",
      "33 \b:\t smr_0 \t [5.19892018e-02 5.96448470e+01 2.14002329e+00]\n",
      "34 \b:\t smr_1 \t [ -4.92569008  28.26894806 -16.32271514]\n",
      "35 \b:\t smr_2 \t [33.20765376  8.20883535 -2.88245804]\n",
      "36 \b:\t smr_3 \t [ 1.75275641 37.66271849 13.77776059]\n",
      "37 \b:\t slogp_0 \t [ 3.36871951 59.22053695  7.22251919]\n",
      "38 \b:\t slogp_1 \t [ 17.78062251  15.38321022 -22.94039741]\n",
      "39 \b:\t slogp_2 \t [ 45.16587135 103.09009748   8.5901998 ]\n",
      "40 \b:\t tpsa \t [117.61 294.98  71.63]\n",
      "41 \b:\t estate_vsa_0 \t [  8.83851534 104.03565703   5.95877058]\n",
      "42 \b:\t estate_vsa_1 \t [-17.58212153  31.32457937 -17.75088654]\n",
      "43 \b:\t estate_vsa_2 \t [-18.51245948   6.27101145  -5.80687904]\n",
      "44 \b:\t vsa_estate_0 \t [20.52439812 20.01961028  8.14875689]\n",
      "45 \b:\t vsa_estate_1 \t [ 4.0928029  -1.30040058  0.87997157]\n",
      "46 \b:\t vsa_estate_2 \t [ -8.32739418 104.34372573  -8.21536084]\n",
      "47 \b:\t fractioncsp3 \t [0.42307692 0.57142857 0.5       ]\n",
      "48 \b:\t heavyatomcount \t [38. 53. 27.]\n",
      "49 \b:\t nhohcount \t [ 1. 11.  1.]\n",
      "50 \b:\t nocount \t [10. 18.  6.]\n",
      "51 \b:\t numaliphaticcarbocycles \t [0. 1. 0.]\n",
      "52 \b:\t numaliphaticheterocycles \t [3. 2. 2.]\n",
      "53 \b:\t numaliphaticrings \t [3. 3. 2.]\n",
      "54 \b:\t numaromaticcarbocycles \t [2. 2. 1.]\n",
      "55 \b:\t numaromaticheterocycles \t [0. 0. 1.]\n",
      "56 \b:\t numaromaticrings \t [2. 2. 2.]\n",
      "57 \b:\t numhacceptors \t [ 8. 18.  4.]\n",
      "58 \b:\t numhdonors \t [ 1. 11.  1.]\n",
      "59 \b:\t numheteroatoms \t [12. 18.  7.]\n",
      "60 \b:\t numrotatablebonds \t [ 8. 13.  4.]\n",
      "61 \b:\t numsaturatedcarbocycles \t [0. 1. 0.]\n",
      "62 \b:\t numsaturatedheterocycles \t [3. 2. 1.]\n",
      "63 \b:\t numsaturatedrings \t [3. 3. 1.]\n",
      "64 \b:\t ringcount \t [5. 5. 4.]\n",
      "65 \b:\t mollogp \t [ 2.8468 -1.8874  1.9935]\n",
      "66 \b:\t molmr \t [144.8695 176.9778  97.9877]\n",
      "67 \b:\t fr_0 \t [-0.14129078 -1.252049   -1.18768026]\n",
      "68 \b:\t fr_1 \t [-0.50498111 -5.86270643 -1.38608368]\n",
      "69 \b:\t fr_2 \t [-0.81737828  0.06581783  0.16989608]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(trainData))\n",
    "for i in range(len(dataLabels)):\n",
    "    print(i, \"\\b:\\t\", dataLabels[i], \"\\t\", trainData[0:3,i])\n",
    "\n",
    "#0:\t fusion_score_max \t [6.7 7.2 7.3]\n",
    "#1:\t docking_score_max \t [5.32109785 5.32583714 5.39357948]\n",
    "\n",
    "#0:\t fusion_score_average \t [-6.51 -6.92 -7.18]\n",
    "#1:\t docking_score_average \t [5.05295382 5.2303369  5.1805407 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-sensitivity",
   "metadata": {},
   "source": [
    "# Apply NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "spoken-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.losses import MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fitted-diversity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6198012513801987\n"
     ]
    }
   ],
   "source": [
    "#note the constant guess:\n",
    "classify = Transformer.toClassification(activitiesTrain)\n",
    "constantGuess = (len(classify[classify == 1]))/len(classify)\n",
    "print(constantGuess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fourth-clone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "340/340 [==============================] - 3s 4ms/step - loss: 3.4390 - accuracy: 0.6198 - val_loss: 1.6714 - val_accuracy: 0.6118\n",
      "Epoch 2/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5812 - accuracy: 0.6198 - val_loss: 1.5912 - val_accuracy: 0.6118\n",
      "Epoch 3/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5521 - accuracy: 0.6198 - val_loss: 1.5793 - val_accuracy: 0.6118\n",
      "Epoch 4/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5444 - accuracy: 0.6198 - val_loss: 1.5743 - val_accuracy: 0.6118\n",
      "Epoch 5/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5408 - accuracy: 0.6198 - val_loss: 1.5719 - val_accuracy: 0.6118\n",
      "Epoch 6/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5391 - accuracy: 0.6198 - val_loss: 1.5707 - val_accuracy: 0.6118\n",
      "Epoch 7/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5382 - accuracy: 0.6198 - val_loss: 1.5701 - val_accuracy: 0.6118\n",
      "Epoch 8/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5378 - accuracy: 0.6198 - val_loss: 1.5698 - val_accuracy: 0.6118\n",
      "Epoch 9/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5376 - accuracy: 0.6198 - val_loss: 1.5697 - val_accuracy: 0.6118\n",
      "Epoch 10/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5375 - accuracy: 0.6198 - val_loss: 1.5696 - val_accuracy: 0.6118\n",
      "Epoch 11/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5696 - val_accuracy: 0.6118\n",
      "Epoch 12/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5696 - val_accuracy: 0.6118\n",
      "Epoch 13/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 14/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 15/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 16/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 17/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 18/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 19/5000\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 20/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 21/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 22/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 23/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 24/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 25/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 26/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 27/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 28/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 29/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 30/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 31/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 32/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 33/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 34/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 35/5000\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 36/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 37/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 38/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 39/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 40/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 41/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 42/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 43/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 44/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 45/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 46/5000\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 47/5000\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 48/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 49/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 50/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 51/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 52/5000\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 53/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 54/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 55/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 56/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 57/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 58/5000\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 59/5000\n",
      "340/340 [==============================] - 1s 4ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 60/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 61/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 62/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 63/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 64/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 65/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 66/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 67/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 68/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 69/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 70/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 71/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 72/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 73/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 74/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 75/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 76/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 77/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 78/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 79/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 80/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 81/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 82/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 83/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 84/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 85/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 86/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 87/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 88/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 89/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 90/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 91/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 92/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 93/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 94/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 95/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 96/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 97/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 98/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 99/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 100/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 101/5000\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 102/5000\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 1.5374 - accuracy: 0.6198 - val_loss: 1.5695 - val_accuracy: 0.6118\n",
      "Epoch 103/5000\n",
      "136/340 [===========>..................] - ETA: 0s - loss: 1.4908 - accuracy: 0.6314"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-120e39743a2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'MeanSquaredError'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m history = model.fit(trainData, Transformer.toClassification(activitiesTrain), \n\u001b[0m\u001b[0;32m     22\u001b[0m                     validation_data = (valData, Transformer.toClassification(activitiesValidate)), epochs=5000, batch_size=8)\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       (graph_function,\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=3e-2,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(np.shape(trainData)[1], input_dim=np.shape(trainData)[1], activation='relu', \n",
    "                kernel_regularizer = keras.regularizers.L2(0.001)))\n",
    "model.add(Dense(100, activation='relu', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model.add(Dense(120, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model.add(Dense(140, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model.add(Dense(120, activation='relu', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model.add(Dense(80, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model.add(Dense(40, activation='relu', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model.add(Dense(20, activation='relu', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model.add(Dense(5, activation='relu', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "model.add(Dense(1, activation='tanh', kernel_regularizer = keras.regularizers.L1(0.001)))\n",
    "\n",
    "model.compile(loss='MeanSquaredError', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model.fit(trainData, Transformer.toClassification(activitiesTrain), \n",
    "                    validation_data = (valData, Transformer.toClassification(activitiesValidate)), epochs=1600, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "micro-challenge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x26c79714c10>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "classify = Transformer.toClassification(activitiesTrain)\n",
    "constantGuess = (len(classify[classify == 1]))/len(classify)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,8))\n",
    "\n",
    "ax.plot(history.history[\"accuracy\"], color=\"green\")\n",
    "ax.plot(history.history[\"val_accuracy\"], color=\"purple\")\n",
    "ax.axhline(constantGuess, color=\"blue\", linestyle=\"dashed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-corporation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
