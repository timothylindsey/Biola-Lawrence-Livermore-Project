{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "impressive-munich",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lemon\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import Loader\n",
    "import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "complicated-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "compoundsTrain, smilesTrain, labelsTrain, compoundDataTrain, activitiesTrain = Loader.getTrain(defaultValue=0)\n",
    "compoundsTest, smilesTest, labelsTest, compoundDataTest, activitiesTest = Loader.getTest(defaultValue=0)\n",
    "compoundsValidate, smilesValidate, labelsValidate, compoundDataValidate, activitiesValidate = Loader.getValidate(defaultValue=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "synthetic-sharp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.32490387 -0.49296417 -0.06196007 -0.15441773 -0.23518289 -0.19279585\n",
      "  -0.25198293 -0.30910509 -0.35565224 -0.39987694 -0.95915226 -1.11258836\n",
      "  -0.9285298  -0.94692905 -0.94571493 -0.97977271 -0.84669681 -0.74529731\n",
      "  -0.82915184 -0.6014727 ]\n",
      " [ 2.38096851  2.29045988  2.16483231  2.21593806  2.12853562  2.07288896\n",
      "   2.12326734  2.08229953  2.10998881  2.02502354 -2.48361232 -2.19807384\n",
      "  -1.29392184 -2.66396392 -1.97343888 -1.80372952 -1.40312727 -0.91523786\n",
      "  -1.88042339 -1.7498292 ]\n",
      " [ 0.4327404   0.28639456  0.27205879  0.4099527   0.32760723  0.37362535\n",
      "   0.31355285  0.26027696  0.42886991  0.37168231  0.24029525 -0.14980388\n",
      "   0.17571035  0.06181023 -0.12495992  0.30221004  0.1015241  -0.20712045\n",
      "   0.49001317  0.09451052]\n",
      " [-0.43313876 -0.27029025 -0.06196007  0.18420452  0.10249119  0.14705687\n",
      "   0.20044569  0.14640055  0.20472072  0.15123681  0.36735066  0.61089672\n",
      "   0.2236392   0.54464638  0.74367204  0.33258657  0.47480186  0.56563983\n",
      "   0.40919943  0.57374169]\n",
      " [-1.29901792 -1.49499683 -1.62071473 -1.62178084 -1.6984372  -1.77877522\n",
      "  -1.8354831  -1.90337483 -1.81262196 -1.83277267 -1.71618771 -1.22849814\n",
      "  -0.96503486 -0.89484621 -1.68766636 -0.97826916 -0.99161844 -1.53386982\n",
      "  -0.61611002  0.04849227]] [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.] [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          5.69897     6.37214121\n",
      "  5.69897     7.56969293  5.23432733  5.12207436  5.31785492  5.59962727\n",
      "  5.34036379  5.83185409  5.69897     5.69897     5.76575729  5.83937845\n",
      "  5.27095816  6.92006368  6.02856318  5.69897     4.6696875   6.31226667\n",
      "  6.51286163  5.69897     7.30102999  4.62115655  4.55817734  5.69897\n",
      "  6.65515153  5.73353311  5.95751708 10.82390874  4.60568408  6.60870195\n",
      "  5.74167406  5.69897     5.91377203  5.36957213  4.69313073  6.54021486\n",
      "  6.40979672  6.40103835  6.39065891  6.93635591  5.26626689  6.4202164\n",
      "  5.69897     5.4710833   5.02992442  5.16035884  6.58391395  6.18520656\n",
      "  5.90457407  5.50881168  8.85387196  4.83768576  5.349485    7.25181197\n",
      "  5.97785651  5.67704294  5.2135085   4.94006685  5.69897     5.80410035\n",
      "  5.76611887  6.98716277  7.01031524  5.38721614  6.10734897  4.69897\n",
      "  6.62282129  6.48584744  5.69897     5.349485    4.6546527   4.97061622\n",
      "  6.18605469  5.61232277  6.04575749  5.69897     5.00607312  6.79377853\n",
      "  6.33184728  5.69897     6.56703071  5.14621693  5.11092437  5.95705731\n",
      "  6.22640797  4.56363219  6.30385466  5.39891827  5.69897     5.4202164\n",
      "  5.19382003  5.35760473  5.00082602  7.69897     7.04686234  5.39030081\n",
      "  7.25181197  5.69897     5.30364361  7.69897     6.82390874  5.69897\n",
      "  5.69897     4.91728786  5.15490196  5.42723232  5.69897     5.07265685\n",
      "  6.9340878   5.0373322   7.68204756  6.62621758  4.62324886  5.10734897\n",
      " 10.          7.16659787  6.24968589  7.09691001  5.62934582  5.11163436\n",
      "  5.92860078  7.22184875  5.26395397  6.61590029  7.69680394  6.34113041\n",
      "  5.1369687   5.26965809  5.64866622  5.08092191  5.20760831  7.06048075\n",
      "  5.47745099  5.55280128  5.39007019  5.04521392  6.32537882  5.69897\n",
      "  7.47353149  4.94859311  6.01772877  7.69897     6.96577274  6.00525275\n",
      "  4.63729051  6.29413629  5.49263197  5.29257156  4.84338581  4.66451452\n",
      "  5.69897     5.39205555  5.69897     7.69897     4.91238826  6.05613457\n",
      "  5.69897     5.36076123  6.61978876  7.43374998  5.46597389  6.85362638\n",
      "  5.43809578  5.56143159  5.33724217  5.35556141  6.38584004  4.62121149\n",
      "  5.69897     6.57511836  6.16013702  4.8096683   6.7212464   6.93858467\n",
      "  5.64235016  6.60899255  4.95044942  5.4290888   5.5972043   5.66473087\n",
      "  7.20252471  7.85387196]\n"
     ]
    }
   ],
   "source": [
    "#after transformations are done assign data\n",
    "dataLabels = labelsTrain[0:20] #only docking and fusion\n",
    "trainData = compoundDataTrain[:,0:20]\n",
    "testData = compoundDataTest[:,0:20]\n",
    "valData = compoundDataValidate[:,0:20]\n",
    "\n",
    "trainData, testData, valData = Transformer.normalizeData(trainData, testData, valData, newMean=0, newStd=1)\n",
    "outputData = Transformer.toClassification(activitiesTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "varying-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.losses import MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "distributed-prisoner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1600\n",
      "340/340 [==============================] - 3s 4ms/step - loss: 1.8679 - accuracy: 0.1023 - val_loss: 1.4410 - val_accuracy: 0.2206\n",
      "Epoch 2/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 1.2035 - accuracy: 0.1248 - val_loss: 0.9869 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.9427 - accuracy: 0.1086 - val_loss: 0.9398 - val_accuracy: 0.4118\n",
      "Epoch 4/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.9102 - accuracy: 0.1233 - val_loss: 0.8992 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.9084 - accuracy: 0.0397 - val_loss: 0.8886 - val_accuracy: 0.3824\n",
      "Epoch 6/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.8952 - accuracy: 0.0766 - val_loss: 0.8938 - val_accuracy: 0.4265\n",
      "Epoch 7/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.8982 - accuracy: 0.0736 - val_loss: 0.8960 - val_accuracy: 0.4235\n",
      "Epoch 8/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.8904 - accuracy: 0.1082 - val_loss: 0.8795 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.8879 - accuracy: 0.0736 - val_loss: 0.8913 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.8843 - accuracy: 0.0534 - val_loss: 0.8850 - val_accuracy: 0.4529\n",
      "Epoch 11/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.8819 - accuracy: 0.1185 - val_loss: 0.8753 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.8794 - accuracy: 0.1251 - val_loss: 0.8729 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.8815 - accuracy: 0.0935 - val_loss: 0.8802 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.8842 - accuracy: 0.0596 - val_loss: 0.8749 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.8836 - accuracy: 0.0714 - val_loss: 0.8886 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.8800 - accuracy: 0.0574 - val_loss: 0.8979 - val_accuracy: 0.4441\n",
      "Epoch 17/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.8802 - accuracy: 0.0788 - val_loss: 0.8796 - val_accuracy: 0.3265\n",
      "Epoch 18/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.8766 - accuracy: 0.0409 - val_loss: 0.8992 - val_accuracy: 0.4676\n",
      "Epoch 19/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.8781 - accuracy: 0.0902 - val_loss: 0.9340 - val_accuracy: 0.4941\n",
      "Epoch 20/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.8788 - accuracy: 0.0813 - val_loss: 0.8810 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.8785 - accuracy: 0.0802 - val_loss: 0.8886 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.8794 - accuracy: 0.0950 - val_loss: 0.8795 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/1600\n",
      "340/340 [==============================] - 1s 2ms/step - loss: 0.8761 - accuracy: 0.0622 - val_loss: 0.8778 - val_accuracy: 0.2882\n",
      "Epoch 24/1600\n",
      " 54/340 [===>..........................] - ETA: 0s - loss: 0.8114 - accuracy: 0.3750"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-119b9edc7ad4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'MeanSquaredError'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m history = model.fit(trainData, Transformer.toClassification(activitiesTrain), \n\u001b[0m\u001b[0;32m     24\u001b[0m                     validation_data = (valData, Transformer.toClassification(activitiesValidate)), epochs=1600, batch_size=8)\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1567\u001b[0m                             \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m                             \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1569\u001b[1;33m                             \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1570\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1571\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mstep_increment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1392\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_spe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1394\u001b[1;33m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1395\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep_increment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1396\u001b[0m         \u001b[1;34m\"\"\"The number to increment the step for `on_batch_end` methods.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=3e-2,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "\n",
    "l1Reg = 1e-10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(np.shape(trainData)[1], input_dim=np.shape(trainData)[1], activation='tanh', \n",
    "                kernel_regularizer = keras.regularizers.L2(l1Reg)))\n",
    "model.add(Dense(50, activation='leaky_relu', kernel_regularizer = keras.regularizers.L1()))\n",
    "model.add(Dense(80, activation='leaky_relu', kernel_regularizer = keras.regularizers.L1(l1Reg)))\n",
    "model.add(Dense(100, activation='leaky_relu', kernel_regularizer = keras.regularizers.L1(l1Reg)))\n",
    "model.add(Dense(80, activation='leaky_relu', kernel_regularizer = keras.regularizers.L1(l1Reg)))\n",
    "model.add(Dense(60, activation='leaky_relu', kernel_regularizer = keras.regularizers.L1(l1Reg)))\n",
    "model.add(Dense(40, activation='leaky_relu', kernel_regularizer = keras.regularizers.L1(l1Reg)))\n",
    "model.add(Dense(20, activation='leaky_relu', kernel_regularizer = keras.regularizers.L1(l1Reg)))\n",
    "model.add(Dense(10, activation='leaky_relu', kernel_regularizer = keras.regularizers.L1(l1Reg)))\n",
    "model.add(Dense(1, activation='tanh', kernel_regularizer = keras.regularizers.L1(l1Reg)))\n",
    "\n",
    "model.compile(loss='MeanSquaredError', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model.fit(trainData, Transformer.toClassification(activitiesTrain), \n",
    "                    validation_data = (valData, Transformer.toClassification(activitiesValidate)), epochs=1600, batch_size=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
